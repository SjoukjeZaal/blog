[{"content":"Over the years, my work spread across many platforms. Blog posts, articles, talks, technical notes, experiments. Some published under company banners, some on personal sites, some simply gone.\nThat fragmentation started to bother me. Not emotionally — architecturally.\nPlatforms change. Companies rebrand. Services shut down. Links decay. If your work depends on someone else’s platform, it is fragile by default.\nSo I chose a different approach.\nThis site is not “another blog”.\nIt is my personal digital archive, intentionally built to remain accessible beyond platforms, trends, and tooling choices.\nWhy static Everything here is static. Plain text. Markdown. Generated with Hugo.\nNo database. No backend. No runtime dependencies.\nThat choice is deliberate.\nStatic content is boring in exactly the right way. It can be copied, rebuilt, validated, and moved without friction. It survives because it assumes very little.\nIf I can recreate this site from a folder of markdown files, I am not locked in.\nIPFS as a distribution layer, not a promise The archive is published on IPFS and addressed by content rather than location. That makes the content verifiable: what you read is exactly what was published.\nBut IPFS is not permanent storage.\nIt does not guarantee availability. It guarantees addressability.\nLong-term availability comes from operations. This site is hosted via 4EVERLAND, which provides managed IPFS, IPNS, replication, and long-term pinning. That operational layer is what keeps the archive online over time.\nThe distinction matters.\nIPNS and continuity Writing evolves. This archive is not frozen.\nIPNS provides a stable reference that can be updated as new material is added, without breaking existing links. The reference stays constant while the content moves forward.\nThat balance — continuity without stagnation — was essential.\nENS, ownership, and sovereignty The archive is reachable via sjoukje.eth, registered through the Ethereum Name Service (ENS).\nThis is not about novelty. It is about sovereignty.\nThe name is not issued by a registrar, governed by platform terms, or subject to policy changes. Control sits at the protocol level. As long as the underlying chain exists, the name remains under my authority.\nThat matters. It means I decide where this archive lives, how it is served, and when it moves. Hosting providers can change. Gateways can shift. Infrastructure can evolve. The identity does not.\nENS turns naming into an architectural choice rather than a commercial dependency.\nWhy it is also available via a regular domain Decentralized infrastructure should not reduce reach.\nThis archive is also available through a traditional domain. Different access paths, identical content. The goal is accessibility, not ideology.\nThis is not nostalgia This archive is not about preserving the past for sentimental reasons.\nIt is about professional continuity. Architecture decisions, lessons learned, and patterns that repeat over time only compound if they remain accessible.\nThe result What you are reading is the outcome of that thinking:\nStatic and portable Distributed via IPFS Updated through IPNS Anchored to an ENS identity Backed by long-term hosting Rebuildable from plain text No dashboards.\nNo feeds.\nNo algorithms.\nJust a record — designed to last.\n","permalink":"//localhost:1313/posts/2026-01-13-a-personal-digital-archive-by-design/","summary":"\u003cp\u003eOver the years, my work spread across many platforms. Blog posts, articles, talks, technical notes, experiments. Some published under company banners, some on personal sites, some simply gone.\u003c/p\u003e\n\u003cp\u003eThat fragmentation started to bother me. Not emotionally — architecturally.\u003c/p\u003e\n\u003cp\u003ePlatforms change. Companies rebrand. Services shut down. Links decay. If your work depends on someone else’s platform, it is fragile by default.\u003c/p\u003e\n\u003cp\u003eSo I chose a different approach.\u003c/p\u003e\n\u003cp\u003eThis site is not “another blog”.\u003cbr\u003e\nIt is my personal digital archive, intentionally built to remain accessible beyond platforms, trends, and tooling choices.\u003c/p\u003e","title":"My Personal Digital Archive — by Design"},{"content":"Data and AI platforms are at the heart of digital transformation. But too often, they are built on proprietary formats and closed models that create silos, raise costs, and limit flexibility. In a world where organizations need interoperability, sovereignty, and speed, this approach is no longer sustainable. Open standards offer a better path forward.\nFrom Lock-In to Openness\nData and AI platforms used to be closed worlds. Proprietary formats, vendor-specific connectors, and black-box models accelerated adoption, but at a cost: lock-in, high switching barriers, and reduced sovereignty. Once data was stored in a proprietary format or models were trained in a closed environment, moving them became difficult and costly. Enterprises often found themselves redesigning workflows, rewriting code, or even discarding valuable assets when switching vendors.\nThat era is ending. Thanks to open standards and an expanding ecosystem of open source AI models, platforms are shifting from controlled silos to flexible, collaborative ecosystems.\nFrom Delta Lake and Apache Iceberg for data storage, to open source AI models like GPT oss, Llama and Mistral, openness is redefining what’s possible. These technologies allow organizations to move data and compound AI model systems (agents) freely, scale AI workloads across clouds, and maintain transparency for regulators and partners.\nWhy Openness Matters\nFor enterprises and governments, the stakes are clear:\nCompliance with evolving AI and data regulation Sovereignty in controlling where data is stored and how it is used Interoperability across hybrid and multi-cloud environments Closed ecosystems make these challenges harder. They increase dependency on one vendor, complicate audits, and reduce flexibility in adopting new technologies. In contrast, open standards provide a shared foundation that works across providers and platforms. Structured and unstructured data stored in an open format like Parquet, Delta Lake or Iceberg can be analyzed by multiple engines without conversion. AI models built on open architectures can be audited, retrained, and reused without hidden restrictions.\nThis interoperability preserves freedom of choice and protects long-term investments, ensuring today’s platform decisions remain valid tomorrow.\nThe Databricks Contribution\nDatabricks has been at the forefront of making openness real in enterprise environments. The company’s origins are in open source, and its engineers have played a central role in creating and fostering community growth some of the most widely adopted standards in data and AI.\nApache Spark transformed large-scale data processing and became the de facto standard for distributed analytics.\nDelta Lake, developed by Databricks and now an open-source project managed by the Linux Foundation, introduced ACID transactions and reliability to data lakes.\nMLflow emerged as the most widely used open-source platform for managing the machine learning lifecycle, from experiment tracking to deployment.\nUnity Catalog serves as the unified governance layer on the Databricks Platform, and its core functionality has also recently been open-sourced.\nSpark Declarative Pipelines is Databricks’ latest donation to Spark, allowing for easy and fast pipeline development.\nDelta Sharing is an open protocol for secure data sharing, making it simple to share data with other organizations regardless of which computing platforms they use.\nMore recently, Databricks has embraced Apache Iceberg, enabling interoperability across engines and vendors, and expanded support for open source AI models, giving enterprises choice beyond proprietary stacks. Databricks is the only vendor that supports multiple formats in Delta Lake and Iceberg.\nThe Databricks Data Intelligence Platform combines these open components with enterprise-grade governance, security, and performance. This ensures organizations can adopt open standards with confidence, scaling them across industries and regions. In short, Databricks shows that openness does not come at the expense of reliability - it strengthens it.\nReal-World Impact\nOpenness is already reshaping industries:\nFinancial services: Banks are under strict scrutiny when it comes to data handling. By building AI models on open data formats, they can meet compliance requirements across jurisdictions while maintaining flexibility. A trading desk in London and a compliance team in Frankfurt can work on the same data without duplicating or reformatting it. This reduces operational risk while speeding up decision-making. Healthcare: Clinical data, imaging, and genomic research data are often stored in different systems, each with its own rules. Using open standards, healthcare organizations can combine these datasets securely, accelerating drug discovery and enabling more personalized care. Instead of spending months reconciling formats, researchers can collaborate on a trusted, common layer, an approach that became especially critical during the pandemic. Public sector: Governments are increasingly focused on digital sovereignty. By deploying open AI models on platforms built around open standards, they reduce reliance on single vendors and ensure long-term control of critical digital infrastructure. Whether it’s analyzing mobility patterns, monitoring energy grids, or building citizen services, open standards allow governments to meet regulatory requirements while still innovating at pace. In each case, Databricks provides the foundation for adopting these open standards at scale, adding the governance and reliability needed for mission-critical use.\nLooking Ahead\nThe next generation of Data \u0026amp; AI platforms will not be defined by single vendors, but by ecosystems built on open standards. The pace of innovation in AI models, storage formats, and regulatory frameworks is simply too fast for closed approaches to keep up.\nEnterprises that adopt open data formats and open AI models will have the agility to innovate, comply, and collaborate globally. They will be able to mix and match tools, clouds, and providers with minimal friction. This avoids the costly rewrites and migrations of the past.\nWith its deep roots in open source and its enterprise platform, Databricks stands at the center of this movement. It demonstrates that openness is not just a technical preference, but a strategic foundation for the future. By combining community-driven innovation with enterprise-grade capabilities, Databricks is enabling organizations to protect their investments, preserve their sovereignty, and accelerate their journey into the data and AI-driven economy.\n","permalink":"//localhost:1313/posts/2025-12-11-open-standards-foundation-future-data-ai/","summary":"\u003cp\u003eData and AI platforms are at the heart of digital transformation. But too often, they are built on proprietary formats and closed models that create silos, raise costs, and limit flexibility. In a world where organizations need interoperability, sovereignty, and speed, this approach is no longer sustainable. Open standards offer a better path forward.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eFrom Lock-In to Openness\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eData and AI platforms used to be closed worlds. Proprietary formats, vendor-specific connectors, and black-box models accelerated adoption, but at a cost: lock-in, high switching barriers, and reduced sovereignty. Once data was stored in a proprietary format or models were trained in a closed environment, moving them became difficult and costly. Enterprises often found themselves redesigning workflows, rewriting code, or even discarding valuable assets when switching vendors.\u003c/p\u003e","title":"Open Standards as the Foundation of Future Data \u0026 AI Platforms"},{"content":"We’ve professionalized technology to the point where we’ve lost touch with it.\nIn many organizations, technology leadership has turned into governance, reporting, and compliance. The intent was good: control risk, manage complexity, scale globally.\nBut in doing so, many organizations created a gap between leadership and the actual technology.\nDecisions are now abstracted behind PowerPoint decks and KPIs instead of grounded in code, infrastructure, or architecture. When governance replaces guidance, you lose speed, context, and credibility. The result is a leadership layer that knows how to measure delivery but not how to drive it.\nThe cost of “governance over guidance” Over the past decade, the balance quietly shifted.\nCIOs, CTOs, and architects became process owners instead of technology shapers. Review boards replaced design sessions. Reports replaced experimentation.\nWhat was meant to scale control ended up scaling distance.\nWhen that happens, technology stops being something you shape; it becomes something you manage.\nAnd that’s a dangerous place to be, especially now.\nThe AI shift raises the stakes The rise of AI is not just another wave of technology. It is a structural change in how systems are built, operated, and evolve.\nIntelligent agents, adaptive architectures, and autonomous workflows are redefining what it means to design and lead technology.\nBut the impact goes much further than IT. AI is starting to reshape how entire organizations operate. Decision-making, customer interaction, and even product creation are becoming continuous, data-driven, and self-optimizing processes.\nWork is no longer defined by fixed systems and human orchestration but by dynamic, learning ecosystems that adjust in real time. The line between business and technology is dissolving.\nIn this new reality, technology leadership is not just about systems. It is about steering the behavior of intelligent, autonomous environments that shape the business itself.\nLeaders who stay distant from technology lose their ability to steer that transformation.\nWithout a real understanding of how technology works, scales, and connects across the organization, you cannot responsibly guide the adoption of AI or turn it into a sustainable advantage.\nThe gap between leadership and technology is no longer just inefficient. It is risky.\nThe fading role of the architect No one has felt this shift more than the architect. Once the bridge between vision and engineering, many architects now find themselves reduced to administrators of complexity. Too technical for business meetings, too strategic for delivery teams.\nTheir days are filled with reviews, frameworks, and approvals, while the actual act of designing and building moves elsewhere.\nBut this is exactly the time when we need architects the most.\nAI, data, and cloud are converging into a new kind of architecture: distributed, adaptive, and continuously learning.\nWe need architects who can translate that complexity into clarity, who understand both the technical and ethical dimensions, and who can design systems that remain trustworthy even as they evolve.\nStaying intellectually close to technology Strong leaders stay intellectually close to the technology they lead.\nThat doesn’t mean writing code every day. It means understanding why things work the way they do, and how small decisions ripple through the system.\nIt means asking questions that go one layer deeper: How does this scale? Where are the dependencies? What are the limits of what we’ve built?\nWhen leaders stay close to those questions, they build credibility and make better strategic choices.\nThe best technology organizations I know are guided by leaders who understand both the architecture and the ambition behind it.\nRebuilding a tech-first culture Reclaiming technology leadership starts with rebuilding a culture where understanding technology is as valuable as managing it.\nThat doesn’t mean abandoning structure or accountability. It means balancing them with curiosity, experimentation, and real technical dialogue.\nGive architects space to explore, not just to approve.\nValue architectural thinking as a strategic skill, not a compliance function.\nAnd above all, reconnect leadership with the systems that now learn, reason, and act on our behalf.\nBecause in the age of AI, technology doesn’t wait for direction; it learns from it.\nAnd if leaders don’t stay close enough to guide that learning, the systems will start leading us instead.\nIt’s time to stop leading technology from a distance and start leading through it again.\n","permalink":"//localhost:1313/posts/2025-11-09-reclaiming-technology-leadership/","summary":"\u003cp\u003eWe’ve professionalized technology to the point where we’ve lost touch with it.\u003cbr\u003e\nIn many organizations, technology leadership has turned into governance, reporting, and compliance. The intent was good: control risk, manage complexity, scale globally.\u003cbr\u003e\nBut in doing so, many organizations created a gap between leadership and the actual technology.\u003cbr\u003e\nDecisions are now abstracted behind PowerPoint decks and KPIs instead of grounded in code, infrastructure, or architecture. When governance replaces guidance, you lose speed, context, and credibility. The result is a leadership layer that knows how to measure delivery but not how to drive it.\u003c/p\u003e","title":"Reclaiming Technology Leadership"},{"content":"The role of the architect is being rewritten.\nNot by new frameworks or methodologies, but by intelligent systems that can design, optimize, and learn on their own.\nAs AI agents, autonomous workflows, and generative tools become part of daily operations, architecture itself becomes dynamic.\nIt’s no longer a static blueprint. It’s a living system that continuously adapts and improves.\nFrom Control to Coordination For years, architecture has centered on control: standards, reviews, governance.\nBut in an agentic world, control alone is not enough.\nAI systems generate code, detect anomalies, and make real-time adjustments faster than any process can document.\nThe architect’s role shifts from enforcing compliance to guiding intelligent collaboration between humans, systems, and autonomous agents.\nWe become orchestrators of interaction, not owners of process.\nAdaptive by Design Modern architecture must be able to evolve.\nA well-designed environment responds to signals: performance, usage, or ethical risk.\nIt learns. It self-corrects.\nThis means architects no longer design systems that simply work; we design systems that grow. Adaptive ecosystems that continuously align technology with intent and outcomes.\nNew Building Blocks Three elements define this next phase of architecture:\nAI Agents: autonomous components that analyse data, act, and improve through feedback. Policy-as-Code: governance and principles embedded directly into automation. Feedback Loops: continuous sensing and learning mechanisms that keep systems aligned with goals and values. Together, they create architectures that are aware, responsive, and self-optimising.\nThe Architect as Ecosystem Designer The modern architect designs behaviour as much as structure.\nWe define how systems and teams interact, how decisions are made, and how accountability is shared.\nOur focus expands from technology stacks to value creation and observability, ensuring that every intelligent component contributes to business outcomes in a responsible way.\nA Redefinition of the Profession AI will not replace architects.\nBut it will challenge us to evolve, to think less about control, and more about direction, coherence, and learning.\nThe future architect builds trust between humans and machines.\nWe design systems that think, act, and improve, responsibly and transparently.\nThe true transformation of architecture lies in its shift from structure to intelligence, evolving into a discipline that learns, adapts, and connects human and machine purpose.\n","permalink":"//localhost:1313/posts/2025-11-02-the-new-role-of-the-architect-in-an-agentic-world/","summary":"\u003cp\u003eThe role of the architect is being rewritten.\u003cbr\u003e\nNot by new frameworks or methodologies, but by intelligent systems that can design, optimize, and learn on their own.\u003c/p\u003e\n\u003cp\u003eAs \u003cstrong\u003eAI agents\u003c/strong\u003e, \u003cstrong\u003eautonomous workflows\u003c/strong\u003e, and \u003cstrong\u003egenerative tools\u003c/strong\u003e become part of daily operations, architecture itself becomes dynamic.\u003cbr\u003e\nIt’s no longer a static blueprint. It’s a living system that continuously adapts and improves.\u003c/p\u003e\n\u003ch3 id=\"from-control-to-coordination\"\u003e\u003cstrong\u003eFrom Control to Coordination\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eFor years, architecture has centered on control: standards, reviews, governance.\u003cbr\u003e\nBut in an agentic world, control alone is not enough.\u003cbr\u003e\nAI systems generate code, detect anomalies, and make real-time adjustments faster than any process can document.\u003c/p\u003e","title":"The New Role of the Architect in an Agentic World"},{"content":"For years, data platforms were designed around people. The main goal was to provide humans with dashboards, reports, and analytics so they could make better decisions. But this model is being reshaped. With the rise of AI agents, data platforms are entering a new era—one where the consumers of data are not only people but also autonomous systems.\nAnd here’s the important part: these agents don’t just consume. They also produce. They generate enriched datasets, annotations, and continuous insights. This dual role changes how we need to think about architecture, governance, and collaboration.\nWhy This Matters Now Organizations have spent decades building systems to make data accessible and useful. From warehouses to lakes, from mesh to fabric, each wave aimed to improve scale, quality, and accessibility. But all of these designs assumed a human at the other end.\nAgents disrupt that assumption. They can query, reason, and act in real time. They can also create new knowledge artifacts that feed into the data platform itself. This is not a theoretical trend—already today, copilots, chatbots, and automated workflows are enriching datasets with tags, predictions, and classifications.\nIf we don’t adapt our data platforms, we risk creating environments where humans and agents operate in silos, duplicating work and reducing trust.\nOpportunities and Challenges Ahead 1. A Two-Way Data Flow Traditional data flows end with a human decision. In an agent-driven world, the loop closes: data goes in, actions and new data come out. For example, an AI agent monitoring energy grids might not just alert engineers about anomalies but also generate a new dataset of patterns that becomes valuable training material for future models.\n2. Lineage and Trust at Scale When agents create data, the question of lineage becomes urgent. Which model produced this dataset? Which source data was used? Was the agent operating within its authorized scope? Without clear lineage, we risk a black box of agent-generated data, which undermines compliance and decision-making.\n3. Governance for Non-Humans Most organizations today focus on user roles, access rights, and compliance rules for people. But agents are new actors. They need identities, access policies, and monitoring. Just as we don’t give every employee unrestricted access, we shouldn’t give agents blanket rights to read or write data. Fine-grained governance, combined with observability, will be critical.\n4. Human + Agent Collaboration The real power lies in combining strengths. Agents can operate at scale and speed, processing volumes no human could. Humans bring context, ethical judgment, and strategic oversight. Data platforms must be designed for this hybrid model, where a compliance officer might validate a dataset enriched by an agent, or a product team might extend features based on agent-detected signals.\nWhat Data Platforms Need to Evolve To support this shift, future platforms should:\nTreat agents as first-class citizens with identities, roles, and accountability.\nEmbed explainability so that both agent and human contributions are transparent.\nProvide shared canvases where human and agent outputs coexist, with clear versioning and traceability.\nEnable continuous governance, where policies apply equally to people and agents, across environments.\nWe can draw inspiration from service mesh and distributed architecture patterns—concepts designed to manage complexity, security, and scale in systems where many services interact. Data platforms now need their own “agent mesh,” ensuring that interactions between humans, agents, and datasets are reliable, secure, and explainable.\nA Future of Mixed Ecosystems The enterprise of tomorrow will not be human-only or agent-only. It will be a mixed ecosystem where humans and agents collaborate side by side. Picture an operations center where human analysts and AI agents both monitor streams of data. The agents handle the noise, surfacing anomalies, while humans apply context and strategy. Both contribute datasets back to the platform, enriching it for the next cycle.\nOrganizations that adapt early will gain an edge. They will have platforms ready not just for today’s dashboards and queries, but for tomorrow’s autonomous collaboration.\nFinal Thought Data platforms were once built for reporting. Then they were built for advanced analytics. Now they must be built for collaboration—between humans and agents alike.\nThe question is no longer whether AI agents will join our data ecosystems. They already have. The question is whether we’re ready to treat them as active participants, with the structures, governance, and trust needed to make the collaboration productive.\n","permalink":"//localhost:1313/posts/2025-08-19-ai-agents-new-consumers-producers-data/","summary":"\u003cp\u003eFor years, data platforms were designed around people. The main goal was to provide humans with dashboards, reports, and analytics so they could make better decisions. But this model is being reshaped. With the rise of AI agents, data platforms are entering a new era—one where the consumers of data are not only people but also autonomous systems.\u003c/p\u003e\n\u003cp\u003eAnd here’s the important part: these agents don’t just consume. They also produce. They generate enriched datasets, annotations, and continuous insights. This dual role changes how we need to think about architecture, governance, and collaboration.\u003c/p\u003e","title":"AI Agents as New Consumers and Producers of Data"},{"content":"Over the past few years, we’ve seen organizations experiment with the concept of an AI Factory—a structured way to industrialize the development and deployment of AI. At the same time, data leaders have embraced the idea of data products and data fabrics to make data more reusable, governed, and available at scale.\nIt’s time to bring these two together!\nFrom Projects to Production Lines Think of how manufacturing evolved: from craft workshops to assembly lines, where processes were standardized and repeatable. We are witnessing a similar shift in data and AI. Instead of bespoke projects—each with its own tools, pipelines, and governance—organizations are moving toward industrialized AI + data factories.\nIn this model:\nData factories prepare, refine, and package high-quality data products.\nAI factories consume these products, train and deploy models, and return insights back into the business.\nTogether, they form a repeatable cycle of design → build → run → improve that can be scaled across business units.\nWhy This Matters Today, most enterprises still lose time reinventing the wheel: rebuilding the same data pipelines, retraining similar models, solving governance issues in silos. The factory mindset replaces this with standardized components and shared services—cutting cost and time, while improving trust and compliance.\nReusability: Once a data product or model is created, it can be reused across teams and industries.\nGovernance by design: Security, privacy, and compliance are embedded, not added later.\nScalability: New use cases move from idea to production in weeks instead of months.\nWhat an AI \u0026amp; Data Supply Chain Could Look Like in 2030 By 2030, we could see organizations operating AI \u0026amp; Data Supply Chains much like today’s physical supply chains:\nRaw Material Layer: Data streaming in from IoT, transactions, documents, and external sources.\nProcessing Layer: Data factories transform raw inputs into standardized data products, with metadata describing quality, lineage, and access.\nAssembly Layer: AI factories use these data products to build models, prompts, and agentic workflows.\nDistribution Layer: Insights, predictions, and autonomous agents are deployed back into operations, products, or customer channels.\nFeedback Loop: Usage data, performance metrics, and human feedback flow back into the chain, improving both data products and AI models.\nJust as manufacturing supply chains depend on logistics, contracts, and quality control, AI \u0026amp; Data Supply Chains will depend on cloud platforms, distributed architectures, service meshes, and governance tooling.\nThe Call to Action The shift from projects to factories is not about technology alone—it requires new operating models, new governance, and a mindset of continuous, repeatable innovation. Organizations that succeed will treat AI and data not as experiments, but as products moving along a supply chain.\nBy 2030, the companies that master this approach will be the ones creating new markets, not just keeping up with them.\n","permalink":"//localhost:1313/posts/2025-08-19-ai-data-factories-innovation-supply-chain/","summary":"\u003cp\u003eOver the past few years, we’ve seen organizations experiment with the concept of an \u003cstrong\u003eAI Factory\u003c/strong\u003e—a structured way to industrialize the development and deployment of AI. At the same time, data leaders have embraced the idea of \u003cstrong\u003edata products\u003c/strong\u003e and \u003cstrong\u003edata fabrics\u003c/strong\u003e to make data more reusable, governed, and available at scale.\u003c/p\u003e\n\u003cp\u003eIt’s time to bring these two together!\u003c/p\u003e\n\u003ch2 id=\"from-projects-to-production-lines\"\u003e\u003cstrong\u003eFrom Projects to Production Lines\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eThink of how manufacturing evolved: from craft workshops to assembly lines, where processes were standardized and repeatable. We are witnessing a similar shift in data and AI. Instead of bespoke projects—each with its own tools, pipelines, and governance—organizations are moving toward \u003cstrong\u003eindustrialized AI + data factories\u003c/strong\u003e.\u003c/p\u003e","title":"AI Factories Meet Data Factories: Building the Supply Chains of Innovation"},{"content":"We’re entering a phase where engineering is less about writing every instruction and more about teaching systems how to think. The shift is clear: from procedural to declarative. From “write every line” to “explain the outcome.” From “code it” to “coach the model.”\nThis isn’t just about GenAI. It changes how we design, build, and operate systems when AI becomes part of the team.\nCode Is No Longer the Core Output Most engineering teams still measure productivity by the amount of code shipped. But with AI-generated code, low-code platforms, and autonomous agents, raw output is no longer the right metric.\nThe value shifts to design clarity, intent, and oversight. Models don’t need detailed instructions — they need structure, context, and outcomes they can align to.\nThink Outcome, Not Steps Traditional development relies on procedural thinking: describe every step, control the flow, handle every exception. But AI systems work better when you describe the destination and let them find the path.\nDeclarative thinking means defining what you want — not how to get there. You already see this in tools like Terraform, Bicep, and serverless workflows. You define the desired state, and the system figures out how to get there.\nThis isn’t just more efficient — it’s more scalable in a world where logic is co-authored by machines.\nCoaching AI Is a Design Discipline To make AI useful, you don’t code logic — you shape behavior. That means:\nFraming clear objectives and constraints Providing relevant examples and signals Reviewing outputs and edge cases Adjusting based on feedback and changing conditions You’re not programming — you’re supervising. The ability to tune, guide, and iterate becomes more valuable than the ability to write perfect functions.\nWhat This Means for Engineering Teams If your team is still optimizing for “how fast we can code,” you’re solving the wrong problem.\nHere’s what needs to change:\nShift from function logic to system behavior Combine code review with prompt, model, and config review Train engineers to think like orchestrators, not implementers Use declarative tools where possible — especially in cloud, infra, and data workflows Define success in terms of outcomes, not just deployments The biggest challenge isn’t tooling. It’s mindset. Final Thought Declarative thinking forces us to simplify, clarify, and trust the system to do its part. That’s how we scale in an AI-first world.\nWe don’t need more code. We need better coaching.\n","permalink":"//localhost:1313/posts/2025-08-06-stop-coding-start-coaching-declarative/","summary":"\u003cp\u003eWe’re entering a phase where engineering is less about writing every instruction  and more about teaching systems how to think. The shift is clear: from procedural to declarative. From “write every line” to “explain the outcome.” From “code it” to “coach the model.”\u003c/p\u003e\n\u003cp\u003eThis isn’t just about GenAI. It changes how we design, build, and operate systems when AI becomes part of the team.\u003c/p\u003e\n\u003ch3 id=\"code-is-no-longer-the-core-output\"\u003e\u003cstrong\u003eCode Is No Longer the Core Output\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eMost engineering teams still measure productivity by the amount of code shipped. But with AI-generated code, low-code platforms, and autonomous agents, raw output is no longer the right metric.\u003c/p\u003e","title":"Stop Coding, Start Coaching: Why It’s Time to Think Declaratively"},{"content":"Artificial Intelligence isn’t slowing down. In fact, it’s speeding up — reshaping how we work, how decisions are made, and how businesses compete. But while the technology keeps evolving, leadership often struggles to keep up. And that’s where the real risk lies.\nIf you’re in a leadership role today — especially in tech, strategy, or operations — you can’t afford to treat AI as a distant future trend or a side experiment. It’s already here. The question is: are you moving fast enough to lead with it?\nThe Acceleration Problem We’ve all seen it. Pilots turn into production in weeks instead of years. Tools like Copilot, ChatGPT, and fine-tuned enterprise models are reshaping everything from coding to compliance. Meanwhile, generative AI and agentic systems are moving from prototypes to business-critical services.\nAnd yet, most leadership teams still treat AI as a tech project. That’s a mistake. The companies pulling ahead are treating it as a business model shift. They’re not waiting for a perfect roadmap. They’re learning by doing — and adjusting as they go.\nYou Don’t Need All the Answers — You Need a Point of View The biggest blocker to progress isn’t lack of tech. It’s indecision.\nLeadership often waits for the full picture before acting. But with AI, the picture is always evolving. You need a clear stance on:\nWhere AI will impact your business most (cost, growth, productivity) What guardrails are non-negotiable (ethics, privacy, compliance) How to scale responsibly, even when things are uncertain That doesn’t mean guessing. It means building feedback loops fast. It means standing up use cases, measuring what works, and adjusting in short cycles.\nRethinking Roles and Responsibilities AI doesn’t just change what we build. It changes how we work. Teams need new skills. Decision-making needs to shift. Governance needs to move closer to the flow of development.\nHere’s the shift I see in organizations that are getting it right:\nCTOs move from platform focus to capability acceleration CIOs own not just systems, but AI-enabled workflows CMOs tap into AI for real-time personalization COOs rethink efficiency through AI-driven automation And all of this needs board-level support — not just funding, but active involvement in AI risk, oversight, and innovation culture.\nSpeed with Guardrails Fast doesn’t mean reckless. You need structure — especially around security, transparency, and trust. That’s where AI governance frameworks, MLOps, and data strategy come in.\nIf you don’t have:\nA responsible AI policy tied to real use cases A way to track AI model behavior post-deployment A plan for workforce re-skilling Then you’re not leading — you’re reacting.\nFinal Thought: Lead from the Front AI is not an IT initiative. It’s a leadership agenda. And the pace won’t slow down just because we’re not ready.\nThe organizations that win won’t be the ones with the most advanced models. They’ll be the ones with leaders who move fast, act with intent, and learn in public.\nIf you’re a leader, now’s the time to move.\n","permalink":"//localhost:1313/posts/2025-07-30-ai-moving-fast-leadership-faster/","summary":"\u003cp\u003eArtificial Intelligence isn’t slowing down. In fact, it’s speeding up — reshaping how we work, how decisions are made, and how businesses compete. But while the technology keeps evolving, leadership often struggles to keep up. And that’s where the real risk lies.\u003c/p\u003e\n\u003cp\u003eIf you’re in a leadership role today — especially in tech, strategy, or operations — you can’t afford to treat AI as a distant future trend or a side experiment. It’s already here. The question is: are you moving fast enough to lead with it?\u003c/p\u003e","title":"AI is Moving Fast. Leadership Must Move Faster"},{"content":"Every company wants to move fast with AI. The real challenge isn’t adoption—it’s making sure innovation is guided by responsibility and strong governance.\nThe question is not whether to adopt AI — it’s about doing it right. AI governance helps organizations find the balance between driving innovation and managing risk. It gives you the structure to build solutions that serve your business and your customers, without losing control.\nThe Innovation Challenge Speed matters. Companies that apply AI to predictive analytics, automation, and personalization are gaining an edge — whether it’s a hospital improving diagnosis or a bank detecting fraud in real time.\nBut moving fast without the right controls can backfire. Poor governance can lead to biased models, privacy issues, or black-box systems. The impact is real — for your business and your customers.\nAI needs to be deployed in a controlled way. The goal is to keep innovation moving while staying responsible.\nBuilding the Foundation: Data and Security Good AI starts with good data. Without it, you can’t trust the outcomes.\nThis means you need clear data governance. You must know where your data comes from, how it is processed, and who can access it. If your AI makes decisions that affect people, you must be able to explain those decisions — not just hope the model gets it right.\nSecurity is just as important. AI models often handle sensitive data. You need to secure both the data and the models to prevent misuse or attack.\nThe Human Element: Keeping People in the Loop AI should support people, not replace them. The best AI solutions I see always include a human in the loop — especially for decisions that can impact lives.\nThis doesn’t slow innovation down. It ensures that humans and AI work together effectively. For example, an AI can flag potential risks, but the final decision can remain with a person.\nTraining is also critical. Teams must understand both what AI can do — and where it can go wrong.\n![][image2]\nPractical Steps for Implementation Start with clear policies. Define how AI will be used, what the guardrails are, and how decisions will be monitored. Keep this practical, not theoretical.\nSet up regular audits to monitor for bias, accuracy, and compliance. AI needs ongoing review — it is not a one-time project.\nAnd bring the right people together: technical experts, business leaders, legal teams. AI governance is not an IT task — it is an organizational responsibility.\nManaging Risk Without Slowing Down Risk management doesn’t mean saying “no” to AI. It means understanding where more controls are needed, and where a lighter touch is fine.\nA chatbot answering basic questions does not need the same level of review as an AI deciding who gets a loan. Classify your AI projects by risk, and apply governance accordingly.\nThis lets you move fast where appropriate, and stay cautious where needed.\nThe Business Case for Good Governance Good governance builds trust — with customers, regulators, and employees. It reduces risk and improves outcomes.\nIt also makes your AI better. Clean data, clear processes, and regular monitoring all lead to better-performing AI systems.\n![][image3]\nLooking Ahead AI will keep evolving — fast. New capabilities, new regulations, new business expectations. You need a governance approach that adapts with this change.\nThe goal is not to slow innovation. It is to make sure innovation is responsible and sustainable.\nGetting Started Take the following steps to get started:\nBegin with your data. Build a clean, secure, well-documented data foundation. Develop practical governance policies that guide day-to-day AI decisions. Invest in training so your teams understand both AI’s power and its risks. Getting this balance right — innovation and responsibility — is what will set leading organizations apart. ","permalink":"//localhost:1313/posts/2025-07-29-ai-governance-balance-innovation-vs-risk/","summary":"\u003cp\u003eEvery company wants to move fast with AI. The real challenge isn’t adoption—it’s making sure innovation is guided by responsibility and strong governance.\u003c/p\u003e\n\u003cp\u003eThe question is not whether to adopt AI — it’s about doing it right. AI governance helps organizations find the balance between driving innovation and managing risk. It gives you the structure to build solutions that serve your business and your customers, without losing control.\u003c/p\u003e\n\u003ch3 id=\"the-innovation-challenge\"\u003e\u003cstrong\u003eThe Innovation Challenge\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eSpeed matters. Companies that apply AI to predictive analytics, automation, and personalization are gaining an edge — whether it’s a hospital improving diagnosis or a bank detecting fraud in real time.\u003c/p\u003e","title":"AI Governance: Balancing Innovation and Responsibility"},{"content":"Many organizations still treat AI like it’s experimental. A side project. A shiny object in a lab that only a few experts are allowed to touch.\nThat mindset is holding them back.\nIf you want AI to drive real impact, it needs to move out of the lab and into the engine room of your organization. That’s what the AI Factory is about. Not experiments, but execution. Not proofs of concept, but products.\nFrom Prototype to Production We’ve all seen it before: a promising AI model built by a small data science team — but no plan to scale it, no ownership after handover, and no integration into business processes.\nThe result? Shelfware.\nThe AI Factory fixes that by treating AI like a product, not a project. It’s a structured setup where teams can build, test, deploy, and improve AI use cases at scale — reliably and securely.\nIt’s not about the tech alone. It’s about standardizing the way you work:\nShared architecture patterns Reusable components and pipelines Embedded compliance and security Clear ownership across data, IT, and business Why It Works An AI Factory lowers the friction to deliver.\nInstead of every team reinventing the wheel, they build on a common platform. Think reusable pods for model training, deployment templates, or automated data validation tools.\nThat doesn’t just save time — it creates trust. You know what you’re building on. You know what’s allowed. You know it will run in production.\nAnd more importantly: you can measure outcomes and scale them.\nIt Changes the Culture Too This model isn’t just for data scientists or engineers. It enables everyone across the organization to participate:\nArchitects can design with AI in mind. Developers can embed models via APIs or SDKs. Business teams can identify new use cases without waiting on central IT. When done right, the AI Factory becomes a shared capability — not a silo.\nWhat It Takes to Get There If you’re setting this up, focus on three things:\nClear governance — who owns what, and how decisions get made. End-to-end tooling — not just model training, but data prep, versioning, monitoring, and retraining. Change leadership — because none of this works if teams still see AI as someone else’s problem. This is the hard part. But it’s also where the value is.\nFinal Thought The AI Factory isn’t a place where things get invented. It’s where they get built — again and again, better every time.\nIf you’re serious about becoming AI-first, start treating AI like a core business capability. Not a pilot. Not a lab.\nBut the engine room powering your future.\n","permalink":"//localhost:1313/posts/2025-07-25-ai-factory-engine-room-not-lab/","summary":"\u003cp\u003eMany organizations still treat AI like it’s experimental. A side project. A shiny object in a lab that only a few experts are allowed to touch.\u003cbr\u003e\nThat mindset is holding them back.\u003cbr\u003e\nIf you want AI to drive real impact, it needs to move out of the lab and into the engine room of your organization. That’s what the AI Factory is about. Not experiments, but execution. Not proofs of concept, but products.\u003c/p\u003e","title":"The AI Factory Is Not a Lab. It’s an Engine Room"},{"content":"AI and Web3 are often discussed as separate trends. But the way they’re starting to influence each other is where the real potential lies. For organizations building digital strategies, understanding how these technologies interact is becoming more important by the day.\nThis isn’t about chasing hype — it’s about practical ways these two technologies can address each other’s current limitations and open up new business models.\nWeb3 Can Help Solve AI’s Trust Problem Trust is one of the biggest blockers for AI adoption. How can we prove that an AI model works as intended? How do we know decisions haven’t been tampered with?\nWeb3 brings a useful capability here: transparency. Blockchain provides immutable records that can be used to track how AI models are performing and how decisions are made. This isn’t theory — it’s already being explored in industries where regulatory scrutiny is high.\nExample: If an AI system is making credit decisions or processing sensitive financial data, every step of that process can be recorded on-chain. This gives auditors and regulators an independent view — without relying on any one company to “explain” what happened.\nAI Is Making Web3 Usable Web3 platforms still have a significant usability challenge. Wallets, gas fees, transaction steps — most of this is too complex for the average user.\nAI is starting to simplify this. Natural language interfaces are reducing the learning curve. You’ll see more AI agents acting on behalf of users — handling transactions, optimizing fees, and interacting with smart contracts in the background.\nThere’s also value in applying AI to optimize the Web3 infrastructure itself. Predictive models can help manage network congestion and reduce transaction costs, making decentralized applications faster and more efficient.\nData Ownership: A Shift in the AI Model Today’s AI models are largely trained on data controlled by a small number of cloud platforms and big tech companies. Web3 flips this — enabling models where individuals own their data and can choose how it’s used.\nThis is an important shift. Blockchain-based identity and consent mechanisms can allow people to selectively share data with AI systems — and get compensated when they do. It also gives AI systems access to more diverse, high-quality, permissioned data.\nDecentralized AI marketplaces are emerging where data contributors, model builders, and end-users can interact without intermediaries taking the largest cut.\nNew Economic Models Are Emerging We’re also seeing early signs of new business models. AI models themselves can be tokenized — meaning developers can raise funding from a broader community, and token holders can share in future revenue.\nThis approach could open up AI innovation to smaller teams that can’t easily access traditional venture capital. It also better aligns incentives between those building AI systems and those using them.\nThere Are Still Big Technical Challenges Of course, there are limits.\nBlockchains aren’t optimized for high-speed AI inference — processing AI workloads fully on-chain is still impractical for most use cases. Privacy is a challenge. Blockchain’s transparency and AI’s need for confidential data don’t always align. Technologies like zero-knowledge proofs show promise, but they aren’t mature enough yet for broad deployment. The skills gap is very real. It’s hard enough to find AI talent or blockchain talent — finding expertise that bridges both is even harder. What Should Organizations Do? You don’t need to wait for all of this to mature before starting.\nThere are practical steps you can take now:\nUse blockchain to record and verify AI model outputs in regulated processes. Experiment with token-based incentives for data sharing — especially in environments where user trust is critical. Focus first on solving real business problems, not on implementing technology for its own sake. Partnerships will be key. Building everything in-house isn’t realistic for most organizations. Collaborating with players who have complementary strengths will accelerate your ability to explore these opportunities.\nThe Road Ahead As both AI and Web3 mature, their influence on each other will grow. We’ll move toward systems where AI agents can transact and negotiate autonomously via smart contracts. This will reshape ideas around digital ownership, IP rights, and compensation models for data and AI contributions.\nThe organizations that take the time to understand this shift — and experiment with it now — will be better positioned as these capabilities go mainstream.\nThis isn’t just about adding new features. It’s about building digital systems that are more transparent, user-friendly, and fair — and that can unlock new sources of value.\n","permalink":"//localhost:1313/posts/2025-07-12-how-ai-and-web3-influence-each-other/","summary":"\u003cp\u003eAI and Web3 are often discussed as separate trends. But the way they’re starting to influence each other is where the real potential lies. For organizations building digital strategies, understanding how these technologies interact is becoming more important by the day.\u003c/p\u003e\n\u003cp\u003eThis isn’t about chasing hype — it’s about practical ways these two technologies can address each other’s current limitations and open up new business models.\u003c/p\u003e\n\u003ch3 id=\"web3-can-help-solve-ais-trust-problem\"\u003eWeb3 Can Help Solve AI’s Trust Problem\u003c/h3\u003e\n\u003cp\u003eTrust is one of the biggest blockers for AI adoption. How can we prove that an AI model works as intended? How do we know decisions haven’t been tampered with?\u003c/p\u003e","title":"How AI and Web3 Influence Each Other"},{"content":"We’re entering the agent era. Is your team ready?\nLarge Language Models have already changed how we interact with data. But the rise of agentic AI — systems that reason, take action, and adapt — goes even further.\nThese agents don’t just answer questions. They perform tasks, orchestrate workflows, and make real-time decisions.\nThat changes everything for data \u0026amp; AI teams.\nIf you’re still focused on dashboards, isolated models, or batch pipelines, you’re building for the past.\nFrom models and pipelines to intelligent systems\nAgentic AI shifts the purpose of the work we’ve done for years.\nHere’s how typical tasks are evolving:\nWhat’s changing:\nFrom building data pipelines → to agents that orchestrate and generate workflows From predictive models → to agents that combine models with reasoning From dashboards → to agents that make or automate decisions From manual cataloging → to agents that explore and reason over metadata It doesn’t mean your work disappears.\nBut it needs a new goal: to enable AI systems that interact, learn, and deliver outcomes.\nWhat Needs to Change (Internally) Agentic AI forces a new operating model.\nHere’s how to structure your team for what’s coming next:\nTeams → From data silos to product pods Form cross-functional pods focused on capabilities, not temporary project scopes (e.g. “real-time pricing”, “intelligent service agents”, “AI-driven compliance automation”) This model works especially well for system integrators and consulting teams:\n- Align pods with industry-specific use cases like “agent-based claims processing” or “AI for smart grid operations”\n- Or build platform capabilities such as “LLMOps and Prompt Engineering” or “Vector Store \u0026amp; Retrieval Frameworks” Each pod owns a capability from architecture to deployment — including accelerators, reusable patterns, and governance The shift is from PoCs and slideware to repeatable, production-grade outcomes Tooling → From static pipelines to AI-native platforms Replace batch processing with streaming and event-driven architectures Adopt orchestration tools that support LLM chaining, vector search, memory, and tool use Build observability in: logging, auditing, and real-time feedback for autonomous agents Make data quality and governance operational:\n- Validate inputs continuously\n- Track lineage across agent workflows\n- Enforce access, retention, and fallback policies by design In an AI-first setup, governance isn’t a gate — it’s part of how intelligent systems run safely and reliably.\nMindset → From model accuracy to system outcomes Agentic AI isn’t just about prediction — it’s about behavior Prioritize task success, reliability, and continuous learning Learn to evaluate agents like you evaluate products: usage, satisfaction, and impact Ways of Working → Co-create with software and business Collaborate early with software engineers (infrastructure, APIs, delivery) Involve business teams from the start — defining tasks, testing behavior, owning value Align on clear business outcomes:\n- How many decisions are being automated?\n- How much time or cost is being saved?\n- Are customer actions improving as a result? AI performance isn’t about precision — it’s about how well it supports the business in real-world workflows.\nWhat Skills Will Matter To stay relevant, teams should start building strength in:\nData:\nReal-time pipelines Data products Embeddings and vector stores AI:\nPrompt design Agent orchestration LLM safety and evaluation Engineering:\nAPI-first architecture DevOps and CI/CD Event streaming Product:\nCapability framing Task definition Iterative feedback loops Governance:\nHuman-in-loop design Fallback logic Explainability and accountability Final Thought Agentic AI turns data \u0026amp; AI teams into designers of intelligent behavior.\nThis shift is big — but it’s also exciting.\nIt’s no longer about optimizing pipelines or fine-tuning a model.\nIt’s about building AI that thinks, acts, and learns — and making that work in production.\nAnd the teams that embrace this now… will lead what’s next.\n","permalink":"//localhost:1313/posts/2025-07-04-agentic-ai-reshaping-data-ai-teams/","summary":"\u003cp\u003eWe’re entering the agent era. Is your team ready?\u003c/p\u003e\n\u003cp\u003eLarge Language Models have already changed how we interact with data. But the rise of \u003cstrong\u003eagentic AI\u003c/strong\u003e — systems that reason, take action, and adapt — goes even further.\u003c/p\u003e\n\u003cp\u003eThese agents don’t just answer questions. They perform tasks, orchestrate workflows, and make real-time decisions.\u003cbr\u003e\nThat changes everything for data \u0026amp; AI teams.\u003cbr\u003e\nIf you’re still focused on dashboards, isolated models, or batch pipelines, you’re building for the past.\u003c/p\u003e","title":"How Agentic AI Will Reshape the Work of Data \u0026 AI Teams — and What to Do About It"},{"content":"The pace of technology change hasn’t slowed down. In fact, it’s accelerating. Every week, new solutions are pitched that promise to transform your business, disrupt your industry, or unlock new revenue streams.\nFor CxOs, this creates both opportunity and risk. It’s easy to get caught up in the latest buzzwords, but with budgets tighter and delivery pressure higher, making smart choices is critical.\nSo what’s real in 2025 — and what’s still more noise than substance? Here’s my radar view, based on what I’m seeing across industries.\nAI Agents and GenAI Platforms — REAL, but maturing AI is no longer theoretical. We see agents handling workflows, customer interactions, data analysis, and even coding tasks.\nHowever, most organizations are still experimenting with small-scale pilots. The market is fragmented, integration takes time, and governance is often an afterthought.\n👉 Advice: Invest in AI platforms that can scale across the business. Build a foundation for secure and governed AI adoption. But don’t overpromise outcomes — responsible rollout is key.\nSovereign and Industry Cloud — REAL Global tensions and new regulations are forcing companies to rethink data control. Sovereign cloud is no longer a compliance topic — it’s becoming a business imperative.\nEnergy, healthcare, public sector, and financial services are leading adoption. Expect to see more multi-cloud strategies where data and services must stay inside certain jurisdictions.\n👉 Advice: Prioritize cloud strategies that give you control over where and how data is stored and processed. Build flexibility to adapt to regulatory change.\nWeb3 and Decentralized Platforms — Still HYPE Web3 promised a revolution. In reality, most enterprise use cases remain niche or unproven. Blockchain is solid for certain scenarios (identity, transparency, traceability), but large-scale business transformation has yet to materialize.\n👉 Advice: Explore specific blockchain applications where they add value. But stay cautious about broader Web3 platform promises for now.\nQuantum Computing — Not Ready Yet Quantum breakthroughs are happening — in labs. But for mainstream business problems, practical quantum computing is still years away. Cloud providers are offering quantum simulation services, but production-ready quantum apps are rare.\n👉 Advice: Keep your innovation teams informed, but don’t allocate significant budget yet. This is a space to watch, not to build around today.\nData Fabric and Data Mesh — REAL Data chaos is one of the biggest barriers to AI and digital business. Here, Data Fabric and Data Mesh approaches are delivering results — making data products more reusable, governable, and accessible across the enterprise.\n👉 Advice: If you haven’t started modernizing your data architecture, now is the time. These patterns help break down silos and enable AI and advanced analytics at scale.\nAI Factories and Industrialized AI — REAL Running AI at scale requires more than models. AI factories — with automated pipelines for data, model management, governance, and deployment — are now key to delivering value repeatably.\n👉 Advice: Treat AI as an industrial process, not a series of one-off projects. Build an AI factory approach to make AI delivery faster, cheaper, and safer.\nAI Governance and Policy — REAL AND URGENT Many organizations underestimated the complexity of AI governance. In 2025, new AI regulations (EU AI Act, etc.) make this a board-level concern. The focus is shifting from can we do it to should we do it — and how do we prove it.\n👉 Advice: Embed AI governance into your AI delivery process now. Prepare for audits. AI that is not explainable or controllable will not scale.\nVector Databases — REAL Vector databases are becoming essential for GenAI and search-based AI use cases. They allow fast similarity search on unstructured data (text, images, audio), powering better AI experiences.\n👉 Advice: Evaluate vector database options as part of your modern data stack. They are a key enabler for RAG and AI-driven applications.\nSynthetic Data — EARLY, but moving to REAL Synthetic data is gaining traction where real data is limited, biased, or sensitive — for training AI models, improving test coverage, and meeting privacy constraints.\n👉 Advice: Explore synthetic data carefully — it is not a full substitute for high-quality real data, but it’s a powerful tool in specific scenarios.\nFinal Thought As a CxO, your job is to balance innovation with execution. Emerging tech can drive advantage — but only if applied with clear business purpose.\nIn 2025, AI, sovereign cloud, and data modernization are real levers for impact. Others, like Web3 and quantum, remain further out on the horizon.\nThe smartest organizations are combining ambition with discipline: building foundations today while staying ready for what’s next.\n","permalink":"//localhost:1313/posts/2025-06-11-tech-radar-cxo-real-vs-hype-2025/","summary":"\u003cp\u003eThe pace of technology change hasn’t slowed down. In fact, it’s accelerating. Every week, new solutions are pitched that promise to transform your business, disrupt your industry, or unlock new revenue streams.\u003c/p\u003e\n\u003cp\u003eFor CxOs, this creates both opportunity and risk. It’s easy to get caught up in the latest buzzwords, but with budgets tighter and delivery pressure higher, making smart choices is critical.\u003c/p\u003e\n\u003cp\u003eSo what’s real in 2025 — and what’s still more noise than substance? Here’s my radar view, based on what I’m seeing across industries.\u003c/p\u003e","title":"Emerging Tech Radar for CxOs: What’s Real and What’s Hype in 2025?"},{"content":"Automated Data Integration and the Future of ETL\nData is one of the most valuable assets for any organization. But turning raw data into useful insights is still a challenge. Traditional ETL (Extract, Transform, Load) processes are slow, rigid, and resource intensive. They require manual coding, monitoring, and maintenance. That’s not scalable in today’s data-driven world—especially with the speed and complexity of modern applications.\nThis is where generative AI steps in.\nA shift from manual to machine-driven\nETL has always been about moving data from different sources, transforming it into a usable format, and loading it into a system where it can be analyzed. But with the explosion of data, increased regulatory pressure, and the move to hybrid and multi-cloud environments, this process has become much more complex.\nGenerative AI is changing the game. Instead of writing and maintaining endless scripts and workflows, organizations can now use AI models to automate ETL pipelines. These models understand the context of the data, learn from existing integration patterns, and generate optimized workflows on the fly.\nThis leads to significant benefits:\nSpeed: AI can generate and update ETL logic in minutes, not days. Consistency: AI-driven pipelines are less prone to human error. Adaptability: They automatically adjust to schema changes or new data sources. Beyond automation: intelligent integration\nAI isn’t just speeding things up—it’s making data integration smarter. By applying natural language understanding, organizations can describe what they want in plain English, and the AI creates the integration pipeline.\nFor example, a demand planner can say: “Extract product inventory from Oracle, combine it with daily sales from Shopify, calculate stock turnover per SKU, and load it into Snowflake for reporting.” With traditional ETL, manual SQL logic, batch jobs, and schema mapping must be created. AI will generate the pipeline on demand from the prompt.\nThis approach democratizes data integration. It removes the dependency on specialized engineers for every change and helps more people in the organization work with data directly.\nHow this fits in a modern data strategy\nGenerative AI for ETL is a natural fit in environments where data fabrics or data mesh architectures are being implemented. Modern data strategies are shifting from centralized control to decentralized ownership. Concepts like data mesh and data fabric are driving this shift, giving teams more flexibility to manage, consume, and share data across systems. In these models, every domain owns its data products, but the organization still needs consistency, compliance, and efficiency at scale.\nSupporting Decentralization Without Losing Control\nIn a data mesh, teams manage their own pipelines. Traditional ETL tools can’t keep up with the constant change and complexity. AI-driven ETL supports this by giving each team a way to build and manage data flows independently—without starting from scratch or involving a central data engineering group every time.\nCross-Cloud Compatibility\nLeading platforms are already moving in this direction:\nGoogle Cloud: With services like BigQuery Dataform and Cloud Data Fusion, Google supports declarative and visual data pipeline development. Generative AI models from Google’s Vertex AI can integrate with these services to streamline data prep and transformation. AWS: Amazon’s Glue Studio offers low-code/no-code pipeline development, and new AI integrations allow users to describe what they want in natural language. Combined with SageMaker and Bedrock, AWS is aiming to simplify the entire data lifecycle—from ingestion to modeling. Microsoft Azure: Azure Data Factory and Synapse Analytics are embedding AI directly into pipeline creation and monitoring. With Microsoft Copilot, users can ask for transformations, lineage, and integration logic using natural language. Databricks: With its Lakehouse architecture, Databricks is adding AI to simplify pipeline generation in notebooks and workflows. Unity Catalog, when paired with LLMs, supports context-aware data discovery and security enforcement. Snowflake: Their growing suite of AI features, including Snowpark and Cortex, allows SQL and Python users to automate parts of the data prep process. With Snowflake’s native LLM support, the platform is well-positioned to offer AI-driven transformations at scale. Open-source \u0026amp; hybrid platforms: Tools like Apache Airflow, Dagster, and dbt are starting to explore AI plugins and extensions. These add automation and intelligence to open workflows, making it easier for developers to generate and maintain pipeline logic. What\u0026rsquo;s next?\nWe are moving toward a future where ETL as we know it may no longer exist. Instead, we’ll see dynamic data integration powered by AI. The concept of “ETL pipelines” will be replaced by intelligent agents that continuously ingest, transform, and validate data in real time, guided by policies and context, not hardcoded rules.\nFor organizations, this means that by embedding generative AI into ETL processes across platforms:\nTime to value shortens: Data products go live faster, helping teams act quickly. Complexity reduces: AI handles edge cases, schema drift, and exception handling in real time. Data quality improves: Built-in rules and real-time validation become part of the generated logic. Business access increases: More users across domains can work with data confidently, without needing to be engineers. This isn’t just a technological shift. It’s a change in how we approach data—moving from pipelines built manually to systems that can learn, generate, and adapt automatically.\nRemember that AI isn\u0026rsquo;t replacing data engineers—it\u0026rsquo;s changing their role. The most successful organizations are those that help their teams adapt to becoming orchestrators and quality managers rather than code writers.\n","permalink":"//localhost:1313/posts/2025-04-17-automated-data-integration-future-etl/","summary":"\u003cp\u003e\u003cstrong\u003eAutomated Data Integration and the Future of ETL\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eData is one of the most valuable assets for any organization. But turning raw data into useful insights is still a challenge. Traditional ETL (Extract, Transform, Load) processes are slow, rigid, and resource intensive. They require manual coding, monitoring, and maintenance. That’s not scalable in today’s data-driven world—especially with the speed and complexity of modern applications.\u003c/p\u003e\n\u003cp\u003eThis is where generative AI steps in.\u003c/p\u003e","title":"Ø ETL - Automated Data Integration and the Future of ETL"},{"content":"In today’s data and AI era, organizations are facing a lot of complexities to become data driven. They are facing issues such as data silos, lack of collaboration between business and IT, high maintenance costs, increasing data privacy concerns and governance needs.\nHowever, in today’s digital economy, data is the new oil. For organizations to remain competitive, relevant, and capable of rapid innovation, it is key to effectively use this extremely valuable resource. It helps to better understand the market and the competition, it improves processes, it helps to give more insights into performance, and ultimately make better decisions. Data is a vital asset that shapes business strategies, drives innovation, and provides a competitive edge in today\u0026rsquo;s dynamic economy.\nThe need for data fabric\nA data fabric is a unified platform that integrates, manages, and secures data across various environments, such as on-premises, cloud, and hybrid environments. It is also capable of integrating structured, semi-structured, and unstructured data. It provides a consistent and scalable architecture, which can support organizations to get the highly needed insights in their data.\nFoundation layer\nData nowadays is everywhere, and the idea of moving all data to one single of truth is, is becoming less relevant in today\u0026rsquo;s complex data environments. Today’s data architectures consist of distributed systems, multi-cloud environments, and real-time processing across various sources. This makes it nearly impossible to centralize everything into one data warehouse or other type of data repository.\nThe foundation layer of a data fabric is built on metadata—the data about data. This layer integrates and connects the diverse data sources by using metadata to automate the process of data discovery, integration, and governance. For this it uses data virtualization to access data in real-time without needing to physically move or copy it.\nThis metadata layer makes the data fabric \u0026ldquo;intelligent,\u0026rdquo; allowing to manage the distributed data environments more efficiently. And the data is instantly accessible for developing new products and services.\nComposable Data Products\nData fabric allows the creation of \u0026ldquo;data products,\u0026rdquo; which are reusable, modular, and governed datasets or services. These products can be mixed and matched to create new business capabilities or services without needing to rebuild them from scratch. By using these existing data products, organizations can create new business capabilities or services more quickly. This will accelerate the innovation process, enabling businesses to respond rapidly to market demands.\nData Democratization \u0026amp; Self-Service\nBy making data more accessible across the organization, and empowering teams to create their own data products or run their own analyses, IT is not needed to manage everything. This democratization enables innovation because teams can experiment and iterate faster. Users from various departments can access data easily and cross-functional collaboration is improved. Developers and data scientists can experiment with new models and prototypes more easily which will ultimately lead to more innovative ideas and solutions.\nAI integration and innovation\nA data fabric can work with the native data as-is. It can handle various data structures, such as graphs, rows and columns, and lists. This enables AI to work seamlessly with all these types of records. With strong governance and lineage implemented, it can provide reliable data for training AI/ML models, as well as using it for generative AI solutions. Organizations can innovate in areas like predictive analytics, personalized recommendations, and automation by leveraging broader datasets in a secure and scalable way.\nThe metadata-driven approach can also be enhanced with AI/ML to automate data integration, quality management, and even anomaly detection. By automating repetitive tasks using AI, organizations can focus on more high-value business innovations.\nThe transformative power of data fabric\nData fabric is transforming how organizations view and manage their data landscapes. By leveraging metadata and integrating AI-driven solutions, organizations can create a flexible, responsive, and innovative data environment. This is essential for gaining insights and developing new capabilities, leading to a competitive advantage.\nAdditionally, data democratization empowers teams to create a culture of innovation and agility, where every member can contribute to the organization\u0026rsquo;s success. As data volumes and complexities increase over time, adopting data fabric will be key for addressing the current challenges organizations face and for creating new opportunities.\nStart innovating now\nBuild a Unified Data Fabric\nStart implementing a data fabric that integrates and manages data across all environments—cloud, on-premises, and hybrid. With real-time data access and seamless connectivity, you eliminate silos and unlock new possibilities for rapid insights and product development.\nCreate Reusable Data Products\nAccelerate innovation by transforming your data into composable, reusable products. These modular datasets can be easily combined to develop new services and capabilities—no need to start from scratch. Faster iteration, faster results.\nEmpower Teams with Data\nMake data accessible to everyone. By democratizing data and enabling self-service, you allow teams to experiment, build, and innovate without waiting on IT. This boosts agility and fosters a culture of continuous, business-driven innovation.\n","permalink":"//localhost:1313/posts/2024-10-17-the-future-of-data-fabric/","summary":"\u003cp\u003eIn today’s data and AI era, organizations are facing a lot of complexities to become data driven. They are facing issues such as data silos, lack of collaboration between business and IT, high maintenance costs, increasing data privacy concerns and governance needs.\u003c/p\u003e\n\u003cp\u003eHowever, in today’s digital economy, data is the new oil. For organizations to remain competitive, relevant, and capable of rapid innovation, it is key to effectively use this extremely valuable resource. It helps to better understand the market and the competition, it improves processes, it helps to give more insights into performance, and ultimately make better decisions. Data is a vital asset that shapes business strategies, drives innovation, and provides a competitive edge in today\u0026rsquo;s dynamic economy.\u003c/p\u003e","title":"The Future of Data Fabric"},{"content":"Digital organisations work in new ways that require us all to adapt and develop new skills and framing and making sense of the world. How to tackle this is one of the more difficult challenges requiring addressing in a digital transformation.\nDave, Sjoukje \u0026amp; Rob talk with Micheal Hamman, Teacher and Coach, about different styles of organisation and about how our beliefs can impact how we act. He also talks about the over emphasis on the engineering mindset, how we need to consider the human technologies and how all of our \u0026quot;human operating systems\u0026quot; may require some new apps.\nFinally, in this weeksTrend, we talk about skills that modern leaders need to adopt and what can we learn from each other.\nTLDR:\n01:14 Intros\n02:02 Cloud conversation with Micheal Hamman\n24:05 Three skills every modern leader needs\n33:19 Vertical Facilitation!\n","permalink":"//localhost:1313/posts/2023-04-27-human-operating-systems-with-michael-hamman-teache/","summary":"\u003cp\u003eDigital organisations work in new ways that require us all to adapt and develop new skills and framing and making sense of the world.  How to tackle this is one of the more difficult challenges requiring addressing in a digital transformation.\u003c/p\u003e\n\u003cp\u003eDave, Sjoukje \u0026amp; Rob talk with Micheal Hamman, Teacher and Coach, about different styles of organisation and about how our beliefs can impact how we act.  He also talks about the over emphasis on the engineering mindset, how we need to consider the human technologies and how all of our \u0026quot;human operating systems\u0026quot; may require some new apps.\u003c/p\u003e","title":"Human operating systems with Michael Hamman, Teacher \u0026 Coach"},{"content":"Conversations around the short and long term risks and potentially significant unintended consequences of AI are increasing in volume. This has culminated recently in a controversial open letter, coordinated by Future of Life institute, from hundreds of leading figures in this space, including Elon Musk, asking for development and AI learning to be paused, until it is better understood. “Advanced AI could represent a profound change in the history of life on Earth, and should be planned for and managed with commensurate care and resources,”the letter says.“Unfortunately, this level of planning and management is not happening, even though recent months have seen AI labs locked in an out-of-control race to develop and deploy ever more powerful digital minds that no one – not even their creators – can understand, predict, or reliably control.”\nDave, Sjoukje \u0026amp; Rob talk with Futurist Theo Preistley about the risks of AI already manifesting in society, what additional features and automations are coming, which potentially add more risk, and what can could and should be done at this point to ensure we bring AI into our lives responsibly.\nFinally, in this weeksTrend, we talk about what organisations should do to prep for AI, and that fact that they still have time to act now to get the right arrangements in place.\nTLDR:\n01:15 Intros\n02:08 Cloud conversation with Theo Preistley\n38:25 What should organisations do to prep for AI? 45:27 The Evil Dead Rise and rewatch Picard!\nFurther Reading:\nhttps://www.theguardian.com/technology/2023/mar/31/ai-research-pause-elon-musk-chatgpt\n","permalink":"//localhost:1313/posts/2023-04-27-the-problem-with-ai-with-theo-priestley-futurist/","summary":"\u003cp\u003eConversations around the short and long term risks and potentially significant unintended consequences of AI are increasing in volume.  This has culminated recently in a controversial open letter, coordinated by Future of Life institute, from hundreds of leading figures in this space, including Elon Musk, asking for development and AI learning to be paused, until it is better understood.  \u003cem\u003e“Advanced AI could represent a profound change in the history of life on Earth, and should be planned for and managed with commensurate care and resources,”\u003cem\u003ethe letter says.\u003c/em\u003e“Unfortunately, this level of planning and management is not happening, even though recent months have seen AI labs locked in an out-of-control race to develop and deploy ever more powerful digital minds that no one – not even their creators – can understand, predict, or reliably control.”\u003c/em\u003e\u003c/p\u003e","title":"The problem with AI with Theo Priestley, Futurist"},{"content":"Digitalisation in an inherently complex activity, with unknowns being very high and experimentation required on an ongoing basis - there is no \u0026lsquo;once and done\u0026rsquo;. The changes in leadership framing, decision making and ways of working required are hugely under estimated in discussions and planning of \u0026rsquo;transformation\u0026rsquo; and failure to engage with that could risk your success.\nDave, Sjoukje \u0026amp; Rob talk with Dave Snowden, Director and Founder of the Cynefin Centre about his work on making sense of complexity, they discuss Dave\u0026rsquo;s seminal work, the Cynefin Framework, how he has subsequently built on that thinking and how you apply that it in the digitalisation process of your organisation. His insights are not to be missed.\nFinally, in this weeksTrend, we set out some perceived wisdom of what is required in digital leadership and see what stacks up.\nTLDR:\n00:43 Intros\n01:21 Cloud conversation with Dave Snowden\n24:53 Six Tips For CEOs for Leading Digital Transformations 40:31 Wainwright walks!\nFurther Reading:\nhttps://thecynefin.co/\n","permalink":"//localhost:1313/posts/2023-04-13-making-sense-of-digital-complexity-with-dave-snowd/","summary":"\u003cp\u003eDigitalisation in an inherently complex activity, with unknowns being very high and experimentation required on an ongoing basis - there is no \u0026lsquo;once and done\u0026rsquo;.  The changes in leadership framing, decision making and ways of working required are hugely under estimated in discussions and planning of \u0026rsquo;transformation\u0026rsquo; and failure to engage with that could risk your success.\u003c/p\u003e\n\u003cp\u003eDave, Sjoukje \u0026amp; Rob talk with Dave Snowden, Director and Founder of the Cynefin Centre about his work on making sense of complexity, they discuss Dave\u0026rsquo;s seminal work, the Cynefin Framework, how he has subsequently built on that thinking and how you apply that it in the digitalisation process of your organisation.  His insights are not to be missed.\u003c/p\u003e","title":"Making sense of digital complexity with Dave Snowden, The Cynefin Centre"},{"content":"AI usability and adoption continues to rapidly accelerate in 2023, but it is important to understand the risks to your business from mis-using AI and best practices for evaluating the right AI tools for the right jobs.\nDave, Sjoukje \u0026amp; Rob talk with Michelle Zhou, Founder and CEO of Juji, about her history and take on the progression of AI over the last 20 years, why democratisation of AI is both increases the safety and precision of AI, as well as likely driving the right and best human experiences. Also, how do you nurture your AI?!\nFinally, in this weeksTrend, we deep dive on democratised AI by looking at no code AI.\nTLDR:\n01:00 Intros\n02:26 Cloud conversation with Michelle Zhou\n32:18 No-Code AI\n41:54 AI Conference!\n","permalink":"//localhost:1313/posts/2023-04-06-democratising-and-nurturing-ai-with-michelle-zhou-/","summary":"\u003cp\u003eAI usability and adoption continues to rapidly accelerate in 2023, but it is important to understand the risks to your business from mis-using AI and best practices for evaluating the right AI tools for the right jobs.\u003c/p\u003e\n\u003cp\u003eDave, Sjoukje \u0026amp; Rob talk with Michelle Zhou, Founder and CEO of Juji, about her history and take on the progression of AI over the last 20 years, why democratisation of AI is both increases the safety and precision of AI, as well as likely driving the right and best human experiences.  Also, how do you nurture your AI?!\u003c/p\u003e","title":"Democratising and nurturing AI with Michelle Zhou, Juji Inc"},{"content":"Platform engineering is a core part of modern digital operating models. The discipline has been developing over the last 10 years and provides underlying safe, secure and optimised platforms, essential for creative and customer centric innovation.\nDave, Sjoukje \u0026amp; Rob talk with Jennifer Riggins, Tech Culture Author about what good platform engineering looks like, why conditions today could lead to good platform engineering practices, how platforms help organisations innovate and deal with technical complexity, how it leads to strong DevOps implementation and how it helps deal with a major issue in tech today, developer cognitive overload.\nTLDR:\n00:54 Intros\n02:16 Cloud conversation with Jennifer Riggins\n22:24 Platform engineering in 2023 - doing more with less 32:19 SPRING AT LAST!\n","permalink":"//localhost:1313/posts/2023-03-30-the-state-of-platform-engineering-with-jennifer-ri/","summary":"\u003cp\u003ePlatform engineering is a core part of modern digital operating models.  The discipline has been developing over the last 10 years and provides underlying safe, secure and optimised platforms, essential for creative and customer centric innovation.\u003c/p\u003e\n\u003cp\u003eDave, Sjoukje \u0026amp; Rob talk with Jennifer Riggins, Tech Culture Author about what good platform engineering looks like, why conditions today could lead to good platform engineering practices, how platforms help organisations innovate and deal with technical complexity, how it leads to strong DevOps implementation and how it helps deal with a major issue in tech today, developer cognitive overload.\u003c/p\u003e","title":"The state of platform engineering with Jennifer Riggins, Tech Culture Journalist"},{"content":"Legacy modernisation is one of the most difficult, and often misunderstood, elements of digital transformation. It can be expensive and very disruptive. It has more in common with business transformation than cloud migration and can lend itself to risky large scale implementations.\nDave, Sjoukje \u0026amp; Rob are joined by John Kodumal, CTO and Co-Founder of LaunchDarkly, they talk about how greenfield development is increasingly rare, evolving a running system is much more common. New method and tools are now available to move this past a static or quarterly update, risk heavy process, such as feature management which allows legacy to be modernised with zero outage and switch over risk.\nIn this weeks trend we talk about the emerging field of developer productivity, why it can be helpful, but its also very difficult to do effectively.\nTLDR:\n00:50 Intros\n01:30 Cloud conversation with John Kodumal\n21:30 Developer Productivity\n29:42 Scaling People by Claire Hughes Johnson (book recommendation)!\nFurther Reading:\nScaling People: https://amzn.eu/d/af6XVeN\n","permalink":"//localhost:1313/posts/2023-03-23-modernising-software-systems-with-john-kodumal-lau/","summary":"\u003cp\u003eLegacy modernisation is one of the most difficult, and often misunderstood, elements of digital transformation.  It can be expensive and very disruptive.  It has more in common with business transformation than cloud migration and can lend itself to risky large scale implementations.\u003c/p\u003e\n\u003cp\u003eDave, Sjoukje \u0026amp; Rob are joined by John Kodumal, CTO and Co-Founder of LaunchDarkly, they talk about how greenfield development is increasingly rare, evolving a running system is much more common.  New method and tools are now available to move this past a static or quarterly update, risk heavy process, such as feature management which allows legacy to be modernised with zero outage and switch over risk.\u003c/p\u003e","title":"Modernising software systems with John Kodumal, LaunchDarkly"},{"content":"\u0026quot;Digital transformation\u0026quot; a phrase that has been used for years, often thought to be overused and perhaps hollow, maybe coming of age. This week we look into this phrase and what its meant over the years, but also try to understand what\u0026rsquo;s changed to make it more material.\nDave, Sjoukje \u0026amp; Rob talk with Jeff DeVerter, Tech Evangelist from Rackspace about what has made this real in 2023 and what is different organisationally, culturally and technically in a \u0026quot;digitally transformed\u0026quot; organisation. Also, they try to understand the difference between digitisation and digitalisation, how this may change professions and how this type of transformation can help us become more customer relevant.\nFinally, in this weeksTrend, we talk about cloud enabled digital transformation and how cloud fits in.\nTLDR:\n01:24 Intros\n02:19 Cloud conversation with Jeff Daverter\n22:30 Why Cloud is key in enabling digital transformation 31:40 Going for a nap!\n","permalink":"//localhost:1313/posts/2023-03-16-digital-transformation-is-now-a-reality-with-jeff/","summary":"\u003cp\u003e\u0026quot;Digital transformation\u0026quot; a phrase that has been used for years, often thought to be overused and perhaps hollow, maybe coming of age.  This week we look into this phrase and what its meant over the years, but also try to understand what\u0026rsquo;s changed to make it more material.\u003c/p\u003e\n\u003cp\u003eDave, Sjoukje \u0026amp; Rob talk with Jeff DeVerter, Tech Evangelist from Rackspace about what has made this real in 2023 and what is different organisationally, culturally and technically in a \u0026quot;digitally transformed\u0026quot; organisation.  Also, they try to understand the difference between digitisation and digitalisation, how this may change professions and how this type of transformation can help us become more customer relevant.\u003c/p\u003e","title":"\"Digital Transformation\" is now a reality with Jeff DeVerter, Rackspace"},{"content":"TelCo is a Sector where Cloud based software defined networks are driving customer expectations combined with potentially providing huge operational efficiencies. Telcos are increasingly moving their networks to the cloud, recognizing the significant advantages this can provide – 31% of global network capacity is being serviced by cloud today, and this is expected to increase to 46% in the next 3 to 5 years.\nDave, Sjoukje \u0026amp; Rob talk with Geoff Hollingworth, CMO of Rakuten Symphony about their experience as a Cloud Native Telco, and what we can learn by looking at what is possible on the other side of cloud transformation. The team explore what a telco cloud transformation is (is it different from cloud transformation in other sectors?), the operational differences between a Cloud Native and traditional Telco and cloud transformation progress and nuances in the sector\nFinally, in this weeksTrend, we talk about the FinOps Foundation and the misconception of FinOps being just about saving money.\nTLDR:\n00:52 Intros\n02:07 Cloud conversation with Geoff Hollingworth\n35:30 The FinOps Foundation 39:51 Getting some sleep!\nFurther Reading:\nhttps://www.capgemini.com/insights/research-library/cloudification-of-networks\n","permalink":"//localhost:1313/posts/2023-03-09-the-cloudification-of-telco-s-with-geoff-hollingwo/","summary":"\u003cp\u003eTelCo is a Sector where Cloud based software defined networks are driving customer expectations combined with potentially providing huge operational efficiencies.  Telcos are increasingly moving their networks to the cloud, recognizing the significant advantages this can provide – 31% of global network capacity is being serviced by cloud today, and this is expected to increase to 46% in the next 3 to 5 years.\u003c/p\u003e\n\u003cp\u003eDave, Sjoukje \u0026amp; Rob talk with Geoff Hollingworth, CMO of Rakuten Symphony about their experience as a Cloud Native Telco, and what we can learn by looking at what is possible on the other side of cloud transformation.  The team explore what a telco cloud transformation is (is it different from cloud transformation in other sectors?), the operational differences between a Cloud Native and traditional Telco and cloud transformation progress and nuances in the sector\u003c/p\u003e","title":"The cloudification of TelCo's with Geoff Hollingworth, Rakuten Symphony"},{"content":"Each year Capgemini publishes TechnoVision - a comprehensive prediction of the year to come, along with useful principles that guided both its prediction and use. It seeks to bring some fresh thinking to the technology business issues of today. In this special two part episode Dave, Sjoukje \u0026amp; Rob talk in detail with the reports leader Ron Tolido and get a special insight into the creation of the work.\nIn this special second episode, the team explore the mindset at the heart of Technovision 2023, helpful to understand the implications of the trends from pt.1, but critical in how you put them to practical use! They walk through the design principles (Every Business is a Tech Business, Do good, do less, do well, With Open Arms, IQ/EQ/CQ, Trust Thrust and No Hands on Deck) and discuss how they come together to paint a picture of a modern digital business.\nFinally, in this weeks Trend, we talk about how code itself can be written to ensure the best and most green performance of your products and platforms.\nTLDR:\n01:23 Intros\n01:51 Cloud conversation with Ron Tolido\n30:31 The rise of green application development \u0026amp; modernisation\n37:48 Re-writing my emails in the style of Earnest Hemmingway!\nFurther Reading:\nhttps://www.capgemini.com/insights/research-library/technovision-2023\n","permalink":"//localhost:1313/posts/2023-03-02-technovision-2023-pt-2-doing-less-with-less-to-dri/","summary":"\u003cp\u003eEach year Capgemini publishes TechnoVision - a comprehensive prediction of the year to come, along with useful principles that guided both its prediction and use.  It seeks to bring some fresh thinking to the technology business issues of today.  In this special two part episode Dave, Sjoukje \u0026amp; Rob talk in detail with the reports leader Ron Tolido and get a special insight into the creation of the work.\u003c/p\u003e\n\u003cp\u003eIn this special second episode, the team explore the mindset at the heart of Technovision 2023, helpful to understand the implications of the trends from pt.1, but critical in how you put them to practical use!  They walk through the design principles (Every Business is a Tech Business, Do good, do less, do well, With Open Arms, IQ/EQ/CQ, Trust Thrust and No Hands on Deck) and discuss how they come together to paint a picture of a modern digital business.\u003c/p\u003e","title":"TechnoVision 2023 pt.2: Doing less with less to drive sustainable digital value with Ron Tolido, Capgemini"},{"content":"Each year Capgemini publishes TechnoVision - a comprehensive prediction of the year to come, along with useful principles that guide both its prediction and use. It seeks to bring some fresh thinking to the technology business issues of today. In this special two part episode Dave, Sjoukje \u0026amp; Rob talk in detail with the reports leader Ron Tolido and get a special insight into the creation of Technovision 2023!\nIn this special first episode, the team explore why Scarcity, Sustainability and Uncertainty have influenced so much of the thinking, what impact this has and then walk through the 5 trends at the centre of the report: Invisible Infostructure, Applications Unleashed, Thriving on Data, Process on the Fly and You Experience/We Collaborate.\nFinally, in this weeksTrend, we talk about how Microsoft and Google are both racing to bring chatGPT supported product to market.\nTLDR:\n01:25 Intros\n02:00 Cloud conversation with Ron Tolido\n45:04 Microsoft and Googles move to incorporate chatGPT into their tooling\n51:09 Surf rock with robots!\n","permalink":"//localhost:1313/posts/2023-02-24-technovision-2023-pt-1-dealing-with-scarcity-susta/","summary":"\u003cp\u003eEach year Capgemini publishes TechnoVision - a comprehensive prediction of the year to come, along with useful principles that guide both its prediction and use.  It seeks to bring some fresh thinking to the technology business issues of today.  In this special two part episode Dave, Sjoukje \u0026amp; Rob talk in detail with the reports leader Ron Tolido and get a special insight into the creation of Technovision 2023!\u003c/p\u003e\n\u003cp\u003eIn this special first episode, the team explore why Scarcity, Sustainability and Uncertainty have influenced so much of the thinking, what impact this has and then walk through the 5 trends at the centre of the report: Invisible Infostructure, Applications Unleashed, Thriving on Data, Process on the Fly and You Experience/We Collaborate.\u003c/p\u003e","title":"TechnoVision 2023 pt.1: Dealing with scarcity, sustainability \u0026amp; uncertainty with Ron Tolido"},{"content":"Artificial intelligence seems to be being talked about everywhere this year, but is really being leveraged as much as that suggests, and do we really understand what it is, its limitations and its dangers?\nDave, Sjoukje \u0026amp; Rob are joined by Tom Godden, Director of Enterprise Strategy at AWS about the uptake of AI, Machine Learning and Deep Learning in businesses today. They also set out some definitions and how each of the technologies can be effectively used. Finally, they talk about some real case studies, including pizza cooking and delivery optimisation, a subject close to all of our hearts.\nTLDR:\n01:43 Intros\n03:01 Cloud conversation with Tom Godden\n32:10 Five predications for AI in 2023\n40:30 Next level technology futures presentations at AWS 2023 events!\n","permalink":"//localhost:1313/posts/2023-02-16-building-a-smarter-organisation-with-machine-learn/","summary":"\u003cp\u003eArtificial intelligence seems to be being talked about everywhere this year, but is really being leveraged as much as that suggests, and do we really understand what it is, its limitations and its dangers?\u003c/p\u003e\n\u003cp\u003eDave, Sjoukje \u0026amp; Rob are joined by Tom Godden, Director of Enterprise Strategy at AWS about the uptake of AI, Machine Learning and Deep Learning in businesses today.  They also set out some definitions and how each of the technologies can be effectively used.  Finally, they talk about some real case studies, including pizza cooking and delivery optimisation, a subject close to all of our hearts.\u003c/p\u003e","title":"Building a smarter organisation with Machine Learning with Tom Godden"},{"content":"Modern ways of working require significant engagement from everyone involved to ensure the right things are getting done and the right time and the levels of creativity are sustained for innovation. How do we get the balance right?\nDave, Sjoukje \u0026amp; Rob are joined by Duena Blomstrom, CEO of PeopleNotTech, author and influencer to discuss what drives a teams performance is it process, technology, method, or does it come down to things that are actually far more human? The team discuss why Psychological Safety remains a critical element in driving outcomes from modern teams and get an insight into Duenas thinking on \u0026quot;Human Debt\u0026quot;. Finally, the team discuss the other elements that make up modern high performing teams.\nTLDR:\n00:38 Intros\n00:52 Cloud conversation with Duena Blomstrom\n27:00 What makes up High Performing Teams\n31:30 I just want to sleep \u0026rsquo;till 05:30!\nFurther Reading:\nGoogle Paper: https://rework.withgoogle.com/print/guides/5721312655835136/ Book: People Before Tech https://amzn.eu/d/aE2t0DX Duenas new book is out later this year ","permalink":"//localhost:1313/posts/2023-02-09-criticality-of-psychological-safety-in-digital-tra/","summary":"\u003cp\u003eModern ways of working require significant engagement from everyone involved to ensure the right things are getting done and the right time and the levels of creativity are sustained for innovation.  How do we get the balance right?\u003c/p\u003e\n\u003cp\u003eDave, Sjoukje \u0026amp; Rob are joined by Duena Blomstrom, CEO of PeopleNotTech, author and influencer to discuss what drives a teams performance is it process, technology, method, or does it come down to things that are actually far more human?  The team discuss why Psychological Safety remains a critical element in driving outcomes from modern teams and get an insight into Duenas thinking on \u0026quot;Human Debt\u0026quot;. Finally, the team discuss the other elements that make up modern high performing teams.\u003c/p\u003e","title":"Criticality of Psychological Safety in Digital Transformation with Duena Blomstrom"},{"content":"For some special Christmas food for thought, we focus on the far reaches of our “Realities”. Here on the show we believe that interventional, courageous leadership combined with cloud can change the future reality of a business or human. But, what is real? What is not? And does it matter?!\nWe look at the thinking currently happening in the space of the Simulation Hypothesis, which suggests that we likely live in a computer simulation, and ask how it can be applied to solve “better problems” and improve our innovation ambition.\nDave and Sjoukje are reunited with re:Invent Roving Reporter Rob Kernahan, who’s finally back from Vegas, and they talk to Anders Indset, Philosopher, Author and renowned speaker to get his insights following some recent research he has been participating in, in this field, as well as his thinking on The Quantum Economy (links below).\nFinally, in tech trends we talk about the EUs recent use of the Metaverse to engage Gen Z in the work they do. It was ill attended sadly, so the team speculate on what might have happened.\nTLDR:\n01:36 Intros\n02:14 Cloud conversation with Anders Indset\nEU in the Metaverse\nChristmas with family and a Quantum Economy trilogy!\nFurther Reading: Do we live in a quantum simulation? Constraints, observations, and experiments on the simulation hypothesis (Anders Inset, Florian Neukart, Markus Pflitsch and Michael Perelshtein)\nhttps://arxiv.org/abs/2212.04921\nThe Quantum Economy: Saving the Mensch with Humanistic Capitalism\nAnders Indset: https://www.amazon.co.uk/Quantum-Economy-Saving-Humanistic-Capitalism-ebook/dp/B08MKLJJBN/ref=sr_1_1?crid=3FWPRKK4C9030\u0026amp;keywords=the+quantum+economy\u0026amp;qid=1671651039\u0026amp;sprefix=the+quantum+economy%2Caps%2C60\u0026amp;sr=8-1\nAdditional reading referenced in the show:\nhttps://www.scientificamerican.com/article/confirmed-we-live-in-a-simulation/ https://futurism.com/sorry-elon-physicists-say-we-definitely-arent-living-in-a-computer-simulation https://www.pbs.org/wgbh/nova/article/physicists-confirm-that-were-not-living-in-a-computer-simulation/ https://www.independent.co.uk/tech/simulation-theory-elon-musk-pong-matrix-b1967844.html https://bigthink.com/thinking/why-the-simulation-hypothesis-is-pseudoscience/ ","permalink":"//localhost:1313/posts/2022-12-22-christmas-special-are-we-living-in-a-simulation-an/","summary":"\u003cp\u003eFor some special Christmas food for thought, we focus on the far reaches of our “Realities”.  Here on the show we believe that interventional, courageous leadership combined with cloud can change the future reality of a business or human.  But, what is real? What is not? And does it matter?!\u003c/p\u003e\n\u003cp\u003eWe look at the thinking currently happening in the space of the Simulation Hypothesis, which suggests that we likely live in a computer simulation, and ask how it can be applied to solve “better problems” and improve our innovation ambition.\u003c/p\u003e","title":"Christmas Special! Are we living in a simulation, and does it matter? with Anders Indset"},{"content":"The healthcare care industry has been disrupted, with new entrants to the industry and tech, such as wearables, providing consumers with far more information and options, huge change is occurring. How are healthcare providers responding to their customers’ expectations and health needs?\nDave and Sjoukje talk to Chief Digital \u0026amp; Information Officer of Baptist Health, Tony Ambrozie to hear about how they are using cloud to respond and transform their organisation, and how a step wise approach to cloud now sees them 70% migrated and well on the way to using tech to proactively improve patient health.\nFinally, in tech trends, they talk about how UK Government is using an innovative AI incentive scheme to help tackle decarbonisation.\nTLDR:\n01:06 Intros\n01:48 Cloud conversation with Tony Ambrozie\n20:54 AI for Decarbonisation\n23:25 Improving patient health!\n","permalink":"//localhost:1313/posts/2022-12-15-digitalisation-in-healthcare-with-tony-ambrozie/","summary":"\u003cp\u003eThe healthcare care industry has been disrupted, with new entrants to the industry and tech, such as wearables, providing consumers with far more information and options, huge change is occurring.  How are healthcare providers responding to their customers’ expectations and health needs?\u003c/p\u003e\n\u003cp\u003eDave and Sjoukje talk to Chief Digital \u0026amp; Information Officer of Baptist Health, Tony Ambrozie to hear about how they are using cloud to respond and transform their organisation, and how a step wise approach to cloud now sees them 70% migrated and well on the way to using tech to proactively improve patient health.\u003c/p\u003e","title":"Digitalisation in healthcare with Tony Ambrozie"},{"content":"The marketing industry has been at the spear head of digitalisation, both in how its customers are changing their expectations and in how marketing professionals work on a day to day basis, increasing both scale of reach and the ability to create meaningful experiences.\nDave and Sjoukje talk to the Founder of Rooster Punk, James Trezona, to hear his perspective on how his profession has been transformed, but also how critical it is to focus on human and customer experience. Finally, they reflect on the implications of Elon Musks recent take-over of Twitter.\nTLDR:\n01:09 Intros\n02:50 Cloud conversation with James Trezona\n27:13 Elon musk buys Twitter\n31:05 The next book!\nFurther Reading:\nHumanising B2B, Paul Cash \u0026amp; James Trezona (https://www.amazon.co.uk/s?k=humanising+b2b\u0026amp;crid=2VH7EAZFPZRWL\u0026amp;sprefix=humanising+%2Caps%2C68\u0026amp;ref=nb_sb_ss_pltr-ranker-opsacceptance_3_11)\n","permalink":"//localhost:1313/posts/2022-12-08-the-digitalisation-of-marketing-with-james-trezona/","summary":"\u003cp\u003eThe marketing industry has been at the spear head of digitalisation, both in how its customers are changing their expectations and in how marketing professionals work on a day to day basis, increasing both scale of reach and the ability to create meaningful experiences.\u003c/p\u003e\n\u003cp\u003eDave and Sjoukje talk to the Founder of Rooster Punk, James Trezona, to hear his perspective on how his profession has been transformed, but also how critical it is to focus on human and customer experience.  Finally, they reflect on the implications of Elon Musks recent take-over of Twitter.\u003c/p\u003e","title":"The digitalisation of marketing with James Trezona"},{"content":"The team are at AWS re:Invent 2022 for our first live shows!\nHemant Sharma, Partner Solutions, AWS joins us to look back over the show, recap the big trends and look at their real world application.\n","permalink":"//localhost:1313/posts/2022-12-02-aws-re-invent-2022-summary-analysis-with-hemant-sh/","summary":"\u003cp\u003eThe team are at AWS re:Invent 2022 for our first live shows!\u003c/p\u003e\n\u003cp\u003eHemant Sharma, Partner Solutions, AWS joins us to look back over the show, recap the big trends and look at their real world application.\u003c/p\u003e","title":"AWS re:Invent 2022: Summary \u0026 Analysis with Hemant Sharma"},{"content":"The team are at AWS re:Invent 2022 for our first live shows!\nJonathan Allen, Director Enterprise Strategy, AWS joins us to talk about what drives cloud transformation and how not to sabotage yours!\nRob updates us on what’s trending on Day 2 of the show.\n","permalink":"//localhost:1313/posts/2022-12-01-aws-re-invent-2022-cloud-driven-business-transform/","summary":"\u003cp\u003eThe team are at AWS re:Invent 2022 for our first live shows!\u003c/p\u003e\n\u003cp\u003eJonathan Allen, Director Enterprise Strategy, AWS joins us to talk about what drives cloud transformation and how not to sabotage yours!\u003c/p\u003e\n\u003cp\u003eRob updates us on what’s trending on Day 2 of the show.\u003c/p\u003e","title":"AWS re:Invent 2022: Cloud driven business transformation with Jonathan Allen"},{"content":"The team are at AWS re:Invent 2022 for our first live shows!\nDimitris Perdikou, Head of Engineering, Home Office, UK Government shares with us the Home Office early adoption (at scale) of the cloud, seven years ago(!) and the challenges that bring when you move to second generation cloud.\nDimitris Perdikou\nIt’s Day 2 and the big themes of the conference are becoming clearer, our Roving Reporter Rob reports!\n","permalink":"//localhost:1313/posts/2022-12-01-aws-re-invent-2022-lessons-from-a-scale-early-adop/","summary":"\u003cp\u003eThe team are at AWS re:Invent 2022 for our first live shows!\u003c/p\u003e\n\u003cp\u003eDimitris Perdikou, Head of Engineering, Home Office, UK Government shares with us the Home Office early adoption (at scale) of the cloud, seven years ago(!) and the challenges that bring when you move to second generation cloud.\u003cbr\u003e\nDimitris Perdikou\u003c/p\u003e\n\u003cp\u003eIt’s Day 2 and the big themes of the conference are becoming clearer, our Roving Reporter Rob reports!\u003c/p\u003e","title":"AWS re:Invent 2022: Lessons from a scale early adopter with Dimitris Perdikou"},{"content":"The team are at AWS re:Invent 2022 for our first live shows!\nTom Metzeler, Partner Solutions, AWS rejoins us to talk about how the cloud can help organisations reduce their impact on the environment. And with all the talk this week of processor heavy work loads, like ML, or supermassive architectures, how can these be implemented safely?\nRoving Reporter Rob reports on Day 3 and the final trends emerging.\n","permalink":"//localhost:1313/posts/2022-11-30-aws-re-invent-2022-cloud-sustainability-with-tom-m/","summary":"\u003cp\u003eThe team are at AWS re:Invent 2022 for our first live shows!\u003c/p\u003e\n\u003cp\u003eTom Metzeler, Partner Solutions, AWS rejoins us to talk about how the cloud can help organisations reduce their impact on the environment.  And with all the talk this week of processor heavy work loads, like ML, or supermassive architectures, how can these be implemented safely?\u003c/p\u003e\n\u003cp\u003eRoving Reporter Rob reports on Day 3 and the final trends emerging.\u003c/p\u003e","title":"AWS re:Invent 2022: Cloud sustainability with Tom Metzeler"},{"content":"The team are at AWS re:Invent 2022 for our first live shows!\nPhil LeBrun, Enterprise Strategist, AWS, talks to us about why re-skilling your teams is critical not just to your transformations success, what it takes and why it’s such a leadership challenge.\nRoving Reporter Rob Kernahan updates us on what else is trending on Day 1 of the show.\n","permalink":"//localhost:1313/posts/2022-11-30-aws-re-invent-2022-re-skilling-your-enterprise-wit/","summary":"\u003cp\u003eThe team are at AWS re:Invent 2022 for our first live shows!\u003c/p\u003e\n\u003cp\u003ePhil LeBrun, Enterprise Strategist, AWS, talks to us about why re-skilling your teams is critical not just to your transformations success, what it takes and why it’s such a leadership challenge.\u003c/p\u003e\n\u003cp\u003eRoving Reporter Rob Kernahan updates us on what else is trending on Day 1 of the show.\u003c/p\u003e","title":"AWS re:Invent 2022: Re-Skilling your enterprise with Phil Le-Brun"},{"content":"In our three launch episodes, Dave and Sjoukje look at the cloud market from three different angles: the market itself, how CxO’s are viewing that market and from a personal leadership experience perspective.\nIn this first episode, they discuss some basic definitions for aspects of the cloud, to level set, then go on to talk about the current trends that are being felt industry wide and that are likely to impact 2023 and beyond. Finally, they discuss the latest in AI image creation, DALL-E, and its possibilities.\nTLDR:\n00:39 Intros 03:09 Cloud conversation: Definitions 15:32 Cloud conversation: Trends 2023 28:10 DALL-E ","permalink":"//localhost:1313/posts/2022-11-24-cloud-definitions-and-2023-trends-overview/","summary":"\u003cp\u003eIn our three launch episodes, Dave and Sjoukje look at the cloud market from three different angles: the market itself, how CxO’s are viewing that market and from a personal leadership experience perspective.\u003c/p\u003e\n\u003cp\u003eIn this first episode, they discuss some basic definitions for aspects of the cloud, to level set, then go on to talk about the current trends that are being felt industry wide and that are likely to impact 2023 and beyond.  Finally, they discuss the latest in AI image creation, DALL-E, and its possibilities.\u003c/p\u003e","title":"Cloud definitions and 2023 Trends overview"},{"content":"In our three launch episodes, Dave and Sjoukje look at the cloud market from three different angles: the market itself, how CxO’s are viewing that market and from a personal leadership experience perspective.\nIn this second episode, they talk to CxO Advisor, Daniel Hartert to get insights into the conversations he is currently having with CxOs on driving value from of the cloud in their organisations and the challenges they face. He also shares his personal journey with the cloud. Finally, they talk about how current economic environment will influence decision making on cloud over the coming years.\nTLDR:\n00:43 Intros 03:00 Cloud conversation with Daniel Hartert 26:59 Cloud cost 32:09 A nice weekend! ","permalink":"//localhost:1313/posts/2022-11-24-the-cxo-perspective-on-cloud-with-daniel-hartert/","summary":"\u003cp\u003eIn our three launch episodes, Dave and Sjoukje look at the cloud market from three different angles: the market itself, how CxO’s are viewing that market and from a personal leadership experience perspective.\u003c/p\u003e\n\u003cp\u003eIn this second episode, they talk to CxO Advisor, Daniel Hartert to get insights into the conversations he is currently having with CxOs on driving value from of the cloud in their organisations and the challenges they face.  He also shares his personal journey with the cloud.  Finally, they talk about how current economic environment will influence decision making on cloud over the coming years.\u003c/p\u003e","title":"The CxO perspective on cloud with Daniel Hartert"},{"content":"In our three launch episodes, Dave and Sjoukje look at the cloud market from three different angles: the market itself, how CxO’s are viewing that market and from a personal leadership experience perspective.\nIn our final launch episode, they talk to Systemic Leadership Sherpa, Alastair Kidd about how embarking on a cloud transformation journey could also be a personal development journey! How that could help reframe your leadership action to more effectively drive business value out of the cloud and why you need to be aware of the Complexity Paradox! Finally, they talk about Googles recent announcement about opening new regions in Europe and speculate on why.\nTLDR:\n01:07 Intros 01:51 Cloud conversation with Alastair Kidd 22:10 The complexity paradox 28:20 Why so many cloud regions? 32:32 Chickens! Further Reading:\nhttps://hbr.org/2007/11/a-leaders-framework-for-decision-making\nhttps://merionwest.com/2022/10/27/the-complexity-paradox/\n","permalink":"//localhost:1313/posts/2022-11-24-the-role-of-leadership-in-cloud-value-creation-wit/","summary":"\u003cp\u003eIn our three launch episodes, Dave and Sjoukje look at the cloud market from three different angles: the market itself, how CxO’s are viewing that market and from a personal leadership experience perspective.\u003c/p\u003e\n\u003cp\u003eIn our final launch episode, they talk to Systemic Leadership Sherpa, Alastair Kidd about how embarking on a cloud transformation journey could also be a personal development journey!  How that could help reframe your leadership action to more effectively drive business value out of the cloud and why you need to be aware of the Complexity Paradox!  Finally, they talk about Googles recent announcement about opening new regions in Europe and speculate on why.\u003c/p\u003e","title":"The role of leadership in cloud value creation with Alastair Kidd"},{"content":"On September 13, we hosted our first XPLR.Lowlands meetup. This was a full virtual event, streamed from our community studio. We hosted the following sessions:\nSessions Navigating the world of AI infrastructure as a data scientist: Gabrielle Davelaar \u0026amp; Ekaterina Sirazitdinova\nTraditionally hardware has always been a job for a High Performance Specialist. With the arrival of State-of-the-Art models such as GPT-3, Turing and Megatron-Turing more compute has been proven to be necessary. This talk is about how to navigate through the world of GPU’s, bandwidth, parallelization from a data science perspective.\nA Quick Trip Across the Metaverse and Realities: Thomas Lewis\nThe volume is increasing from news around the Metaverse as well as Mixed Reality across Virtual and Augmented Reality. It can be a bit confusing. Also, what is Microsoft doing in all of these spaces? Join us for a fast-paced tour of all the realities, what Microsoft is doing in this space, understanding why this is a space for us to invest in and a look into the future.\nRecording You can watch the recording of the stream here:\nhttps://youtu.be/S1NRLIZssBg\nAll sessions will be published on our YouTube channel. Please subscribe here:\n","permalink":"//localhost:1313/posts/2022-09-19-xplr-lowlands-september-13-virtual-/","summary":"\u003cp\u003eOn September 13, we hosted our first XPLR.Lowlands meetup. This was a full virtual event, streamed from our community studio. We hosted the following sessions:\u003c/p\u003e\n\u003ch2 id=\"sessions\"\u003eSessions\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"%22https://lowlands.events/xplr-lowlands-september/navigating-the-world-of-ai-infrastructure-as-a-data-scientist/%22\"\u003eNavigating the world of AI infrastructure as a data scientist\u003c/a\u003e: Gabrielle Davelaar \u0026amp; Ekaterina Sirazitdinova\u003c/p\u003e\n\u003cp\u003eTraditionally hardware has always been a job for a High Performance Specialist. With the arrival of State-of-the-Art models such as GPT-3, Turing and Megatron-Turing more compute has been proven to be necessary. This talk is about how to navigate through the world of GPU’s, bandwidth, parallelization from a data science perspective.\u003c/p\u003e","title":"XPLR.Lowlands: September 13 (virtual)"},{"content":"Governments and organizations are focusing on digital transformation to fundamentally transform the way they operate and deliver services to their customers. Cloud adoption has increased tremendously in the last couple of years, also due to the COVID-19 pandemic. But as they move to the cloud, organizations want to maintain the same level of control over their IT resources as they have in their datacenters. Concerns about cloud sovereignty, which includes data, operational, and technical issues, are not new and have been increasing because of rising geopolitical tensions; changing data and privacy laws in different countries; the dominant role of cloud players concentrated in a few regions; and the lessons learned through the pandemic. As a result, governments and organizations are re-evaluating their external exposure and looking for ways to maintain physical and digital control over strategic assets.\nTo adhere to these concerns, Microsoft has released a new solution called Microsoft Cloud for Sovereignty. This solution is aimed to meet compliance, security, and policy requirements that governments and organizations are facing. With the addition of Microsoft Cloud for Sovereignty, governments and organizations will have more control over their data, and it will increase the transparency of operations and governance processes of the cloud.\nMicrosoft Cloud for Sovereignty is designed to be a partner-led solution, where partners will play a vital role in delivering the solutions. One of Microsoft\u0026rsquo;s European Cloud principles is that Microsoft will provide cloud offerings that meet European government sovereign needs in partnership with local trusted technology providers. For instance, in May 2021, Blue was announced: a partnership to meet sovereignty requirements in France, together with Capgemini and a French company owned by Orange. Microsoft Cloud for Sovereignty will offer the tools, the processes, and the transparency to trusted partners, and these partners will then support governments and organizations on their digital transformation journey.\nWith Microsoft Cloud for Sovereignty, Microsoft is focusing on the following pillars:\nData residency Data residency is the requirement that data must be stored within a specific geographic boundary, such as a national boundary. Azure offers data residency for many services in over 35 countries with over 60 different datacenter regions worldwide (and growing). This enables residency options for Azure, Microsoft 365, and Dynamics 365, where many clients can store and process their data locally. By implementing policies, clients can meet their regulatory requirements to store their applications and data in the required geographical boundary. For Europe, the forthcoming EU Data Boundary will ensure that data will be stored and processed in the EU and European Free Trade Association.\nSovereign controls In addition to the specific regions and geographic boundaries where applications and data are stored and processed, Microsoft also offers a set of sovereign controls that provide additional layers to protect and encrypt sensitive data. These controls span the entire Microsoft cloud: SaaS offerings such as Power Platform, Microsoft 365 and Dynamics 365, and the cloud infrastructure and the PaaS services that are available in Azure.\nThe following offerings can be leveraged by clients for sovereign protection:\nAzure Confidential Computing: Azure Confidential Computing consists of confidential virtual machines and confidential containers. This enables data to be encrypted in rest, but also in use. Specialized hardware is used to create isolated and encrypted memory, which is called Trusted Execution Environments (TEEs). TEEs guarantees that data and code that is processed cannot be accessed from outside the TEE. Client owned encryption keys are released directly from a Managed HSM (Hardware Security Module) into the TEEs. The client keys are secured, also when in use, and it ensures that data is encrypted in use, transit and in rest.\nDouble Key Encryption (DKE): DKE uses two keys together to access protected content. One key is stored in Azure and the other key is hold by the client. It comes with Microsoft 365 E5 and it is intended for the most sensitive data that is subject to the strictest protection requirements.\nCustomer Lockbox: Customer Lockbox ensures that Microsoft can\u0026rsquo;t access client data and content without explicit approval from the client during service operations. Customer Lockbox is offered for Microsoft 365, Microsoft Azure, Power Platform, and Dynamics 365.\nAzure Arc: Azure Arc extends the Azure services, management, and governance features and capabilities to run across datacenters, at the edge, and in multicloud environments. Clients can centrally manage a wide range of resources including Windows and Linux servers, SQL Server, Kubernetes clusters, and other Azure services. Virtual machine lifecycle management can be performed from a central location. Governance and compliance standards can be met by implementing Azure Policy across these different resources. And services such as Azure Monitor, and Microsoft Defender for Cloud can be enrolled as well.\nSovereign Landing Zone: Microsoft Cloud for Sovereignty will include a Sovereign Landing Zone. This landing zone is built upon the enterprise scale Azure Landing Zone and will make deployments automatable, customizable, repeatable and consistent. This landing zone will extend into Azure Information Protection, which also enables policy and labeling for access control and protection on email and document data. Clients can also define custom policies to meet specific industry and regulatory requirements.\nGovernance and transparency The Government Security Program (GSP) provides participants from over 45 countries and international organizations represented by more than 90 different agencies, with the confidential security information and resources they need to trust Microsoft\u0026rsquo;s products and services. These participants have access to five globally distributed Transparency Centers, receive access to source code, and can engage on technical content about Microsoft\u0026rsquo;s products and services. Microsoft Cloud for Sovereignty will expand GSP to increase cloud transparency, starting with key Azure infrastructure components.\nWrap up In this article I wanted to focus on what Microsoft Cloud for Sovereignty has to offer for clients that want to leverage the Microsoft cloud for their digital transformation journey, but also want to maintain the same level of control over their IT resources as they have in their own datacenters. Cloud adoption has accelerated enormously in the last couple of years, which also has makes cloud sovereignty much more important for governments and organizations. Microsoft offers the tools, processes, and transparency to partners and clients, to support the increasing sovereignty requirements that clients have on their datival transformation journey.\nDue to these increasing sovereignty requirements Capgemini has conducted research to look deeper into organizational awareness and key priorities when it comes to cloud sovereignty, and the role it plays in overall cloud strategy. We have released a whitepaper with the results, which can be downloaded here: https://www.capgemini.com/gb-en/news/while-definitions-of-sovereign-cloud-vary-43-of-organisations-are-currently-focusing-on-data-localisation/?utm_source=sociabble\u0026amp;utm_medium=social\u0026amp;utm_content=insightsdata_countryorganic_link_blog_uk_blog\u0026amp;utm_campaign=cloud\n","permalink":"//localhost:1313/posts/2022-08-25-microsoft-cloud-for-sovereignty-maintain-control-over-strategic-digital-assets/","summary":"\u003cp\u003eGovernments and organizations are focusing on digital transformation to fundamentally transform the way they operate and deliver services to their customers. Cloud adoption has increased tremendously in the last couple of years, also due to the COVID-19 pandemic. But as they move to the cloud, organizations want to maintain the same level of control over their IT resources as they have in their datacenters. Concerns about cloud sovereignty, which includes data, operational, and technical issues, are not new and have been increasing because of rising geopolitical tensions; changing data and privacy laws in different countries; the dominant role of cloud players concentrated in a few regions; and the lessons learned through the pandemic. As a result, governments and organizations are re-evaluating their external exposure and looking for ways to maintain physical and digital control over strategic assets.\u003c/p\u003e","title":"Microsoft Cloud for Sovereignty - maintain control over strategic digital assets"},{"content":"This Global XR Talks, Alexander Meijers presented the following session:\nIntroduction to Microsoft Remote Assist and Guides Microsoft offers a broad range of solutions modernizing field services with Mixed Reality for technicians. It empowers them by offering modern tools like Mixed Reality devices, Video calls, Annotations and File Sharing capabilities. These tools allow field service workers to solve complex problems even faster, collaborate together with experts and gives them easy access to work orders. Another one of them is creating customized training modules for new workers on the factory floor. Using Dynamics 365 and Microsoft HoloLens 2 we are able to create augmented training which allows new workers to be more quickly in learning their daily job, support workers in complex tasks and even guide audits. This session will give you an introduction to both first-party apps of Microsoft.\nWatch the recording You can watch the recording of the session here:\nhttps://youtu.be/nNUsyKqmX_s\n","permalink":"//localhost:1313/posts/2022-04-22-global-xr-talks-april-2022/","summary":"\u003cp\u003eThis Global XR Talks, Alexander Meijers presented the following session:\u003c/p\u003e\n\u003ch2 id=\"introduction-to-microsoft-remote-assist-and-guides\"\u003eIntroduction to Microsoft Remote Assist and Guides\u003c/h2\u003e\n\u003cp\u003eMicrosoft offers a broad range of solutions modernizing field services with Mixed Reality for technicians. It empowers them by offering modern tools like Mixed Reality devices, Video calls, Annotations and File Sharing capabilities. These tools allow field service workers to solve complex problems even faster, collaborate together with experts and gives them easy access to work orders. Another one of them is creating customized training modules for new workers on the factory floor. Using Dynamics 365 and Microsoft HoloLens 2 we are able to create augmented training which allows new workers to be more quickly in learning their daily job, support workers in complex tasks and even guide audits. This session will give you an introduction to both first-party apps of Microsoft.\u003c/p\u003e","title":"Global XR Talks - April 2022"},{"content":"We are reaching the end of the Global AI Bootcamp 2022, and we’d like to introduce you to organizers from all over the world to learn more about their events: who are they and what content (did) they provide? In 6 different episodes (and timezones) we’ll focus on specific regions, and like the bootcamp, we’ll go around the world to connect to everyone.\nThe Global AI Bootcamp is a series of free one-day events organized across the world by local communities that are passionate about artificial intelligence on Microsoft Azure. Check out the official Global AI Bootcamp website, for more information and an overview of all the events.\nBehind the Scenes Down Under Learn about PyTorch on Azure Machine Learning and join us in a friendly chat with organizers from Australia and New Zealand.\nSpeaker: Sabrina Smai\nCommunity organizers: Akanksha Malik \u0026amp; Anupama Natarajan\nhttps://youtu.be/RQhwzkQSIlA\nBehind the Scenes Africa Learn about Semantic Search on Azure and join us in a friendly chat with organizers from Morocco and Mauritius.\nSpeaker: Derek Legenzoff - Senior Program Manager @Microsoft\nCommunity organizers: Hassan Fadili \u0026amp; Zakiya Buhora\nhttps://youtu.be/SoctPIvOQ6I\nBehind the Scenes South America Learn about Semantic Search on Azure and join us in a friendly chat with organizers from Morocco and Mauritius.\nSpeaker: Amira Youssef - Princiap Program Manager @Microsoft\nCommunity organizers: Patricio Cofre \u0026amp; Luis Beltrán\nhttps://youtu.be/a6FjLW2GAnI\nBehind the Scenes South America Learn about the Azure Percept devices and join us in a friendly chat with organizers from Chili and Mexico.\nSpeaker: Amira Youssef - Principal Program Manager @Microsoft\nCommunity organizers: Patricio Cofre \u0026amp; Luis Beltrán\nBehind the Scenes North America Learn about Document Processing on Azure devices and join us in a friendly chat with organizers from Canada and the USA.\nSpeaker: Neta Haiby - Group Product Manager Azure AI @ Microsoft\nCommunity organizers: Ron Dagdag \u0026amp; Ehsan Eskandari\nhttps://youtu.be/jSTXMLAK-oI\nBehind the Scenes North America Learn about Document Processing on Azure devices and join us in a friendly chat with organizers from Canada and the USA.\nSpeaker: Neta Haiby - Group Product Manager Azure AI @ Microsoft\nCommunity organizers: Ron Dagdag \u0026amp; Ehsan Eskandari\nBehind the Scenes Asia Join us in a friendly chat with organizers from India and Taiwan.\nCommunity organizers: Kasam Ahmed Shaikh \u0026amp; Ning Chen\nhttps://youtu.be/HXIed8pgC-0\nBehind the Scenes Europe Learn about the responsible AI Dashboard and join us in a friendly chat with organizers from the UK and the Netherlands\nSpeaker: Mehrnoosh Sameki - Senior Program Manager @ Microsoft\nCommunity organizers: Alyona Galyeva \u0026amp; Terry McCann\nhttps://youtu.be/R3uBe2WIugU\n","permalink":"//localhost:1313/posts/2022-03-20-global-ai-bootcamp-behind-the-scenes/","summary":"\u003cp\u003eWe are reaching the end of the Global AI Bootcamp 2022, and we’d like to introduce you to organizers from all over the world to learn more about their events: who are they and what content (did) they provide? In 6 different episodes (and timezones) we’ll focus on specific regions, and like the bootcamp, we’ll go around the world to connect to everyone.\u003c/p\u003e\n\u003cp\u003eThe Global AI Bootcamp is a series of free one-day events organized across the world by local communities that are passionate about artificial intelligence on Microsoft Azure. Check out the official \u003cem\u003e\u003cstrong\u003e\u003ca href=\"%22https://globalai.community/bootcamp-2022/%22\"\u003eGlobal AI Bootcamp\u003c/a\u003e\u003c/strong\u003e\u003c/em\u003e website, for more information and an overview of all the events.\u003c/p\u003e","title":"Global AI Bootcamp - Behind the Scenes"},{"content":"Sarah Lean: When superpowers combine; Azure \u0026amp; Octopus Deploy Microsoft Azure is a great option for organisations looking to host their IT infrastructure in the cloud or as a hybrid partner. While Octopus Deploy is a tool that can help you automate the deployment of your infrastructure and software releases all in one place.\nIn this talk, we\u0026rsquo;ll cover off the possibilities of using Octopus Deploy to help your developers, release managers, and operations folk deploy and manage their Azure resources.\nhttps://youtu.be/W_HdBTp1s3U\nKendall Roden: Azure Container Apps At Ignite, Microsoft announced Azure Container Apps, a serverless application-centric hosting service that enables users to quickly deploy containerized applications and microservices to Azure without the need to configure and manage underlying infrastructure resources or container orchestrators. Azure Container Apps runs on Azure Kubernetes Service, and includes several open-source projects: Kubernetes Event Driven Autoscaling (KEDA), Distributed Application Runtime (Dapr), and Envoy. This open-source foundation enables teams to build and run portable applications powered by Kubernetes and open standards without the operational overhead and management complexity of working with the platform directly. Join this session to hear from the container apps PG about the offering, how it fits into the Azure Portfolio and where it is headed!\nhttps://youtu.be/Wbz2Ghs6IWQ\n","permalink":"//localhost:1313/posts/2022-03-05-azure-thursday-march-2022/","summary":"\u003ch2 id=\"sarah-lean-when-superpowers-combine-azure--octopus-deploy\"\u003eSarah Lean: When superpowers combine; Azure \u0026amp; Octopus Deploy\u003c/h2\u003e\n\u003cp\u003eMicrosoft Azure is a great option for organisations looking to host their IT infrastructure in the cloud or as a hybrid partner. While Octopus Deploy is a tool that can help you automate the deployment of your infrastructure and software releases all in one place.\u003c/p\u003e\n\u003cp\u003eIn this talk, we\u0026rsquo;ll cover off the possibilities of using Octopus Deploy to help your developers, release managers, and operations folk deploy and manage their Azure resources.\u003c/p\u003e","title":"Azure Thursday - March 2022"},{"content":"InFebruary we kicked off 2022 with the following great session:\nA brand new way to develop AR/VR apps for Oculus using WebXR PWA In this session we will learn about XR Development on AR/VR using Javascript to distribute our Apps or Games into Oculus Platform as a Playable Application.\nSpeaker: R Surahutomo Aziz Pradana\nWatch the recording You can watch the recording of the session below:\nhttps://www.youtube.com/watch?v=-SicCAs2kag\n","permalink":"//localhost:1313/posts/2022-02-28-global-xr-talks-february-2022/","summary":"\u003cp\u003eInFebruary we kicked off 2022 with the following great session:\u003c/p\u003e\n\u003ch2 id=\"a-brand-new-way-to-develop-arvr-apps-for-oculus-using-webxr-pwa\"\u003eA brand new way to develop AR/VR apps for Oculus using WebXR PWA\u003c/h2\u003e\n\u003cp\u003eIn this session we will learn about XR Development on AR/VR using Javascript to distribute our Apps or Games into Oculus Platform as a Playable Application.\u003c/p\u003e\n\u003cp\u003eSpeaker:  R Surahutomo Aziz Pradana\u003c/p\u003e\n\u003ch2 id=\"watch-the-recording\"\u003eWatch the recording\u003c/h2\u003e\n\u003cp\u003eYou can watch the recording of the session below:\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.youtube.com/watch?v=-SicCAs2kag\"\u003ehttps://www.youtube.com/watch?v=-SicCAs2kag\u003c/a\u003e\u003c/p\u003e","title":"Global XR Talks - February 2022"},{"content":"Esther Barthel: Transitioning Ops to the Cloud, adding Dev skills to the mix This session offers tips and tricks to add core Developer skills to your skillset, so you can transition from (on-premises) Operations to a Cloud DevOps role. This session will zoom in on some basic knowledge and terminology that will make it easier to understand the shift in work and competencies for DevOps engineers.\nKeep in mind that this session is not a technical deep dive that helps you pick the right tool for the job at hand, but it focuses on some basic knowledge of DevOps processes that will help you to plan the next steps in your career and pick those DevOps competencies that will put the fun back into your work.\nhttps://youtu.be/FRAKRgvtFlA\nBarbara Forbes: Infra as Code: Bicep for the Azure enterprise If you work with Azure, you will want to use infra as code. Maybe you have are familiar with ARM templates already. Have you heard of Bicep? It has a lot to offer when you use infra as code in an enterprise environment. How do you make use of all the possibilities that are available to deploy your Azure infrastructure? We will consider a few of the great code solutions that Bicep has to offer and how VSCode can help you create templates. To see a real-life scenario, we take a look at how Bicep can be used to ensure the right mix between flexibility and usability. And to bring it together, let\u0026rsquo;s see how it fits into your CICD process. In the end, you will have the tools to start and the insights to take it all to the next level.\nhttps://youtu.be/vEDiSgeSTik\n","permalink":"//localhost:1313/posts/2022-02-21-azure-thursday-february-2022/","summary":"\u003ch2 id=\"esther-barthel-transitioning-ops-to-the-cloud-adding-dev-skills-to-the-mix\"\u003eEsther Barthel: Transitioning Ops to the Cloud, adding Dev skills to the mix\u003c/h2\u003e\n\u003cp\u003eThis session offers tips and tricks to add core Developer skills to your skillset, so you can transition from (on-premises) Operations to a Cloud DevOps role. This session will zoom in on some basic knowledge and terminology that will make it easier to understand the shift in work and competencies for DevOps engineers.\u003c/p\u003e\n\u003cp\u003eKeep in mind that this session is not a technical deep dive that helps you pick the right tool for the job at hand, but it focuses on some basic knowledge of DevOps processes that will help you to plan the next steps in your career and pick those DevOps competencies that will put the fun back into your work.\u003c/p\u003e","title":"Azure Thursday - February 2022"},{"content":"Frank Buters: Getting rid of silo’s, Embracing MLOps, Democratize data all sounds catchy but how? Having come across of many examples, especially within the financial sector (Banks, Insurance, Pensions), where data, modelling and IT activities take place in different teams across departments, we know about some of the pains of our customers. So how can technology, and in particular Azure technology, help here? During this Azure Thursday event we will discuss and experience how Azure Synapse Analytics together with Azure Machine Learning can help relieve some of these pains. We will show you how a model can be deployed as a stand-alone API, and how easy it is to use the same model to score records in a traditional data warehouse. In this way, we hope to give a flavor of how state-of-the art techniques can go hand-in-hand with more traditional ways of processing and storing data.\nhttps://youtu.be/pT8YxeOL8j8\nAnna-Maria Wykes: Terraforming the Cloud: A guide to using Terraform to control cloud deployments Azure provides us with many ways to deploy our workloads, but how can we do this repeatable and cross-cloud? During this session, I am going to start with the basics and work my way up to managing dependencies, managing state, and how to leverage Terraform in Azure DevOps.\nhttps://youtu.be/se7nhILOP6o\n","permalink":"//localhost:1313/posts/2022-01-16-azure-thursday-january-2022/","summary":"\u003ch2 id=\"frank-buters-getting-rid-of-silos-embracing-mlops-democratize-data-all-sounds-catchy-but-how\"\u003eFrank Buters: Getting rid of silo’s, Embracing MLOps, Democratize data all sounds catchy but how?\u003c/h2\u003e\n\u003cp\u003eHaving come across of many examples, especially within the financial sector (Banks, Insurance, Pensions), where data, modelling and IT activities take place in different teams across departments, we know about some of the pains of our customers. So how can technology, and in particular Azure technology, help here? During this Azure Thursday event we will discuss and experience how Azure Synapse Analytics together with Azure Machine Learning can help relieve some of these pains. We will show you how a model can be deployed as a stand-alone API, and how easy it is to use the same model to score records in a traditional data warehouse. In this way, we hope to give a flavor of how state-of-the art techniques can go hand-in-hand with more traditional ways of processing and storing data.\u003c/p\u003e","title":"Azure Thursday - January 2022"},{"content":"This evening we had two amazing speakers again!\nGianni Castaldi \u0026amp; Jeroen Niesen: How to run an Azure SOC like Ninja\u0026rsquo;s Most companies use Microsoft Azure as their cloud. With the use of Azure, companies are bringing sensitive data to Azure. Just as on-premises you want to make sure that your data is well protected and is compliant with the law and regulations of your company’s industry. Azure has all components available to create a next-gen SOC. In this session,\nJeroen and Gianni will tell you how you can build a SOC in Microsoft Azure and make sure that you get the best alerts, and do not get overloaded with security alerts. Jeroen and Gianni will Discuss Microsoft Sentinel, Microsoft 365 Defender, Kusto, API, and much more.\nhttps://youtu.be/koLC7yYj-fM\nSuzanne Daniels: Hej! An intro to Backstage Backstage is Spotify\u0026rsquo;s Developer Portal, and now a thriving Open Source project donated to CNCF with a growing community of contributors. It’s been adopted with companies like Expedia, Netflix, American Airlines, and Epic Games.\nSuzanne will share the story why Backstage was developed and why it became the core of Developer Experience at Spotify. You’ll get a brief tour of the Backstage platform, the plugin ecosystem, and get an understanding of the key use cases for Backstage. Suzanne\u0026rsquo;s passion is finding ways to help developers and engineers get the tools and skills to do what they do best: creating the software this world runs on while trying to innovate and make sense of buzzwords at the same time. She\u0026rsquo;s organizer of events, host and a speaker on both technical topics and more in general on transformation \u0026amp; innovation.\nhttps://youtu.be/t4N35x_kj-Q\n","permalink":"//localhost:1313/posts/2022-01-15-azure-thursday-december-2021/","summary":"\u003cp\u003eThis evening we had two amazing speakers again!\u003c/p\u003e\n\u003ch2 id=\"gianni-castaldi--jeroen-niesen-how-to-run-an-azure-soc-like-ninjas\"\u003eGianni Castaldi \u0026amp; Jeroen Niesen: How to run an Azure SOC like Ninja\u0026rsquo;s\u003c/h2\u003e\n\u003cp\u003eMost companies use Microsoft Azure as their cloud. With the use of Azure, companies are bringing sensitive data to Azure. Just as on-premises you want to make sure that your data is well protected and is compliant with the law and regulations of your company’s industry. Azure has all components available to create a next-gen SOC. In this session,\u003c/p\u003e","title":"Azure Thursday - December 2021"},{"content":"On December 15 we wanted to zoom in on accessibility opportunities in (web) tech at the Tech A11y Summit.\nDisability never hold anyone back. Disability is not something that people need to overcome. The barriers that exist are created by society and it\u0026rsquo;s up to every single one of us to work together to remove those barriers.\nDuring the event interpretation into International Sign was provided.\nBelow, you see an overview of the sessions:\nhttps://www.youtube.com/playlist?list=PLxu2-n2PUPT1NQAyoARJ3XVCisingsOO8\n","permalink":"//localhost:1313/posts/2022-01-07-tech-a11y-summit/","summary":"\u003cp\u003eOn December 15 we wanted to zoom in on accessibility opportunities in (web) tech at the Tech A11y Summit.\u003c/p\u003e\n\u003cp\u003eDisability never hold anyone back. Disability is not something that people need to overcome. The barriers that exist are created by society and it\u0026rsquo;s up to every single one of us to work together to remove those barriers.\u003c/p\u003e\n\u003cp\u003eDuring the event interpretation into International Sign was provided.\u003c/p\u003e\n\u003cp\u003eBelow, you see an overview of the sessions:\u003c/p\u003e","title":"Tech A11y Summit"},{"content":"Nowadays, nearly every developer is a developer of distributed applications or systems. Modern software development involves building distributed applications: almost every application leverages a distributed architecture, and most are backed by services in the public cloud. Microservices are a successful approach for building distributed services. And by using containers to build these microservices, you will introduce scalability, faster deployments, and the ability to run applications everywhere.\nMicrosoft recently released a new container service in Azure: Azure Container Apps, which is a serverless container service for running modern apps at scale. Azure Container Apps is released during Ignite, in September 2021, and currently still in preview. The IT industry is embracing containerization heavily the last couple of years, but struggles managing the infrastructure. Kubernetes, the facto standard at many organizations, is challenging to learn and to manage for many IT professionals. Azure Container Apps offers a solution for this: this service is specifically aimed to deploy containerized applications without the need to manage complex infrastructure. It manages the details of Kubernetes and container orchestrations for you.\nIn this article I will tell you more about the features Azure Container Apps provide, and how this service differs from the other container services that Microsoft offers in Azure.\nAzure Container Apps With Azure Container Apps you can run your containerized applications and your microservices on a serverless platform. Applications can be written in different programming languages or frameworks, and it offers full support for Distributed Application Runtime (DAPR) and Kubernetes Event-Driven Autoscaling (KEDA) to scale your applications or microservices dynamically based on the HTTP traffic or events that occur.\nAzure Container Apps is suitable for running microservices, handling event-driven processing, hosting background applications, and deploying API endpoints. Your applications can scale dynamically based on CPU or memory load, event-driven processing, HTTP traffic, or KEDA-supported scalers.\nThis service offers the following features and capabilities:\nRun multiple revisions of containers: Azure Containers Apps works application lifecycle resolves around revisions. When you deploy your first application in Azure Container Apps, the first revision of your application is automatically created. As containers get change or get updated, more revisions get created.\nSupport for running applications from multiple container registries: Azure Container Apps supports multiple container registries to store your images. It offers support for both public and private registries, such as Azure Container Registry, Docker. Basically, any Linux-based container image is supported from any public or private registry.\nAutoscaling: Applications can be scaled automatically by any KEDA-supported scale trigger. Most applications can scale to zero, except applications that are configured to scale on CPU or memory load.\nBuilding microservices with DAPR: Building applications using microservices based architecture, introduces a lot of complexity. You need to account for failures, retries, and timeouts, spread across the network and the distributed services. DAPR includes features to make life a lot easier, it includes features like observability, retries, pub/sub, and service-to-service invocation with mutual TLS, and much more.\nSupport for ARM templates, PowerShell, and Azure CLI: You can use ARM templates, PowerShell, and the Azure CLI to deploy your containers apps services in Azure and applications that run on it. Besides ARM and the Azure CLI, you can also use Bicep to deploy Azure Container Apps and the applications.\nEnables HTTP Ingress: Applications and microservices that run on Azure Container Apps can be exposed to the public web by enabling ingress. By using ingress, you don\u0026rsquo;t need to create an Azure Load Balancer, public IP address, or any other Azure resource to enable incoming HTTPS request. In my opinion, this is most suitable for smaller organizations. For most enterprises a load balancer or multiple load balancers are already part of the architecture (and security requirements) and the preferred way for enabling incoming traffic.\nInternal ingress and service discovery: Ingress can also be enabled for internal communication between the different microservices, or applications deployed inside the same Container Apps environment. When ingress is enabled, each container app will be exposed through a domain name.\nSplit traffic: Azure Container Apps offer support for traffic direction strategies, such as A/B testing and Bluegreen deployments. For instance, you can configure traffic splitting rules that split traffic based on percentages. You can assign percentage values to balance traffic among different revisions.\nViewing application logs: Azure Container Apps has support for Azure Log Analytics. Data logged via a container app are stored inside a custom table in the Log Analytics workspace. These logs can then be viewed through the Azure portal or with the Azure CLI.\nComparing Container Apps with other Azure container options Currently, there are a lot options you can choose from when developing container applications and host them on Azure. Microsoft has released a number of Azure container-based products that all have their own characteristics and are suitable in certain scenarios. Below, you see an overview of the different services and the scenarios and use cases for each service, compared to Azure Container Apps:\nAzure Container Apps As mentioned before, with Azure Container Apps you can deploy containerized apps without managing the complex infrastructure. It is powered by Kubernetes and open-source technologies like Dapr, KEDA, and envoy. It fully supports Kubernetes style applications and microservices with features like service discovery and traffic splitting. It also offers support for scaling based on traffic and pulling from event sources like queues, including scale to zero.\nHowever, because it is a serverless platform, it doesn\u0026rsquo;t provide direct access to the underlying Kubernetes APIs. If access to the Kubernetes APIs and control plane is required, then you should consider using Azure Kubernetes Service. If this access is not required, then Azure Container Apps provide a fully managed experience based on industry best-practices.\nAzure Container Instances Azure Container Instances offers a single pod of Hyper-V isolated containers on demand. Compared to Azure Container Apps, this can be thought of as a lower level \u0026quot;building block\u0026quot;. ACI does not provide features such as scaling, load balancing, and certificates. For instance, to scale to three container instances, you create three distinct container instances. ACI is often used to interact with through other services, Azure Kubernetes can layer orchestration and scale on top of ACI through virtual nodes. If you don\u0026rsquo;t have scaling, load balancing, or certificate requirements for your applications, ACI can be a good solution.\nAzure App Service Azure App Service is optimized for web applications. It provides fully managed hosting for web applications including websites and web APIs. These applications can be deployed on Azure App Service using code or containers, and it integrates with other Azure services including Azure Container Apps or Azure Functions. This makes it most suitable for scenario\u0026rsquo;s where you only build web apps.\nAzure Kubernetes Service Azure Kubernetes Service (AKS) offers a fully managed Kubernetes option in Azure, which includes access to the Kubernetes API and control plane. The full cluster resides in your Azure subscription, which makes you responsible for managing the cluster configurations and operations. If you are looking for a fully managed version of Kubernetes in Azure, AKS is a good solution.\nAzure Functions Azure Functions is optimized for running event-driven applications using the functions programming model. It offers similar functionality with Azure Container Apps around scaling and integration with events but is mostly optimized for ephemeral functions. They can be deployed using code and containers, where the latter makes it portable to deploy them to other container-based platforms. They are most suitable in scenarios where you want to trigger the execution of your functions on events and bind to other data sources in Azure.\nAzure Spring Cloud When you are developing Spring Boot microservice applications, Azure Spring Cloud makes it easy to deploy them to Azure without any code changes. This is a fully managed service so developers can focus only on writing the code, instead of managing the environment. Azure Spring Cloud provides lifecycle management using CI/CD integration, configuration management, monitoring and diagnostics, service discovery, blue-green deployments, and more. In scenarios where your development team is predominantly Spring, this is a good solution.\nGetting started Want to get started with Azure Container Apps and experience it yourself? Here is a list of useful articles that can get you started:\nAzure Container Apps Preview environments: https://docs.microsoft.com/en-us/azure/container-apps/environment\nMicroservices with Azure Containers Apps Preview: https://docs.microsoft.com/en-us/azure/container-apps/microservices\nQuickstart: Deploy your first container app: https://docs.microsoft.com/en-us/azure/container-apps/get-started\nPublish revisions with GitHub Actions in Azure Container Apps Preview: https://docs.microsoft.com/en-us/azure/container-apps/github-actions-cli?tabs=bash\n","permalink":"//localhost:1313/posts/2021-12-30-an-introduction-to-azure-container-apps/","summary":"\u003cp\u003eNowadays, nearly every developer is a developer of distributed\napplications or systems. Modern software development involves building\ndistributed applications: almost every application leverages a\ndistributed architecture, and most are backed by services in the public\ncloud. Microservices are a successful approach for building distributed\nservices. And by using containers to build these microservices, you will\nintroduce scalability, faster deployments, and the ability to run\napplications everywhere.\u003c/p\u003e\n\u003cp\u003eMicrosoft recently released a new container service in Azure: Azure\nContainer Apps, which is a serverless container service for running\nmodern apps at scale. Azure Container Apps is released during Ignite, in\nSeptember 2021, and currently still in preview. The IT industry is\nembracing containerization heavily the last couple of years, but\nstruggles managing the infrastructure. Kubernetes, the facto standard at\nmany organizations, is challenging to learn and to manage for many IT\nprofessionals. Azure Container Apps offers a solution for this: this\nservice is specifically aimed to deploy containerized applications\nwithout the need to manage complex infrastructure. It manages the\ndetails of Kubernetes and container orchestrations for you.\u003c/p\u003e","title":"An introduction to Azure Container Apps"},{"content":"On December 1-3, we organized the largest global community event about Virtual Reality, Augmented Reality, Mixed Reality and WebXR!\nAll sessions were streamed to our YouTube channel and can be watched there! See below, an overview of all the YouTube playlist:\nhttps://www.youtube.com/playlist?list=PLHulrhe_CI0gtt6sPqgiTNKBDCOAlqXZh\n","permalink":"//localhost:1313/posts/2021-12-10-global-xr-conference-2021/","summary":"\u003cp\u003eOn December 1-3, we organized the largest global community event about Virtual Reality, Augmented Reality, Mixed Reality and WebXR!\u003c/p\u003e\n\u003cp\u003eAll sessions were streamed to our YouTube channel and can be watched there! See below, an overview of all the YouTube playlist:\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.youtube.com/playlist?list=PLHulrhe\"\u003ehttps://www.youtube.com/playlist?list=PLHulrhe\u003c/a\u003e_CI0gtt6sPqgiTNKBDCOAlqXZh\u003c/p\u003e","title":"Global XR Conference 2021"},{"content":"This is the second article in a series about the different solutions that Microsoft Azure has to offer for specific industries. In my first post, I covered how these industry specific solutions help organizations to accelerate their digital transformation, and why this is so important nowadays.\nIn this second part, we will look at the manufacturing, financials services and government industries.\nAzure for manufacturing In the upcoming years, instead of selling unconnected products, manufacturers will start focusing more on selling products that come with connectivity services. They will also move from selling a discrete product to selling products as a service.\nTo accommodate global crises, manufacturers will also seek ways to be more agile and responsive to changing market demands. Manufacturers are building digital feedback loops that help companies connect with their products and customers to continuously learn, grow, and improve existing services, as well as build new ones.\nWith Azure for manufacturing, organizations can keep pace with customer needs and market trends and drive business transformation, by modernizing to a smart factory with Azure.\nAzure for manufacturing includes the following tools and services:\nAzure AI and Machine Learning: With these tools, manufacturers can capitalize on the potential of Industry 4.0 with intelligent manufacturing operations and supply chain powered by proven and responsible AI practices. Azure Industrial IoT: Help securely connect new and existing industrial machinery to the cloud with open manufacturing solutions that support brownfield connectivity and enable system interoperability. Azure Stack: Build and deploy hybrid and edge-computing intelligent manufacturing applications and run them consistently across location boundaries. Azure Synapse Analytics: With Azure Synapse for Analytics, manufacturers can collect, store, process, analyze, and visualize their operational and supply chain data of any variety, volume, or velocity to pave your way to smart manufacturing. High Performance computing: Rapidly iterate on product design to reduce time to market and improve product quality with scalable and secure on-demand infrastructure. Azure Mixed Reality: Blend your digital world and physical world to create immersive, collaborative experiences across the manufacturing floor and beyond, by for instance, using the Microsoft HoloLens 2. SAP on Azure: Run your IoT manufacturing and supply chain operations with SAP workloads on Azure. Azure Virtual Desktop: Enable a secure remote desktop experience from virtually anywhere. Azure for financial services For financial services, technology will become a transformative tool that enables everything from contactless cards, new forms of payment to defeating financial crime. AI will become the foundation for driving intelligent banking for all financial services organizations.\nThis will also improve the ability to deliver new customer experiences, empower employees, and drive innovation in security, compliance and regulatory environments. With Azure for financial services, financial organizations can transform their company with the speed and security of the cloud. They can offer new customer experiences, enhance risk management, and fight fraud.\nAzure for financial services includes the following tools and services:\nAzure AI and Machine Learning: Create financial models and build the next generation of intelligent financial services with proven, secure, and responsible AI. Azure Synapse Analytics: Gather, store, process, analyze, and visualize your financial data of any variety, volume, or velocity to pave your way to intelligent banking. High Performance Computing: Execute complex calculations and large datasets in seconds with scalable and highly secure on-demand infrastructure. Azure Virtual Desktop: Quickly deploy virtual desktops and apps to enable secure remote work. Azure Stack: Extend your Azure capabilities across datacenters, edge locations, and remote offices to build and run hybrid applications with more flexibility. Azure Security: Gain protection with built-in cloud protection and stay ahead of risks with AI-powered monitoring tools. Azure for government The need for greater efficiencies and better communication with citizens is making digital transformation initiatives crucial across local, regional, and national governments. Especially in crisis situations.\nWe can expect that in the near future governments are going to deepen their reliance on remote access, empowering cross-agency collaboration, and delivering trusted and secure services to engage with citizens better and more safely and improve citizen public health. Azure for government supports to meet the needs of citizens and helps to innovate faster. And on top of that government cloud security and compliance standards.\nAzure for government includes the following tools and services:\nAzure AI and Machine Learning: With Azure AI and Machine Learning, governments can build mission-critical solutions that can analyze images, comprehend speech, and make predictions using proven, secure, and responsible AI capabilities. Azure Synapse Analytics: Query data on your terms, using serverless or provisioned resources at scale. Azure Stack: Extend Azure services and capabilities to the environment of choice. This can be from the datacenter to edge locations and remote offices. High Performance Computing: Execute complex calculations and large datasets in seconds with scalable and highly secure on-demand infrastructure. Azure Virtual Desktop: Easily and securely access corporate applications, data, and resources from anywhere on any device. SAP on Azure: Run your mission-critical SAP workloads in the cloud. Azure Security: Gain protection with built-in cloud protection and stay ahead of risks with AI-powered monitoring tools. Azure Sentinel: Make your threat detection and response smarter and faster with AI. Eliminate security infrastructure setup and maintenance and elastically scale to meet your government cloud security needs. Azure IoT: Securely connect assets and equipment to the cloud with solutions that unlock real-time insights and enable system interoperability. Wrap up In this second part of the series of blogs about Azure for Industries, we have covered manufacturing, financial services and governments. Also, for these industries, Microsoft offers a comprehensive toolset.\nImplementing these cloud solutions on a large scale and embedding it into your organization, involves a structured approach, a cultural shift, and a need for a solid and secure architecture. This also includes setting up a Cloud Center of Excellence, and embracing cloud native technologies and the Microsoft Well Architected Framework.\n","permalink":"//localhost:1313/posts/2021-11-08-adapt-and-thrive-with-microsoft-azure-for-industries-part-2/","summary":"\u003cp\u003eThis is the second article in a series about the different solutions that Microsoft Azure has to offer for specific industries. In my first post, I covered how these industry specific solutions help organizations to accelerate their digital transformation, and why this is so important nowadays.\u003c/p\u003e\n\u003cp\u003eIn this second part, we will look at the manufacturing, financials services and government industries.\u003c/p\u003e\n\u003ch2 id=\"azure-for-manufacturing\"\u003eAzure for manufacturing\u003c/h2\u003e\n\u003cp\u003eIn the upcoming years, instead of selling unconnected products, manufacturers will start focusing more on selling products that come with connectivity services. They will also move from selling a discrete product to selling products as a service.\u003c/p\u003e","title":"Adapt and thrive with Microsoft Azure for Industries – part 2"},{"content":"Sustainability and humanity\u0026rsquo;s response to climate change is one of the biggest challenges of our lifetime. And luckily, this is becoming more important for most companies, across all different industries.\nAt Capgemini we set a bold ambition: We will achieve carbon neutrality no later than 2025 and become a net zero business by 2030. This means accelerating our approach to carbon reduction across our key impact areas. And together with reducing our own environmental impacts for a net zero future, we also want to help our clients to transform to a low carbon future.\nAnd one area where organizations can become more carbon efficient is by using public cloud services, such as Microsoft Azure. A study from 2018 found that using the Microsoft Azure cloud platform can be up to 93 percent more energy efficient and up to 98 percent more carbon efficient than on-premises solutions. This study looked at four services, which are responsible for nearly half of the energy consumed in the Microsoft datacenters: Azure Compute, Azure Storage, Exchange Online, SharePoint Online. In January 2020, Microsoft announced that they will be carbon negative by 2030.\nTo help achieve this goal, the Microsoft Azure data centers provide an incredible opportunity to give back to all the communities where they operate. And for this, Microsoft Azure has committed to focus on the following key areas of environmental impact:\nCarbon: 100% renewable energy by 2025\nWater: Water positive by 2030, replenish more water than which is consumed\nWaste: Zero waste certification by 2030\nEcosystems: Net-zero deforestation from new construction\nThese commitments and investments will not only help Microsoft to reach their goals and reduce their environmental impact, but it will also bring huge opportunities for all clients and partners to achieve their own sustainability goals.\nAnd in addition to the benefits that on-premises datacenter migrations to cloud solutions can bring, enterprises are also looking for additional insights into the carbon impact of their cloud workloads. These impacts can then help them to make more sustainable computer decisions. Microsoft has released a Power BI application called the Microsoft Sustainability Calculator, to provide clients those new insights.\nThe Microsoft Sustainability Calculator for valuable insights The Microsoft Sustainability Calculator gives clients valuable insights about their carbon emissions data associated with their Azure services. It gives the ability to quantify the carbon impact of each Azure subscription over a period, datacenter regions, and the estimated savings from running those workloads in on-premises datacenters vs Azure.\nThe calculations are based on the Azure consumption, and it uses the energy requirements of the Azure services, the energy mix of the electric grid serving the hosting datacenters, Microsoft\u0026rsquo;s procurement of renewable energy in those datacenters and the emissions associated with the transfer of data over the internet as input. The result is an estimate of the greenhouse gas (GHG) emissions, measured in total metric tons of carbon equivalent (MTCO2e) related to a customer\u0026rsquo;s consumption of Azure.\nThe calculator will also give a granular overview of all the estimated emissions savings from your running workloads on Azure. For that it takes into account, Microsoft\u0026rsquo;s IT operational efficiency, IT equipment efficiency, and datacenter infrastructure efficiency compared to that of a typical on-premises deployment.\nGet started with the Microsoft Sustainability Calculator Azure Enterprise customers can get started with the Microsoft Sustainability Calculator by downloading the Power BI application from AppSource. There, you can download and install the application.\nOne thing to note, is that you need to have access to the Azure Enterprise Portal as an admin and you need to have the current tenant ID of your Azure tenant.\nTo get started, you can use the following sources:\nMicrosoft Sustainability Calculator: https://appsource.microsoft.com/en-us/product/power-bi/coi-sustainability.sustainability_dashboard\nMicrosoft Sustainability Calculator Support Page: https://docs.microsoft.com/en-us/power-bi/connect-data/service-connect-to-microsoft-sustainability-calculator\nAn excellent way to become more carbon efficient In this blog I have highlighted our sustainable transformation goals at Capgemini and how we want to help our clients to transform to a low carbon future. Embracing cloud services and migrate workloads from on-premises datacenters to the cloud, is one step that will definitely have a positive impact at achieving these goals.\nIn line with Capgemini\u0026rsquo;s sustainable transformation and vision, Microsoft also commits to a sustainable future and has invested heavily in achieving this. Microsoft Azure brings you the benefits of these investments and commitments to help achieve your own sustainability goals.\nIf you want more information about our investments and commitments to environmental sustainability, you can look at our website, or you can download our Environmental Sustainability report.\n","permalink":"//localhost:1313/posts/2021-11-05-sustainability-cloud-computing-with-microsoft-azure/","summary":"\u003cp\u003eSustainability and humanity\u0026rsquo;s response to climate change is one of the biggest challenges of our lifetime. And luckily, this is becoming more important for most companies, across all different industries.\u003c/p\u003e\n\u003cp\u003eAt Capgemini we set a bold \u003ca href=\"https://www.capgemini.com/our-company/our-corporate-social-responsibility-program/environmental-sustainability/\"\u003eambition\u003c/a\u003e: We will achieve carbon neutrality no later than 2025 and become a net zero business by 2030. This means accelerating our approach to carbon reduction across our key impact areas. And together with reducing our own environmental impacts for a net zero future, we also want to help our clients to transform to a low carbon future.\u003c/p\u003e","title":"Sustainability cloud computing with Microsoft Azure "},{"content":"The way we work and live has changed. As a result of the recent global health crisis, many organizations are accelerating their digital transformation efforts to meet the challenges that they were exposed to. Employees around the world shifted to remote work, stores needed to shift to a buy online, pick up in-store model.\nIn manufacturing, remote capabilities became key. Digital transformation has now become a requirement for business continuity, and digital technology has universally become key to business resilience and transformation. This enormous pressure on digital transformation, since the pandemic struck, has affected all industries.\nA recent study from the Economist and Microsoft shows that organizations that already started their digital transformation before the pandemic, were able to adjust more quickly to meet the new customer’s needs. The digital infrastructure that these industries already put in place, allowed them to not only remain competitive in the market, but also respond to societal disruption in a nimbler way.\nTo meet the challenges of a rapidly changing economy, across all different industries, it is no longer sufficient to just adopt technology. They need to build their own technology to compete and grow. And this makes every company a technology company.\n“We’ve seen two years’ worth of digital transformation in two months. From remote teamwork and learning, to sales and customer service, to critical cloud infrastructure and security—we are working alongside customers every day to help them adapt and stay open for business in a world of remote everything.”\n— Satya Nadella, CEO, Microsoft\nOver the past year, Microsoft has been working closely with leaders in every industry to help them navigate the crisis. They equipped them with the right technology and tools to accelerate the digital transformation. Industry specific solutions are the key to ensure business resiliency and accelerating growth.\nIn this first part of a series blogs about industry solutions build on the Microsoft Azure platform, I’m going to focus on retail and healthcare solutions.\nAzure for retail Gone are the days when retailers choose when, where, and what to sell. In this changing world, retailers are being challenged to figure out how best engage with customers within new constraints. One example of this is adapting business processes to provide BOPIS (buy-online-and-pickup-in-store- services).\nAlso, many retailers are driving a more sustainable model proliferation of data, are including the ability to deliver remote sales and services and are addressing the need to better equip store associates with technology.\nWith Azure for retail, organizations are capable of building personalized experiences, optimize their supply chains, and reimagine multichannel retail using Microsoft Azure. This includes predictive AI, machine learning, IoT, hybrid cloud, computer vision and analytics.\nAzure for Retail includes the following tools and services:\nAzure Synapse Analytics: With Azure Synapse for Analytics, retailers can gather, store, process, analyze, and visualize data of any volume, variety, or velocity to pave the way to intelligent retail. Azure AI and Machine Learning: With these tools, retailers can build intelligent, personalized customer experiences and optimized systems powered by the proven, responsible, and secure AI that Microsoft Azure has to offer. Azure Cognitive Services: Bring AI within the reach of every developer, without the need to require machine-learning expertise. Azure IoT: Enabling smart retail through Azure IoT. Retailer can securely connect their assets and equipment to the retail cloud with the different IoT solutions that Azure has to offer to unlock real time insights and enable system interoperability. Azure Mixed Reality: Blend the digital and the physical worlds to create immersive, and collaborative experiences. Azure Stack: Build and run hybrid applications across datacenters, edge locations, and the cloud. Azure Virtual Desktop: Deploy virtual desktops and apps to enable secure remote work. SAP on Azure: Run your organizations retail operations with SAP workloads on Azure to increase agility, drive strategic innovation and perform at scale. Azure for healthcare The healthcare industry is highly regulated. In this field, digital change is driven by the need to lower the risk of delivering patient care, while doing so at scale. The rapidly changing world is pressuring healthcare organizations to evolve how they deliver patient care as well.\nOne example of this, is that healthcare organizations are increasingly using technology platform to shift to more telehealth services. And this example is very spot on as well in the current pandemic. By meeting patients virtually, healthcare professionals can treat (more) patients while minimizing the risk of exposure to themselves and other patients.\nWith Azure for healthcare, healthcare organizations can deliver better health insights and outcomes as they enhance patient engagement, empower health team collaboration, and improve clinical informatics and operational insights. All backed by a secure and trusted cloud. This includes hybrid cloud, mixed reality, AI, and IoT—to drive better health outcomes, improve security, scale faster, and enhance data interoperability.\nAzure for Healthcare includes the following tools and services:\nAzure Healthcare APIs: With Azure healthcare specific APIs, organizations can securely manage different formats of protected health data, accelerate machine learning, and enable a secure exchange of health data within a global infrastructure. Azure AI and Machine Learning: Deliver better healthcare outcomes with personalized, preventative care, and intelligent systems powered by proven, secure, and responsible AI. Azure Synapse Analytics: With Azure Synapse for Analytics, healthcare organizations can gather, store, process, analyze, and visualize clinical data of any volume, variety, or velocity to pave the way to smart healthcare. Azure IoT: Deliver personalized care, empower care teams and employees, and improve operational outcomes. Securely connect health devices and equipment to the cloud with healthcare solutions to unlock real-time insights and enable system interoperability. High Performance Computing: Accelerate insights in genomics precision medicine and clinical trials with near-finite high performance bioinformatics infrastructure. Azure Stack: Build and run hybrid applications across datacenters, edge locations, remote clinical facilities, and the cloud. Azure Mixed Reality: Blend the digital and the physical worlds to create immersive, and collaborative experiences, across the operating room and other health facilities. Azure Security: Protection from the edge to the cloud and stay ahead of risks with intelligent monitoring tools built with powerful AI. Wrap up Microsoft offers a comprehensive toolset for the retail and healthcare industries. Implementing these cloud solutions on a large scale and embedding it into your organization, involves a structured approach, a cultural shift, and a need for a solid and secure architecture.\n","permalink":"//localhost:1313/posts/2021-11-01-adapt-and-thrive-with-microsoft-azure-for-industries-part-1/","summary":"\u003cp\u003eThe way we work and live has changed. As a result of the recent global health crisis, many organizations are accelerating their digital transformation efforts to meet the challenges that they were exposed to. Employees around the world shifted to remote work, stores needed to shift to a buy online, pick up in-store model.\u003c/p\u003e\n\u003cp\u003eIn manufacturing, remote capabilities became key. Digital transformation has now become a requirement for business continuity, and digital technology has universally become key to business resilience and transformation. This enormous pressure on digital transformation, since the pandemic struck, has affected all industries.\u003c/p\u003e","title":"Adapt and thrive with Microsoft Azure for Industries – part 1"},{"content":"Sjoukje Zaal is haar cariere begonnen in de gezondheidszorg maar is nu werkzaam als CTO voor Microsoft bij Capgemini, is Microsoft Most Valuable Professional en Microsoft Regional Director.\nHoe ben je in de IT terecht gekomen?\nOp jonge leeftijd kreeg ik van mijn broer zijn oude Commodore 64. Daar speelde ik games op en ik programeerde ook spelletjes. Heel erg leuk om te doen! Gamen ben ik altijd heel actief blijven doen maar ik heb het programmeren losgelaten en ben me op andere hobbies gaan richten zoals het maken van muziek.\nMijn cariere ben ik niet begonnen in de IT maar in de gezondheidszorg. Als vakantiebaan heb ik jaren in de thuiszorg gewerkt en daarna heb ik de opleiding gedaan voor Social Pedagogisch Werk. Na het behalen van mijn diploma ben ik aan het werk gegaan als begeleider van verstandelijk gehandicapten. Na een aantal jaren in de zorg gewerkt te hebben kwam ik tot de conclusie dat ik dat niet tot mijn pensioen wilde blijven doen. Ik drumde in die tijd in een heavy metal band en ben toen daarvoor een website gaan maken. Er ging een hele nieuwe wereld voor mij open. Ik vond het echt zo onwijs leuk om te zien hoeveel creativiteit je hierin kwijt kunt. Je kunt zulke mooie dingen maken en programmeren, de mogelijkheden zijn eindeloos.\nToen ben ik door middel van zelfstudie HTML, CSS, en Javascript gaan leren. Heel lastig, echt een andere manier van denken! Maar ik heb doorgezet, ben toen doorgegaan met het leren van C# en ben me op Microsoft technologien gaan richten. Ik heb zonder opleiding een baan gevonden in de IT als developer. Helaas was ik hierin minder succesvol, het lukte mij niet om een rekenmachine te programmeren. Ik werd ontslagen en moest weer op zoek naar een nieuwe baan. Na lang zoeken is dit gelukt, en in die baan was ik gelukkig meer succesvol. Overdag werkte ik als developer, en in de avonden studeerde ik voor het behalen van Microsoft examens.\nWat doe je nu?\nIk heb het traditionele pad doorlopen, van developer naar architect, van architect naar principal architect en ben nu werkzaam als CTO bij Capgemini op het gebied van Microsoft. In deze rol ben ik verantwoordelijk voor de strategie en visie op het gebied van Microsoft technologieën binnen Capgemini Nederland. Mijn werkzaamheden bestaan voornamelijk uit het adviseren van onze klanten over de inzet van Microsoft technologien, ik ben soms ook ingezet als chief architect op projecten en daarnaast onderhoud ik contacten met Microsoft vanuit ons partnership. Thought leadership is ook een groot onderdeel van mijn dagelijkse werkzaamheden bij Capgemini. Hieronder valt het schrijven van blogs en het presenteren op conferenties.\nDaarnaast ben ik sinds 2012 heel erg actief binnen de Microsoft community. Dit doe ik op persoonlijke titel en staat los van mijn werkzaamheden voor Capgemini. Microsoft heeft mij in 2015 hiervoor de Most Valuable Professional award gegeven. Deze award wordt jaarlijks uitgerijkt aan professionals die hun kennis met veel passie delen met de Microsoft community. In 2019 heb ik daarnaast ook de Microsoft Regional Director award gekregen, een titel die uitgereikt wordt aan professionals in leidinggevende posities. Het is een selecte groep van ongeveer 160 onafhankelijke technologie \u0026lsquo;visionairs\u0026rsquo; wereldwijd. Deze groep mensen zijn door Microsoft uitgekozen vanwege hun bewezen expertise op verschillende platformen en hun betrokkenheid bij de community.\nWat zijn volgens jou de voordelen van diversiteit op de werkvloer\nDiversiteit zorgt voor meer creativiteit en voor vernieuwing. Dit zijn cruciale onderdelen van het vak. In de IT gaan de ontwikkelingen heel erg snel. Om al deze technologien met elkaar te kunnen verbinden en hier een werkende oplossing van te kunnen creeëren, is diversiteit absoluut noodzakelijk.\nDoor de inzet van diverse teams binnen IT projecten kan er dus vanuit een breder aspect naar problemen en oplossingen gekeken kunnen worden. Dit zorgt dan voor andere inzichten binnen het team en dus ook voor meer innovatie. Daarnaast zullen teamleden met uiteenlopende achtergronden ook beter weten wat er speelt bij klanten en de doelgroep voor wie je een oplossing ontwikkeld.\nPersoonlijk vind ik het werken met mensen met verschillende achtergronden ook heel inspirerend en leerzaam. Bij Capgemini werken veel mensen met verschillende culturen en achtergronden. Ik leer dagelijks van de verschillende invalshoeken die dat met zich meebrengt en ook in de verschillen waarop besluiten genomen worden.\nHeb je tips voor vrouwen die in de IT willen werken? / Wat zou jij vrouwen in de IT willen meegeven?\nIT is tegenwoordig overal. Er wordt ook niet voor niets gezegd dat iedere organisatie tegenwoordig een IT organisatie is. Dit brengt heel veel mogelijkheden met zich mee. Er is een enorme vraag naar verschillende rollen binnen de IT. Rollen waarin ook heel veel creativiteit, communicatie en innovatie nodig is. Daarvoor zijn vrouwen ook echt nodig binnen de IT.\nJe hoeft echt geen dy-hard programmeur te zijn om een leuke en succesvolle baan in de IT te hebben. Laat je eens goed adviseren over wat voor functies er allemaal zijn binnen de IT. Iedere dag is anders en er zijn heel veel mogelijkheden om jezelf te blijven ontwikkelen. En ook veel mogelijkheden om een andere richting binnen de IT te kiezen als je uitgekeken bent op je huidige rol en functie.\n","permalink":"//localhost:1313/posts/dutch-women-in-tech---sjoukje-zaal/","summary":"\u003cp\u003eSjoukje Zaal is haar cariere begonnen in de gezondheidszorg maar is nu werkzaam als CTO voor Microsoft bij Capgemini, is Microsoft Most Valuable Professional en Microsoft Regional Director.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eHoe ben je in de IT terecht gekomen?\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eOp jonge leeftijd kreeg ik van mijn broer zijn oude Commodore 64. Daar speelde ik games op en ik programeerde ook spelletjes. Heel erg leuk om te doen! Gamen ben ik altijd heel actief blijven doen maar ik heb het programmeren losgelaten en ben me op andere hobbies gaan richten zoals het maken van muziek.\u003c/p\u003e","title":"Interview Dutch Women in Tech"},{"content":"The October event was a collaboration with our sponsor Capgemini. This was a fully virtual event recorded in our studio and streamed to our YouTube channel. This time we had three speakers:\nArjan Nieuwenhuis: Keeping track on all your apps Managing your apps during the whole software development lifecycle and beyond is hard as apps come in all shapes and sizes and can target multiple platforms. Keeping track on your apps from early stage development to production can be a handful, especially as target platforms are whole ecosystems.\nTo help you bringing these ecosystems together Visual Studio App Center provides a integrated service on Azure. Assisting in all the stages of the software development lifecycle, bringing it all together in one integrated place.\nJoin me in exploring the current state of Visual Studio App Center and sneak peek on what is coming soon.\nhttps://www.youtube.com/watch?v=9_Nl4EPUh4k\nMarilag Dimatulac: Applying Message Brokering \u0026amp; Event-Driven Architecture on Azure The most fun part with any cloud architecture styles is seeing it in action. In this session, I\u0026rsquo;ll demonstrate how we can design and implement event-driven applications on the cloud. We\u0026rsquo;ll look into several building blocks, from using event modeling to communicate our specification, to implementing in-process messaging in .Net Core with the mediator pattern, and finally using Queues \u0026amp; Event Grid on Azure to wire things together and complete our story.\nhttps://youtu.be/tG-V_oQVHsk\nJohn Papa: You\u0026rsquo;ve built an app and you want it to scale. Do you want CI/CD, custom domains, SSL certificates, APIs, global scale of your static in assets, authentication, and authorization? And whether your favorite web framework is Svelte, Vue, React, Angular or something else entirely, you\u0026rsquo;ll learn how to build a static web app on Azure and go from GitHub repo to global scale.\nhttps://youtu.be/bEQZE6MtYSo\n","permalink":"//localhost:1313/posts/2021-10-09-azure-thursday-october-2021/","summary":"\u003cp\u003eThe October event was a collaboration with our sponsor Capgemini. This was a fully virtual event recorded in our studio and streamed to our YouTube channel. This time we had three speakers:\u003c/p\u003e\n\u003ch2 id=\"arjan-nieuwenhuis-keeping-track-on-all-your-apps\"\u003eArjan Nieuwenhuis: Keeping track on all your apps\u003c/h2\u003e\n\u003cp\u003eManaging your apps during the whole software development lifecycle and beyond is hard as apps come in all shapes and sizes and can target multiple platforms. Keeping track on your apps from early stage development to production can be a handful, especially as target platforms are whole ecosystems.\u003c/p\u003e","title":"Azure Thursday - October 2021"},{"content":"The September event was the first event after summer break and our first in-person event since March 2020! Of course, we streamed the event to our YouTube channel as well. This time we have two amazing speaker again:\nStacey Cashmore: Building your personal online brand using Static Blazor Apps, one step at a time We\u0026rsquo;re told that by using services such as Medium, dev.to etc that we are diluting our personal brand. That we should be posting to our own site and building ourselves up. But making that move can seem huge! Rather than eating the elephant - which can seem impossible - how about slowly moving to your own space? Build your brand whilst still using those great features that attracted you to your chosen platform in the first place. With the joint power of client-side Blazor and static web apps in Azure you no longer need to run web servers or App Services in order to host your site online. You can get set up in minutes! In this session we\u0026rsquo;ll set up a new Blazor Wasm Application and Azure Function, deploy to Azure and then fetch blog posts from dev.to to link to them in our page. We\u0026rsquo;ll also talk about next steps - and how to eat the elephant one bite at a time - to create your personal brand!\nhttps://www.youtube.com/watch?v=XJqc7YdKl_k\nMarcel de Vries: Azure Cloud Transformation done right The past years we have been involved in many so-called cloud transformation projects. Many of those projects had big issues, and we learned a ton of things how not to do the transition! Think about things that will cause problems like: A cloud Competence Center, A Cloud Platform team, Asking the operations team how to move to the cloud, etc. These are common mistakes made every day that prevent you from gaining traction and speed. You might succeed, but you will go through a lot of pain and suffering. In this session, I will share an approach that we build based on our mistakes and learnings that will help you move to the cloud at speed. I will share the organizational, human an cultural side of things, a strategy on how to onboard teams and how to avoid common mistakes that are even promoted as best practices by high paid cloud consultants! I will also cover some technical details like setting up subscriptions, permissions and tools in the right way, so it won’t hurt you after your 10th team is onboarding! This session is all about what we learned and how you can avoid these issues yourself.\nhttps://www.youtube.com/watch?v=Ax_maMJeU9s\nWatch the stream You can watch the full recording of the stream here:\nhttps://www.youtube.com/watch?v=Uo7hcdC0s2k\n","permalink":"//localhost:1313/posts/2021-09-29-azure-thursday-september-2021/","summary":"\u003cp\u003eThe September event was the first event after summer break and our first in-person event since March 2020! Of course, we streamed the event to our YouTube channel as well. This time we have two amazing speaker again:\u003c/p\u003e\n\u003ch2 id=\"stacey-cashmore-building-your-personal-online-brand-using-static-blazor-apps-one-step-at-a-time\"\u003eStacey Cashmore: Building your personal online brand using Static Blazor Apps, one step at a time\u003c/h2\u003e\n\u003cp\u003eWe\u0026rsquo;re told that by using services such as Medium, dev.to etc that we are diluting our personal brand. That we should be posting to our own site and building ourselves up. But making that move can seem huge! Rather than eating the elephant - which can seem impossible - how about slowly moving to your own space? Build your brand whilst still using those great features that attracted you to your chosen platform in the first place. With the joint power of client-side Blazor and static web apps in Azure you no longer need to run web servers or App Services in order to host your site online. You can get set up in minutes! In this session we\u0026rsquo;ll set up a new Blazor Wasm Application and Azure Function, deploy to Azure and then fetch blog posts from dev.to to link to them in our page. We\u0026rsquo;ll also talk about next steps - and how to eat the elephant one bite at a time - to create your personal brand!\u003c/p\u003e","title":"Azure Thursday - September 2021"},{"content":"On September 23 after our summer break, we organized another Global XR Talks! This time we had the following awesome session:\nZaid Zaim: Mixed Reality for Cultural Heritage Zaid is going to share his journey and experience on how he came to Mixed Reality and working with HoloLens.\nThen He will take the audience on a journey to the ancient city of Palmyra (UNSECO Cultural Heritage site in Syria) and will demonstrate within his talk a live demo with HoloLens 2 bringing Palmyras monuments back to life.\nWatch the recording You can watch the recording of the stream here:\nhttps://www.youtube.com/watch?v=ptTLMjh5X4o\n","permalink":"//localhost:1313/posts/2021-09-29-global-xr-talks-september-2021/","summary":"\u003cp\u003eOn September 23 after our summer break, we organized another Global XR Talks! This time we had the following awesome session:\u003c/p\u003e\n\u003ch2 id=\"zaid-zaim-mixed-reality-for-cultural-heritage\"\u003eZaid Zaim: Mixed Reality for Cultural Heritage\u003c/h2\u003e\n\u003cp\u003eZaid is going to share his journey and experience on how he came to Mixed Reality and working with HoloLens.\u003cbr\u003e\nThen He will take the audience on a journey to the ancient city of Palmyra (UNSECO Cultural Heritage site in Syria) and will demonstrate within his talk a live demo with HoloLens 2 bringing Palmyras monuments back to life.\u003c/p\u003e","title":"Global XR Talks - September 2021"},{"content":"Cloud computing is redefining how modern services and applications are designed. Instead of building traditional monoliths, applications are broken down into smaller, decentralized microservices. And instead of running on a traditional server, they are using serverless services where you don\u0026rsquo;t have to worry about infrastructure at all and only pay for the services you actually use. Microservices typically communicate with other microservices by through APIs and asynchronous messaging. This allows applications to scale more efficiently, be more cost effective and easier to replace and deploy. Cloud platforms and microservices ask for a different type of architecture.\nThe are many cloud providers in the market, but Microsoft Azure is one of the biggest, used by 95 percent of the Fortune 500 companies. Microsoft provides a lot of resources on how to architect and develop applications and services on their platform, which will be covered in the next sections.\nAzure Architecture Center The Azure Architecture Center is a great starting point for everyone who wants to start developing services and applications on Azure. It offers proven guidance for architecting solutions on Azure including patterns and practices. You can use the technology choices and guides to decide which services are right for your solution. The guides include all aspects for building apps and services for the cloud, such as operations, reliability, performance, security, and cost optimization. There are a couple of sections that I want to highlight here:\nReference Architectures: Microsoft provides a collection of reference architectures that you can use as a starting point for building your own applications and services. This also gives architects a lot of in-depth information about how to combine the different services.\nCloud best-practices: This guide presents structured approach for designing scalable, resilient, secure, and highly available applications and services on Azure. A great starting point for gaining knowledge about building distributed apps.\nTo get started, you can refer to the following overview page: https://docs.microsoft.com/en-us/azure/architecture/\nCloud Adoption Framework The Cloud Adoption Framework is a collection of documentation, best-practices, implementation guidance and tools that can accelerate cloud adoption in organizations. It is focused on different methodologies, such as strategy, planning, readiness, migrations, innovation, governance, managing, and organizing which all together offers a full broad adoption lifecyle framework, which supports organizations throughout each phase of cloud adoption.\nTo get started, you can refer to the following site: https://docs.microsoft.com/en-us/azure/cloud-adoption-framework/\nLanding zones Microsoft offers a set of enterprise scale landing zones that you can use to get started as well. A landing zone is an environment for hosting your workloads, preprovisioned through code. Because no single solution will fit all technical environments, there are a couple of landing zone implementations that you can choose from to get started:\nEnterprise-Scale foundation: This landing zone offers a foundation for organizations to support their application portfolios, regardless of whether the applications are being migrated or are newly developed and deployed to Azure. It enables organizations to start as small as needed and scale alongside their business requirements. It is a good solution when hybrid connectivity to on-premises datacenters is not required from the start.\nEnterprise-scale for small enterprises: This landing zone is similar to the foundation one above. It also allows organizations to start small. This reference implementation is aimed at organizations that do not have a large IT team and do not require fine grained administration delegation models.\nEnterprise-Scale with Azure VWAN: This landing zone offers a foundation for organizations that want to focus on application portfolios and hybrid connectivity with ExpressRoute or VPN. It also offers an architecture based on an Azure Virtual WAN network topology.\nEnterprise-Scale with hub and spoke architecture: This landing zone example is also for organizations that want to support their application portfolios and add hybrid connectivity with ExpressRoute or VPN. If organizations require hybrid connectivity to on-premises locations from the beginning, you can start with this architecture based on the traditional hub and spoke network topology.\nWell-Architected Framework Lastly, I want to cover the Azure Well Architected Framework. This can be used to improve the quality of your workloads on Azure. The framework consists of five pillars of architecture excellence:\nCost optimization: The principles of cost optimization are a series of important considerations that can help achieve both business objectives and cost justification.\nOperational Excellence: This pillar covers the operations processes that keep an application running in production. If focuses on reliable and automated deployments, which can easily roll back or forward when required.\nPerformance Efficiency: Performance efficiency is the ability of your workload to scale to meet the demands.\nReliability: In the cloud, we acknowledge up front that failures will happen. Instead of trying to prevent failures altogether, the goal is to minimize the effects of a single failing component.\nSecurity: Extremely important nowadays. You should protect your applications and data from threats at all costs.\n","permalink":"//localhost:1313/posts/2021-09-03-getting-started-with-azure-architecture/","summary":"\u003cp\u003eCloud computing is redefining how modern services and applications are\ndesigned. Instead of building traditional monoliths, applications are\nbroken down into smaller, decentralized microservices. And instead of\nrunning on a traditional server, they are using serverless services\nwhere you don\u0026rsquo;t have to worry about infrastructure at all and only pay\nfor the services you actually use. Microservices typically communicate\nwith other microservices by through APIs and asynchronous messaging.\nThis allows applications to scale more efficiently, be more cost\neffective and easier to replace and deploy. Cloud platforms and\nmicroservices ask for a different type of architecture.\u003c/p\u003e","title":"Getting started with Azure architecture"},{"content":"Technology is not only improving people\u0026rsquo;s lives but is also making work easier in many sectors. On top of that, we have all witnessed how the pandemic has accelerated digital transformation. Launching technology-driven initiatives to enable remote work and distance learning, online shopping and more. For most organizations, this shift was not possible without cloud technology.\nCloud is here to stay, and to stay ahead of competition and on top of customer demands, more organizations need to shift to cloud computing. But the only way to truly succeed with cloud, is by embracing it fully across your entire organization. And this is more then just technology. This also means that people and processes need to align to the strategy as well.\nIn the next sections, we are going to look at some important aspects that make up a successful cloud strategy.\nPreparing and training IT staff for the cloud The first important step is to prepare ant train your IT staff. They need to function as change agents for supporting and emerging cloud technologies. The staff needs to have a clear understanding of both their roles and any changes to their current position, and the business case behind it, and they need time and resources to explore the new cloud technologies. This team will need to lead the adoption of cloud services and help the organization understand and embrace all necessary changes.\nEstablish a Cloud Center of Excellence\nEvery business nowadays needs to drive digital transformation to stay ahead of competition. This requires constantly evolving software to keep up with competitors. Therefore, Agile development and DevOps practices need to be in place. DevOps needs immediate availability of infrastructure in place for deployment. And therefore, PaaS and containers are well suited. This makes it very important to the cloud strategy that continuous delivery is well implemented. And one way to establish this is by implementing a Cloud Center of Excellence, which is a control centre which the IT department can implement, supervise and manage its cloud projects operationally. It is responsible for implementing and managing security, compliance, performance, and cost control in a (fully) automatic way.\nCloud enabled applications To fully opt-in for cloud, prioritize cloud migrations for applications that are heavily used in your organization. This will not result in cost-reduction in the beginning, however in the long end it will be cost effective. It enables flexibility and scalability for apps with variable usage. It also enables frequent deployments for these applications. Customer feedback and changes can be incorporated at a rapid pace, which will lead to higher agility and customer satisfaction. For all new applications, a cloud first strategy should be the approach.\nEstablish a cloud culture This last topic is in my opinion the most important one in being successful in cloud. And unfortunately, this is overlooked at many organizations. Not only the IT staff needs to be trained and shifted to a cloud centric approach. When you do a cloud transformation, is most cases it is technology that is the biggest challenge. It is the people and the culture. And this goes beyond your IT staff and the professionals that run the Cloud Center of Excellence. The whole leadership must be in and support and drive these changes in the organization. You need some champions that advocate the new way of working, but also recruit new passionate and skilled people. Continuously measuring progress and outcomes must be a top priority. And don\u0026rsquo;t be afraid to fail and change the approach if needed. Changing the way people think and work, is the most challenging part of cloud adoption.\n","permalink":"//localhost:1313/posts/2021-09-03-onboarding-an-organisation-for-a-fully-cloud-strategy/","summary":"\u003cp\u003eTechnology is not only improving people\u0026rsquo;s lives but is also making work\neasier in many sectors. On top of that, we have all witnessed how the\npandemic has accelerated digital transformation. Launching\ntechnology-driven initiatives to enable remote work and distance\nlearning, online shopping and more. For most organizations, this shift\nwas not possible without cloud technology.\u003c/p\u003e\n\u003cp\u003eCloud is here to stay, and to stay ahead of competition and on top of\ncustomer demands, more organizations need to shift to cloud computing.\nBut the only way to truly succeed with cloud, is by embracing it fully\nacross your entire organization. And this is more then just technology.\nThis also means that people and processes need to align to the strategy\nas well.\u003c/p\u003e","title":"Onboarding an organisation for a fully cloud strategy"},{"content":"\u0026ldquo;It works on my machine\u0026rdquo; - a phrase that every developer has uttered, at least once in his or her career.\nContainers are the solution to the problem of how to get software run reliably when moved between computing environments. From a developer laptop to production environments, or from on-premises environments to cloud environments. By packaging all the required dependencies together with the application, you ensure that it can run in every environment. That is one of the reasons that container deployment is so successful nowadays. They also give you the portability to run exactly the same workloads across different cloud providers. You see that a lot of enterprises are heavily investing in multi-cloud environments. Gartner forecasts that by 2022, more than 75% of global organizations will be running containerized applications in production.\nMicrosoft Azure provides different solutions to deploy and run applications in Azure. In this article I will dive briefly into the various offerings that Microsoft provides for running container deployments and when to use what for your workloads.\nWeb App for Containers With Web App for Containers, you can easily deploy your custom Docker images in Azure App Services and take advantage of the managed platform. Patching and managing is all handled for you by Azure. You can deploy both Windows and Linux containers. By running your containers in Azure App Services, you can also make use of the features that it provides, such as autoscaling, security, load balancing, DevOps capabilities such as continuous deployment from Azure DevOps, GitHub, Docker Hub, and other sources. It also provides package management, staging environments, custom domain, and TLS/SSL certificates and integration with other PaaS services in Azure and Azure AD.\nWhen to use Azure Web App for Containers: If your development team is already familiar with Azure Web Apps, you have one or a few long-running containers/services that are being deployed, you want to deploy container applications quickly, don\u0026rsquo;t want to manage the platform, and run containers on a small scale. And at last, you want to take advantage of built-in auto-scaling capabilities available through Azure Monitor.\nAzure Container instances (ACI) Azure Container Instances is a fully managed service and offers the fastest and simplest way to run a container in Azure, without having to manage any virtual machines and without having to adopt a higher-level service. ACI offer significant startup benefits over virtual machines (VMs). Azure Container Instances can start containers in Azure in seconds, without the need to provision and manage VMs. You can also run Windows and Linux containers in ACI. ACI can also provide \u0026lsquo;virtual nodes\u0026rsquo; to form the backbone of a serverless cluster within AKS.\nWhen to use Azure Container Instances: if you want to get started with containers, you only want to pay based on consumption, have simple requirements and have short-lived workloads that respond to on-demand events or schedules, ACI is a well fitted solution.\nAzure Kubernetes Services (AKS) For scenarios where you need full container orchestration, including service discovery across multiple containers, automatic scaling, and coordinated application upgrades, AKS is recommended. Kubernetes has established itself as the standard for container orchestration nowadays. Azure offers a fully managed service for Kubernetes that offers integrated features such as identity with Azure AD, networking and monitoring capabilities. It also reduces the configuration overhead of the cluster.\nWhen to use Azure Kubernetes Services: if you\u0026rsquo;re looking to run containers at scale with flexible networking and customization options. If you also want your infrastructure to be portable for on-premises and other cloud providers and want to use as many open sources tooling as possible. If you also have a development team that is familiar with Kubernetes, AKS is a good solution.\n","permalink":"//localhost:1313/posts/2021-08-31-azure-containers-overview/","summary":"\u003cp\u003e\u0026ldquo;It works on my machine\u0026rdquo; - a phrase that every developer has uttered, at\nleast once in his or her career.\u003c/p\u003e\n\u003cp\u003eContainers are the solution to the problem of how to get software run\nreliably when moved between computing environments. From a developer\nlaptop to production environments, or from on-premises environments to\ncloud environments. By packaging all the required dependencies together\nwith the application, you ensure that it can run in every environment.\nThat is one of the reasons that container deployment is so successful\nnowadays. They also give you the portability to run exactly the same\nworkloads across different cloud providers. You see that a lot of\nenterprises are heavily investing in multi-cloud environments. Gartner\nforecasts that by 2022, more than 75% of global organizations will be\nrunning containerized applications in production.\u003c/p\u003e","title":"Azure containers overview"},{"content":"Thursday, July 3rd was our last Azure Thursday before summer break! This time, we had the following awesome speakers:\nPatrick van den Born \u0026amp; Rick Stijnman | Deploying Windows 10 Enterprise multi-session with Azure DevOps, Terraform and Packer DevOps, Scrum, infrastructure-as-code, AVD, and cloud: what do they mean to the sysadmin? Many organizations are on the eve of, or already busy with, the migration from a traditional management or project organization to an agile approach, but what does such a transition entail?\nRick will explain the journey from a traditional management organization to a Scrum/DevOps approach. Patrick will then explore the DevOps approach in practical terms, focusing on rolling out Windows 10 Enterprise multi-session (‘Win10MS’) on Azure. You will learn how infrastructure-as-code (via Terraform) and image-as-code (via Packer) can be deployed using Azure DevOps.\nhttps://youtu.be/b30YObS3cFA\nPoornima Nayar | Getting started with Azure Static Web Apps In this session, we will get introduced to Azure Static Web Apps. We will look into what it takes to build a Blazor WebAssembly based Static Web App to Azure. We will first look at what it takes to power it using a REST API while getting into grips with some of the basic concepts about Azure Static Web Apps. After that we will also have a look at how we can get a Static Web App powered by a GraphQL endpoint.\nhttps://youtu.be/024nEM2Aywg\nWatch the stream You can watch the full recording of the stream here:\nhttps://youtu.be/C6vWT5m6G8E\n","permalink":"//localhost:1313/posts/2021-07-06-azure-thursday-july-2021/","summary":"\u003cp\u003eThursday, July 3rd was our last Azure Thursday before summer break! This time, we had the following awesome speakers:\u003c/p\u003e\n\u003ch2 id=\"patrick-van-den-born--rick-stijnman--deploying-windows-10-enterprise-multi-session-with-azure-devops-terraform-and-packer\"\u003ePatrick van den Born \u0026amp; Rick Stijnman | Deploying Windows 10 Enterprise multi-session with Azure DevOps, Terraform and Packer\u003c/h2\u003e\n\u003cp\u003eDevOps, Scrum, infrastructure-as-code, AVD, and cloud: what do they mean to the sysadmin? Many organizations are on the eve of, or already busy with, the migration from a traditional management or project organization to an agile approach, but what does such a transition entail?\u003c/p\u003e","title":"Azure Thursday - July 2021"},{"content":"On June 23, we organized another Global XR Talks! This time we had the following awesome sessions:\nFields of Light In this session, Milan will talk about light fields, what are they, what are they useful for, how do they work and what hardware and software solutions are there currently.\nSpeaker: Milan Pollé\nExtended Reality NFTs: The Future of Art Learn all about Extended Reality NFTs. What is an NFT? How can an NFT be XR? How do I make an NFT? This talk fills you in on all things XR NFT related including information on environmentally friendly NFT blockchains.\nSpeaker: Victoria Del Castillo\nWatch the recording You can watch the recording of the stream here:\nhttps://youtu.be/ZmcDmdniT24\n","permalink":"//localhost:1313/posts/2021-07-06-global-xr-talks-june-2021/","summary":"\u003cp\u003eOn June 23, we organized another Global XR Talks! This time we had the following awesome sessions:\u003c/p\u003e\n\u003ch2 id=\"fields-of-light\"\u003eFields of Light\u003c/h2\u003e\n\u003cp\u003eIn this session, Milan will talk about light fields, what are they, what are they useful for, how do they work and what hardware and software solutions are there currently.\u003cbr\u003e\nSpeaker: Milan Pollé\u003c/p\u003e\n\u003ch2 id=\"extended-reality-nfts-the-future-of-art\"\u003eExtended Reality NFTs: The Future of Art\u003c/h2\u003e\n\u003cp\u003eLearn all about Extended Reality NFTs. What is an NFT? How can an NFT be XR? How do I make an NFT? This talk fills you in on all things XR NFT related including information on environmentally friendly NFT blockchains.\u003cbr\u003e\nSpeaker: Victoria Del Castillo\u003c/p\u003e","title":"Global XR Talks – June 2021"},{"content":"Thursday, June 3rd was Azure Thursday again. This time, we had the following awesome speakers:\nYusuf Yusuf | Monitoring \u0026amp; Debugging Distributed Systems in Azure The development of containers and container orchestrator has made it easier for people to develop distributed systems. Distributed systems have the benefit of making an application more scalable and reliable. But when something goes wrong within that system, it becomes more difficult to pinpoint and fix the issue compared to a standard single system. In this session I will demonstrate how with the help of Azure Monitor, you can not only detect and diagnose issues, but also visualize performance.\nhttps://youtu.be/KhTjYw-G5v0\nJordi van Drunen \u0026amp; Stefan Dingemanse | Introduction to WVD and how to use Azure DevOps for Image Quick introduction to WVD and how to use Azure DevOps for Image and Software management.\nhttps://youtu.be/rUGLQYJg238\nWatch the stream You can watch the full recording of the stream here:\nhttps://youtu.be/_9ReiUqFl4I\n","permalink":"//localhost:1313/posts/2021-06-06-azure-thursday-june-2021/","summary":"\u003cp\u003eThursday, June 3rd was Azure Thursday again. This time, we had the following awesome speakers:\u003c/p\u003e\n\u003ch2 id=\"yusuf-yusuf--monitoring--debugging-distributed-systems-in-azure\"\u003eYusuf Yusuf | Monitoring \u0026amp; Debugging Distributed Systems in Azure\u003c/h2\u003e\n\u003cp\u003eThe development of containers and container orchestrator has made it easier for people to develop distributed systems. Distributed systems have the benefit of making an application more scalable and reliable. But when something goes wrong within that system, it becomes more difficult to pinpoint and fix the issue compared to a standard single system. In this session I will demonstrate how with the help of Azure Monitor, you can not only detect and diagnose issues, but also visualize performance.\u003c/p\u003e","title":"Azure Thursday - June 2021"},{"content":"On May 26 we had an awesome Global XR Talks again! We had the following sessions:\nXR Session 1 - VR/AR Rehabilitation: It’s Not Only About Healthcare An immersive talk on the great initiatives of VR/AR smart solutions for rehabilitation goals. The new rehabilitation is a must: fast, measurable, techy oriented, and goals achieved. In this session, you will learn how to develop the architecture of solution aimed to help the targeted group to reach healthcare goals with existing solutions.\nSpeakers:\nMarina Petrakova - CEO and Co-Founder of VREACH\nLinda Lancere - Digital Physical Therapy, Human Wellbeing Strengthening Implementor\nXR Session 2 – Reimagining Tourism Through Extended Reality With the restart of global tourism due to the Covid-19 pandemic, there has been a considerable demand for the use of technology, especially immersive technologies, to help restart tourism as more users now engage with a virtual experience of destinations they would love to visit online. I will be sharing insights, current solutions on how virtual reality, augmented reality can be used as a massive tool for destination experiences and content development to boost tourism globally.\nSpeaker:\nArome Ibrahim - 360VR Developer, Co-Founder - Experis Immersive\nWatch the recording You can watch the recording of the stream here:\nhttps://www.youtube.com/watch?v=N2jxGj4euZk\n","permalink":"//localhost:1313/posts/2021-05-31-global-xr-talks-may-2021/","summary":"\u003cp\u003eOn May 26 we had an awesome Global XR Talks again! We had the following sessions:\u003c/p\u003e\n\u003ch2 id=\"xr-session-1---vrar-rehabilitation-its-not-only-about-healthcare\"\u003eXR Session 1 - VR/AR Rehabilitation: It’s Not Only About Healthcare\u003c/h2\u003e\n\u003cp\u003eAn immersive talk on the great initiatives of VR/AR smart solutions for rehabilitation goals. The new rehabilitation is a must: fast, measurable, techy oriented, and goals achieved. In this session, you will learn how to develop the architecture of solution aimed to help the targeted group to reach healthcare goals with existing solutions.\u003c/p\u003e","title":"Global XR Talks - May 2021"},{"content":"Thursday, May 6, we organized another Azure Thursday again. This time, we had the following speakers:\nSarah Maston | Project 15 Project 15 from Microsoft (aka.ms/project15) began when Sarah Maston realized that solutions and skills of the commercial solutioning world could help scientific projects be accelerated. She realized this after a safety platform she had invented for safe schools had a new use case of anti-poaching. Additionally, what was the real difference in the prevention of loss from a store from preventing the loss of a pangolin.\nNow entering its second year, Project 15 from Microsoft has partnered with many NGO organizations including the GEF Small Grants Programme implemented by the United Nations Development Programme. This session will cover the origin of Project 15 and how as Sarah and her co-founder Daisuke Nakahara developed the Project 15 Open Platform to accelerate the development of IoT solutions in the scientific space.\nhttps://youtu.be/KsUkvrVHerQ\nSander van de Velde | Solving real world problems using ML.Net and Azure IoT edge Life is strange for IoT devices. They are \u0026lsquo;born\u0026rsquo; in the outside world all alone, or they are on the move, or they are located in places which are hazardous, not easy to access, and nobody is there to nurture them.\nBut still, they have to execute logic locally and make intelligent decisions on their own using eg. ML.\nLuckily, Azure IoT Edge comes to the rescue Zero-touch provisioning and deployment.\nIn this session, attendees learn how Azure IoT Edge is brought to life successfully for intelligence on the edge and how Azure provisioning services help unattended provisioning.\nhttps://youtu.be/Rb7c--w_2K0\nWatch the stream You can watch the full recording of the stream here:\nhttps://youtu.be/AdXoGNBAQwk\n","permalink":"//localhost:1313/posts/2021-05-07-azure-thursday-may-2021/","summary":"\u003cp\u003eThursday, May 6, we organized another Azure Thursday again. This time, we had the following speakers:\u003c/p\u003e\n\u003ch2 id=\"sarah-maston--project-15\"\u003eSarah Maston | Project 15\u003c/h2\u003e\n\u003cp\u003eProject 15 from Microsoft (aka.ms/project15) began when Sarah Maston realized that solutions and skills of the commercial solutioning world could help scientific projects be accelerated. She realized this after a safety platform she had invented for safe schools had a new use case of anti-poaching. Additionally, what was the real difference in the prevention of loss from a store from preventing the loss of a pangolin.\u003c/p\u003e","title":"Azure Thursday - May 2021"},{"content":"On April 21st we had an amazing Global XR Talks again! We had the following session:\nXR Session - Building WebXR Apps for the HoloLens Microsoft recently released the Chromium version of Microsoft Edge for HoloLens. This means that a lot of Web APIs that weren\u0026rsquo;t previously available now can be used, like the WebXR Device API for example. By building a simple game, I\u0026rsquo;ll show you how you can use WebXR and other APIs on the HoloLens.\nSpeaker: Timmy Kokke - Lead Developer ⍟ Software Architect ⍟ Mobile, Web and VR/AR Developer ⍟ Microsoft MVP at Velicus\nAnd of course, after te session we did some virtual socializing in our Global XR Community zone in AltSpaceVR!\nWatch the recording You can watch the recording of the stream here:\nhttps://www.youtube.com/watch?v=6fb08YZ3qSI\n","permalink":"//localhost:1313/posts/2021-04-22-global-xr-talks-april-2021/","summary":"\u003cp\u003eOn April 21st we had an amazing Global XR Talks again! We had the following session:\u003c/p\u003e\n\u003ch2 id=\"xr-session---building-webxr-apps-for-the-hololens\"\u003eXR Session - Building WebXR Apps for the HoloLens\u003c/h2\u003e\n\u003cp\u003eMicrosoft recently released the Chromium version of Microsoft Edge for HoloLens. This means that a lot of Web APIs that weren\u0026rsquo;t previously available now can be used, like the WebXR Device API for example. By building a simple game, I\u0026rsquo;ll show you how you can use WebXR and other APIs on the HoloLens.\u003c/p\u003e","title":"Global XR Talks - April 2021"},{"content":"On April 1st, we had our first Global XR Talks special! This was all about the release of Microsoft Mesh.\nA panel of industry experts discussed this amazing technology and all the functionalities it has to offer. With hot new information straight from Microsoft and well known Microsoft MVPs, host Jesse McCulloch and a brilliant lineup of speakers (Simon Skaria, Dorrene Brown, René Schulte, Ivana Tilca and Joost van Schaik) are joined by our own Saskia Groenewegen and Alexander Meijers for an hour of inspiration and innovation.\nWatch the recording You can watch the recording of the stream here:\nhttps://www.youtube.com/watch?v=QMcmu50VpXE\n","permalink":"//localhost:1313/posts/2021-04-08-global-xr-talks-special-microsoft-mesh-panel/","summary":"\u003cp\u003eOn April 1st, we had our first Global XR Talks special! This was all about the release of Microsoft Mesh.\u003c/p\u003e\n\u003cp\u003eA panel of industry experts discussed this amazing technology and all the functionalities it has to offer. With hot new information straight from Microsoft and well known Microsoft MVPs, host Jesse McCulloch and a brilliant lineup of speakers (Simon Skaria, Dorrene Brown, René Schulte, Ivana Tilca and Joost van Schaik) are joined by our own Saskia Groenewegen and Alexander Meijers for an hour of inspiration and innovation.\u003c/p\u003e","title":"Global XR Talks Special - Microsoft Mesh Panel"},{"content":"April 1st was Azure Thursday again! This time, we had three amazing speakers and sessions:\nJonah Andersson | Things to Consider Before Migrating Old .NET Applications to Cloud Jonah Andersson shares her past experience and important lessons learned about migrating and developing old .NET applications to the Azure cloud. Find out how that project turned into a fiasco not because of Azure but of other factors. Never make the same mistakes.\nhttps://youtu.be/tmL4iuc05kU\nDawuda Iddrisu | Best practices for deploying MERN stack applications on Azure As an engineer, it never ends with development. Deployment is always part of your package. Microsoft Azure has proven to be reliable to deploy just about any application. In this session, Kati Frantz and Dawood Iddris will show you all about deploying your MERN Stack Application and some best practices to follow. A couple of questions that will be answered include should I use PAAS or IAAS, how do I handle the CORS issue, how do I provision a database, do I need a replica of the database, how about latency, how often should I back up my database and so much more. Join us to learn it all along with us in a demo session.\nhttps://youtu.be/RyYKxI_WlSk\nRobin Smorenburg | The Azure Bakery Welcome to \u0026lsquo;The Azure Bakery.\u0026rsquo; In this talk, we\u0026rsquo;ll bake a delicious layered Azure cake. The idea behind this talk is to approach Azure in an easy to understand and fun way. The goal is to engineer and deploy solutions in the best way possible. At the same time, we\u0026rsquo;re trying to keep it simple. We\u0026rsquo;ll bake the foundational layers of every future Azure solution. What is needed when building these solutions?\nhttps://youtu.be/qIfNqGj6LE8\nWatch the stream You can watch the full recording of the stream here:\nhttps://youtu.be/I9sTf_MnNpo\n","permalink":"//localhost:1313/posts/2021-04-03-azure-thursday-april-2021/","summary":"\u003cp\u003eApril 1st was Azure Thursday again! This time, we had three amazing speakers and sessions:\u003c/p\u003e\n\u003ch2 id=\"jonah-andersson--things-to-consider-before-migrating-old-net-applications-to-cloud\"\u003eJonah Andersson | Things to Consider Before Migrating Old .NET Applications to Cloud\u003c/h2\u003e\n\u003cp\u003eJonah Andersson shares her past experience and important lessons learned about migrating and developing old .NET applications to the Azure cloud. Find out how that project turned into a fiasco not because of Azure but of other factors. Never make the same mistakes.\u003c/p\u003e","title":"Azure Thursday - April 2021"},{"content":"Today we publish our first episode of Walking with the RD\u0026rsquo;s!\nTogether with my fellow Microsoft Regional Directors from the Netherlands, Marcel de Vries: CTO of Xpirit, Maarten Goet: Director at Wortell, Maarten Eekels: Chief, Digital Officer and Managing Partner at Portiva, and Andre Carlucci, Global Director of Application Engineering at Kinley, we will do a monthly walk where we will discuss various business related topics.\nIn this first episode, we will talk about the following topic:\n“Working from home’ while still performing. How do we deal with this and what has worked very well for ourselves and the teams we work with.\nWhat are Microsoft Regional Directors? The Regional Director Program provides Microsoft leaders with the customer insights and real-world voices it needs to continue empowering developers and IT professionals with the world\u0026rsquo;s most innovative and impactful tools, services, and solutions.\nIf you want more information about the Microsoft Regional Director program, you can take a look here.\nIf you want to know how we experienced the last year working from home, what are challenges are and how we supported our teams, check out this first episode!\nhttps://youtu.be/0JqTgMqXTYs\n","permalink":"//localhost:1313/posts/2021-03-24-walking-with-the-rd-s/","summary":"\u003cp\u003eToday we publish our first episode of \u003cstrong\u003eWalking with the RD\u0026rsquo;s\u003c/strong\u003e!\u003c/p\u003e\n\u003cp\u003eTogether with my fellow Microsoft Regional Directors from the Netherlands, \u003cstrong\u003eMarcel de Vries\u003c/strong\u003e: CTO of Xpirit, \u003cstrong\u003eMaarten Goet\u003c/strong\u003e: Director at Wortell, \u003cstrong\u003eMaarten Eekels\u003c/strong\u003e: Chief, Digital Officer and Managing Partner at Portiva, and \u003cstrong\u003eAndre Carlucci\u003c/strong\u003e, Global Director of Application Engineering at Kinley, we will do a monthly walk where we will discuss various business related topics.\u003c/p\u003e\n\u003cp\u003eIn this first episode, we will talk about the following topic:\u003c/p\u003e","title":"Walking with the RD's"},{"content":"During our Azure Thursday meetup on March 4, we looked back at the Ignite conference together with some different people over the globe and debate the highlights and new announcements!\nConfirmed guests are:\nSeth Juarez Lars Klint Suzanne Daniels Watch the recording You can watch the recording of the stream here:\nhttps://youtu.be/Js7mF4-JXlw\n","permalink":"//localhost:1313/posts/2021-03-06-azure-thursday-march-2021/","summary":"\u003cp\u003eDuring our Azure Thursday meetup on March 4, we looked back at the Ignite conference together with some different people over the globe and debate the highlights and new announcements!\u003c/p\u003e\n\u003cp\u003eConfirmed guests are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSeth Juarez\u003c/li\u003e\n\u003cli\u003eLars Klint\u003c/li\u003e\n\u003cli\u003eSuzanne Daniels\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"watch-the-recording\"\u003eWatch the recording\u003c/h2\u003e\n\u003cp\u003eYou can watch the recording of the stream here:\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://youtu.be/Js7mF4-JXlw\"\u003ehttps://youtu.be/Js7mF4-JXlw\u003c/a\u003e\u003c/p\u003e","title":"Azure Thursday - March 2021"},{"content":"Our Azure Thursday meetup was a huge success again! This time, we had the following speakers and sessions:\nRory Preddy | Programming for Accessibility Building accessibility into the planning stages of programming can eliminate barriers to participation and create an inclusive environment for people with disabilities. Programming for diversity serves as an unquestionable indicator that your software embraces the diversity of your users and cares about their safety and comfort.\nhttps://www.youtube.com/watch?v=vuP3pBU-Z-k\nAndre van den Berg | Blogging with Markdown and Azure DevOps Explaining the basics of markdown, and then show how you can build a static site with Hugo generated from the markdown files. Then will show how to automate this with Azure DevOps. So first we put the markdown files on a Azure Repo so we can have version control. Then we build a Build pipeline in Azure DevOps to generate the Artifact that we can use in the Release pipeline to Publish the static generated website on Azure Webapps when there is a commit on the master of the Repo.\nhttps://www.youtube.com/watch?v=9BV7VDouLPI\nEsther Barthel \u0026amp; Freek Berson | Empowering ARM and JSON with Project \u0026lsquo;Bicep\u0026rsquo; 70% of all declarative resources created in Azure are done via ARM Templates! ARM Templates are based on JSON and a declarative syntax, but how easy is it to author these? Join Esther and Freek for a fun and demo-heavy session and learn how to empower your ARM Templates with Project \u0026lsquo;Bicep\u0026rsquo;!\nhttps://www.youtube.com/watch?v=eTwfgq_2fLU\n","permalink":"//localhost:1313/posts/2021-02-18-azure-thursday-february-2021/","summary":"\u003cp\u003eOur Azure Thursday meetup was a huge success again! This time, we had the following speakers and sessions:\u003c/p\u003e\n\u003ch2 id=\"rory-preddy--programming-for-accessibility\"\u003eRory Preddy | Programming for Accessibility\u003c/h2\u003e\n\u003cp\u003eBuilding accessibility into the planning stages of programming can eliminate barriers to participation and create an inclusive environment for people with disabilities. Programming for diversity serves as an unquestionable indicator that your software embraces the diversity of your users and cares about their safety and comfort.\u003c/p\u003e","title":"Azure Thursday - February 2021"},{"content":"One of our clients supports several retailers, both local and global, in how to acquire, engage, and retain their customers. They achieve this by providing strategy, tools, and tactics around this. These services are provided to the customer in a digital way. The customer can use web portals, a variety of different services, and can get valuable insights in their data.\nTo bring their services to the next level, and to address the requirements that their customers are having now, and in the future, they decided to take advantage of using the cloud. By leveraging cloud native services, they are able to provide their customers with a set of secure services and give real time insights in data.\nTo support their customers in the most effective way, they decided to host their services on Microsoft Azure.\nThe challenge The challenge that this client is facing is quite similar to the challenges that a lot of organizations are facing right now. There is an urgent need for digital transformation to keep addressing customer needs, be competitive in the market, and being able to innovate and use state of the art technologies. But most services that are offered to their customers are still running on an on-premises infrastructure that is not ready to support this.\nThis was also the challenge that our client was facing. They were providing services that were still running on an on-premises environment, which was not able to provide innovative technologies and scale accordingly to address the future needs.\nThis customer reached out to us to help them implement cloud native services to renew their IT landscape, offer a set of services to their customers that are specifically designed for performance, security and redundancy, and provide real time insights in data coming from various sources. This data is partially stored in Azure, but also still stored in on-premises databases.\nOur approach Together with the client we decided to take advantage of all the cloud native services that Azure has to offer, from a micro-services and data analytics and insights perspective. The project got divided into two smaller projects, starting with building a full cloud native micro-services environment using only serverless technology. This will then be followed by starting a new project for storing customer data using Azure Data Lake, implementing real-time insights using Azure Event Hub, and using various services to provide interactive, immersive dashboards and reports, such as Azure Data Share and other tooling.\nWe decided that our cloud native development offering was most applicable to this project. With our offering, we are providing the following to our clients:\nDomain Driven Design: When implementing a micro-services architecture, Domain Driven Design (DDD) is a design approach where you can benefit from. Where to draw the boundaries is the key task when designing and defining a microservice. DDD patterns help you understand the complexity in the domain.\nCloud Native Design Patterns: To build highly reliable, scalable, secure applications and services, every developer needs to make use of common Cloud Native Design Patterns. We focus fully on implementing Microsoft Best-Practices and Patterns.\nDev/Test optimization: We bring our own development and test environments to the project. For this, we are using container technologies which have all the commonly used tooling and software pre-deployed. Next, to that we are using automated performance and acceptance tests, fully integrated in Azure DevOps.\nEverything-as-code: We are offering out-of-the-box landingzones, which include security and compliance policies, and monitoring rules. Those monitoring rules are based on our experiences and best-practices that we have developed over the years managing cloud environments for our global customers. We are implementing zero-touch deployments using Azure DevOps and CI/CD pipelines for automatically building and releasing applications and services.\nHow we implemented it The first step was to deploy the landingzone which included an API Management gateway, a VNet, Log Analytics, Application Insights, security policies, and default monitoring and logging rules in the Azure subscription. We deployed it automatically using CI/CD pipelines so that it can easily be deployed across different environments. Next, we started building the first APIs, by using serverless services, such as Azure Functions, Azure Storage, Azure Service Bus, an Azure Key Vault, and more. We implemented cloud native design patterns to build them. To get access to the data that still resides in the SAP on-premises environment, an Express Route connection was set up. For authentication, we used Azure Active Directory, Auth 2.0, Open ID Connect and the out-of-the-box libraries that are provided by Microsoft, such as MSAL.\nBy using landingzones, cloud native patterns \u0026amp; Microsoft best practices, and securing it using Azure Policies and Azure Active Directory in our solution, we now have a solid foundation for rapidly building and deploying additional services.\nNext steps At this stage, we now have successfully implemented a set of secure micro-services for the client, which are automatically deployed across environments, securely connecting to an on-premises SAP environment, and exposed via a single gateway. Next, will be implementing the second project, where we will form an additional DevOps team that will implement the solution for storing customer data, and providing real-time insights.\nSummary This blog gave an overview of one of the cloud native projects that we are currently implementing at one of our customers.\n","permalink":"//localhost:1313/posts/2021-01-29-microsoft-azure-a-cloud-native-success-story-copy/","summary":"\u003cp\u003eOne of our clients supports several retailers, both local and global, in how to acquire, engage, and retain their customers. They achieve this by providing strategy, tools, and tactics around this. These services are provided to the customer in a digital way. The customer can use web portals, a variety of different services, and can get valuable insights in their data.\u003c/p\u003e\n\u003cp\u003eTo bring their services to the next level, and to address the requirements that their customers are having now, and in the future, they decided to take advantage of using the cloud. By leveraging cloud native services, they are able to provide their customers with a set of secure services and give real time insights in data.\u003c/p\u003e","title":"Microsoft Azure -- a Cloud Native Success Story"},{"content":"Cloud skills are becoming more popular every day! A lot of organizations are embracing the cloud for their applications, infrastructure, Machine Learning and IoT solutions. And this will grow significally in the next years!\nThis also means that (Microsoft) IT professionals need to update their skills as well. In this article I will give an overview how you can get started updating your skills and be ready for all the Azure work that is coming in the near future!\nCreate a free Azure account The first step, is to create a free Azure account. With this account you can test all the different services and deploy your code to Azure during the different trainings. You get 12 months of free access to all the different Azure services for a limited amount per month.\nYou can create a free Azure account here: Create your Azure free account today | Microsoft Azure. There is also a module on Microsoft Learn that gives you more information about creating a free Azure account, and how billing and support works in Azure. This module is called: Create an Azure account.\nLearn Azure on Microsoft Learn Microsoft Learn offers free, interactive, hands-on training to help you develop Azure technical skills. You can find a variety of learning paths on Microsoft Learn, such as Azure, Microsoft 365, .NET development, Power Platform an more. You can also watch Learn TV, or explore the different Azure certifications from there.\nMicrosoft Learn TV\nFor start learning Azure, the following learning paths are very interesting:\nAzure Fundamentals Learning Paths: Azure Fundamentals part 1: Describe core Azure concepts: This learning path covers the benefits of cloud computing in Azure. It explains cloud concepts such as high availability, scalability, elasticity, agility, and disaster recovery. It also covers geographic distribution concepts such as Azure regions, region pairs, and availability zones. Azure Fundamentals part 2: Describe core Azure services: This covers the different services that are available in Azure, including compute, network, storage, and databases. It also covers virtualization services such as Azure Virtual Machines, Azure Container Instances, Azure Kubernetes Service, and Windows Virtual Desktop. Azure Fundamentals part 3: Describe core solutions and management tools on Azure: Azure offers a wide array of tools and services, such as Azure Artificial Intelligence services, cloud monitoring services, Azure management tools, serverless computing technologies, Azure IoT services. This learning path will help you choose the best one for your business. Azure Fundamentals part 4: Describe general security and network security features: Every application and service needs to be designed with security in mind. This means that having a good security strategy is essential in today\u0026rsquo;s digital world. This learning path covers the different services that can help ensure that your cloud resources are safe, secure, and trusted. Azure Fundamentals part 5: Describe identity, governance, privacy, and compliance features: With the rise of remote work, the primary security boundaries shifted. Understanding who is using your systems and what they have permission to do are critical to keeping your data safe from attackers. This learning path covers the cloud governance strategy, and how to implement regulatory and compliance standards. Azure Fundamentals part 6: Describe Azure cost management and service level agreements: Migrating to the cloud also means that you need to rethink your cost strategy and expenses. This learning path covers factors that influence cost, tools you can use to help estimate and manage your cloud spend. Azure Fundamentals Learning Paths\nThese learning paths will give you a very comprehensive introduction to Azure. By completing the learning path, you will also be ready to take the AZ-900: Microsoft Azure Fundamentals certification.\nLearn Azure using Microsoft Docs Another great source for learning Azure are the Microsoft Docs. I make lots of use it for writing my Azure books. You can find anything that you want to know about Azure there.\nAzure documentation on Microsoft Docs\nLearn Azure using books There is also a variety of books available to learn Azure. You can have a look at Amazon for the different books that are available.\n-Sjoukje\n","permalink":"//localhost:1313/posts/2021-01-14-how-to-learn-azure/","summary":"\u003cp\u003eCloud skills are becoming more popular every day! A lot of organizations are embracing the cloud for their applications, infrastructure, Machine Learning and IoT solutions. And this will grow significally in the next years!\u003c/p\u003e\n\u003cp\u003eThis also means that (Microsoft) IT professionals need to update their skills as well. In this article I will give an overview how you can get started updating your skills and be ready for all the Azure work that is coming in the near future!\u003c/p\u003e","title":"How to learn Azure"},{"content":"Yesterday we had a very interesting Azure Thursday meetup again. We had the following speakers and sessions:\nScott Hanselman | Part II: Moving a 17 year old legacy blog platform to the cloud Scott\u0026rsquo;s blog is super old. The tech is super old. Scott is super old. What happens when he tries to move the whole Hanselman online mess to Azure? Let\u0026rsquo;s talk to him and find out.\nThis is part II in this saga we started last July. Scott will tell us how he succeeded and everyone knows the part II in the saga is even better than the first one.\nhttps://www.youtube.com/watch?v=2bd5V8LwCkE\nMartijn Beenker - The enterprise data lake: monitoring at scale When data lakes grow, they grow in size, complexity and inhabitants - creating complex dependencies across wide ranges of Azure services and different teams. Creating a true operations challenge: how do you find and respond to incidents, and who should respond?\nDuring this session, we’ll look at how to monitor across the boundaries of Azure services and development teams that together make the ecosystem of the data lake. Aided by the Four Golden Signals and Azure Monitor capabilities we’ll create the blueprint for effective monitoring and incident handling. No matter how big the lake.\nhttps://www.youtube.com/watch?v=Be33U7QxsC8\nMenaka Baskerpillai | All about Azure Network Securities In this session Menaka is going to explain how to implement network security in Azure. Networking security plays an important role in Todays world. Implementing security via cloud services provider plays a vital role.\nhttps://www.youtube.com/watch?v=FJr2Q6MbezE\nYou can watch the whole recording of yesterdays stream here:\nhttps://www.youtube.com/watch?v=wSWNNL4JlTs\n","permalink":"//localhost:1313/posts/2021-01-08-azure-thursday-january-2021/","summary":"\u003cp\u003eYesterday we had a very interesting Azure Thursday meetup again. We had the following speakers and sessions:\u003c/p\u003e\n\u003ch2 id=\"scott-hanselman--part-ii-moving-a-17-year-old-legacy-blog-platform-to-the-cloud\"\u003eScott Hanselman | Part II: Moving a 17 year old legacy blog platform to the cloud\u003c/h2\u003e\n\u003cp\u003eScott\u0026rsquo;s blog is super old. The tech is super old. Scott is super old. What happens when he tries to move the whole Hanselman online mess to Azure? Let\u0026rsquo;s talk to him and find out.\u003c/p\u003e\n\u003cp\u003eThis is part II in this saga we started last July. Scott will tell us how he succeeded and everyone knows the part II in the saga is even better than the first one.\u003c/p\u003e","title":"Azure Thursday - January 2021"},{"content":"The AZ-303: Implementing Microsoft Azure Architect Technologies is published. This book is written together with Brett Hargreaves, he updated the original AZ-300 book that was published a year ago.\nWhat you will learn Manage Azure subscriptions and resources Ensure governance and compliance with policies, roles, and blueprints Build, migrate, and protect servers in Azure Configure, monitor, and troubleshoot virtual networks Manage Azure AD and implement multi-factor authentication Configure hybrid integration with Azure AD Connect Find out how you can monitor costs, performance, and security Develop solutions that use Cosmos DB and Azure SQL Database Table of Contents Implementing Cloud Infrastructure Monitoring Creating and Configuring Storage Accounts Implementing and Managing Virtual Machines Implementing and Managing Virtual Networking Creating Connectivity between Virtual Networks Managing Azure Active Directory (Azure AD) Implementing Multi-Factor Authentication (MFA) Implementing and Managing Hybrid Identities Managing Workloads in Azure Implementing Load Balancing and Networking Security Implementing Azure Governance Solutions Creating Web Apps Using PaaS and Serverless Designing and Developing Apps for Containers Implementing Authentication Developing Solutions that Use Cosmos DB Storage You can order the book on Amazon using this link: https://amzn.to/2KzaMyJ\n","permalink":"//localhost:1313/posts/2020-12-29-implementing-microsoft-azure-architect-technologie/","summary":"\u003cp\u003eThe AZ-303: Implementing Microsoft Azure Architect Technologies is published. This book is written together with Brett Hargreaves, he updated the original AZ-300 book that was published a year ago.\u003c/p\u003e\n\u003ch4 id=\"what-you-will-learn\"\u003eWhat you will learn\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eManage Azure subscriptions and resources\u003c/li\u003e\n\u003cli\u003eEnsure governance and compliance with policies, roles, and blueprints\u003c/li\u003e\n\u003cli\u003eBuild, migrate, and protect servers in Azure\u003c/li\u003e\n\u003cli\u003eConfigure, monitor, and troubleshoot virtual networks\u003c/li\u003e\n\u003cli\u003eManage Azure AD and implement multi-factor authentication\u003c/li\u003e\n\u003cli\u003eConfigure hybrid integration with Azure AD Connect\u003c/li\u003e\n\u003cli\u003eFind out how you can monitor costs, performance, and security\u003c/li\u003e\n\u003cli\u003eDevelop solutions that use Cosmos DB and Azure SQL Database\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"table-of-contents\"\u003eTable of Contents\u003c/h4\u003e\n\u003col\u003e\n\u003cli\u003eImplementing Cloud Infrastructure Monitoring\u003c/li\u003e\n\u003cli\u003eCreating and Configuring Storage Accounts\u003c/li\u003e\n\u003cli\u003eImplementing and Managing Virtual Machines\u003c/li\u003e\n\u003cli\u003eImplementing and Managing Virtual Networking\u003c/li\u003e\n\u003cli\u003eCreating Connectivity between Virtual Networks\u003c/li\u003e\n\u003cli\u003eManaging Azure Active Directory (Azure AD)\u003c/li\u003e\n\u003cli\u003eImplementing Multi-Factor Authentication (MFA)\u003c/li\u003e\n\u003cli\u003eImplementing and Managing Hybrid Identities\u003c/li\u003e\n\u003cli\u003eManaging Workloads in Azure\u003c/li\u003e\n\u003cli\u003eImplementing Load Balancing and Networking Security\u003c/li\u003e\n\u003cli\u003eImplementing Azure Governance Solutions\u003c/li\u003e\n\u003cli\u003eCreating Web Apps Using PaaS and Serverless\u003c/li\u003e\n\u003cli\u003eDesigning and Developing Apps for Containers\u003c/li\u003e\n\u003cli\u003eImplementing Authentication\u003c/li\u003e\n\u003cli\u003eDeveloping Solutions that Use Cosmos DB Storage\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eYou can order the book on Amazon using this link: \u003ca href=\"%22https://amzn.to/2KzaMyJ%22\"\u003ehttps://amzn.to/2KzaMyJ\u003c/a\u003e\u003c/p\u003e","title":"Implementing Microsoft Azure Architect Technologies: AZ-303"},{"content":"My new book: Azure DevOps Explained is just released!\nI\u0026rsquo;ve written this book together with Amit Malik, and Stephano Demilliani. It is aimed at developers, solutions architects, and DevOps engineers interested in getting started with cloud DevOps practices on Azure.\nWhat you will learn Get to grips with Azure DevOps Find out about project management with Azure Boards Understand source code management with Azure Repos Build and release pipelines Run quality tests in build pipelines Use artifacts and integrate Azure DevOps in the GitHub flow Discover real-world CI/CD scenarios with Azure DevOps Table of Contents Azure DevOps Overview Managing Projects with Azure DevOps Boards Source Control Management with Azure DevOps Understanding Azure DevOps Pipelines Running Quality Tests in a Build Pipeline Hosting Your Own Azure Pipeline Agent Using Artifacts with Azure DevOps Deploying Applications with Azure DevOps Integrating Azure DevOps with GitHub Using Test Plans with Azure DevOps Real-World CI/CD Scenarios with Azure DevOps You can order the book on Amazon using this link: https://amzn.to/3oOixPu\n","permalink":"//localhost:1313/posts/2020-12-15-azure-devops-explained/","summary":"\u003cp\u003eMy new book: Azure DevOps Explained is just released!\u003c/p\u003e\n\u003cp\u003eI\u0026rsquo;ve written this book together with Amit Malik, and Stephano Demilliani. It is aimed at developers, solutions architects, and DevOps engineers interested in getting started with cloud DevOps practices on Azure.\u003c/p\u003e\n\u003ch4 id=\"what-you-will-learn\"\u003eWhat you will learn\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eGet to grips with Azure DevOps\u003c/li\u003e\n\u003cli\u003eFind out about project management with Azure Boards\u003c/li\u003e\n\u003cli\u003eUnderstand source code management with Azure Repos\u003c/li\u003e\n\u003cli\u003eBuild and release pipelines\u003c/li\u003e\n\u003cli\u003eRun quality tests in build pipelines\u003c/li\u003e\n\u003cli\u003eUse artifacts and integrate Azure DevOps in the GitHub flow\u003c/li\u003e\n\u003cli\u003eDiscover real-world CI/CD scenarios with Azure DevOps\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"table-of-contents\"\u003eTable of Contents\u003c/h4\u003e\n\u003col\u003e\n\u003cli\u003eAzure DevOps Overview\u003c/li\u003e\n\u003cli\u003eManaging Projects with Azure DevOps Boards\u003c/li\u003e\n\u003cli\u003eSource Control Management with Azure DevOps\u003c/li\u003e\n\u003cli\u003eUnderstanding Azure DevOps Pipelines\u003c/li\u003e\n\u003cli\u003eRunning Quality Tests in a Build Pipeline\u003c/li\u003e\n\u003cli\u003eHosting Your Own Azure Pipeline Agent\u003c/li\u003e\n\u003cli\u003eUsing Artifacts with Azure DevOps\u003c/li\u003e\n\u003cli\u003eDeploying Applications with Azure DevOps\u003c/li\u003e\n\u003cli\u003eIntegrating Azure DevOps with GitHub\u003c/li\u003e\n\u003cli\u003eUsing Test Plans with Azure DevOps\u003c/li\u003e\n\u003cli\u003eReal-World CI/CD Scenarios with Azure DevOps\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eYou can order the book on Amazon using this link: \u003ca href=\"%22https://amzn.to/3oOixPu%22\"\u003ehttps://amzn.to/3oOixPu\u003c/a\u003e\u003c/p\u003e","title":"Azure DevOps Explained"},{"content":"Why did you want to become an MVP?\nThe Microsoft community helped me to become the Microsoft professional that I am right now. I wanted to give something back to the community and help others to learn and become a better professional. This is the main reason why I wanted to become an MVP: sharing knowledge with the community, learn from the community, and grow both professionally and personally.\nWhat do you love the most about being an MVP?\nWhen you become an MVP you get to meet a lot of new people that share the same passion. They are all eager to share their knowledge with the community and learn from the community. Together, you can make great things happen such as, delivering great conferences, writing books, organizing global events, and much more.\nI\u0026rsquo;m still learning, meeting new people, and expanding my knowledge every day. New opportunities arise to grow both professionally and personally. And it also resulted in making lots of new friends around the globe.\nWhat are the top three benefits of being an MVP\nFor me the top 3 benefits are:\nBeing able to work with and learn from the top professionals in the field\nTraveling and meeting new people\nWorking closely with Microsoft\nHow has being an MVP improved your standing in the technical community?\nA lot of doors opened up when I became an MVP. In my first year I was asked to write my first book. The second year I started with public speaking and I became part of organizing global events in my third year. All these activities helped me to gain a strong footprint in the Microsoft community.\nHow do contribute to the community?\nI really love what I\u0026rsquo;m doing so I\u0026rsquo;m very active in the community! I write books and blogs, which are mostly Azure related. I\u0026rsquo;m part of organizing lots of global and local events, such as Azure Thursday, Global Azure and Global XR Talks. I\u0026rsquo;m running the community website Tech Daily Chronicle and I\u0026rsquo;m involved in the Global AI Community. I also really like public speaking, so I travel around the globe to speak at conferences and user groups.\n","permalink":"//localhost:1313/posts/why-did-you-want-to-become-an-mvp/","summary":"\u003cp\u003e\u003cstrong\u003eWhy did you want to become an MVP?\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe Microsoft community helped me to become the Microsoft professional that I am right now. I wanted to give something back to the community and help others to learn and become a better professional. This is the main reason why I wanted to become an MVP: sharing knowledge with the community, learn from the community, and grow both professionally and personally.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWhat do you love the most about being an MVP?\u003c/strong\u003e\u003c/p\u003e","title":"Interview MVP Community"},{"content":"These days, applications have become very complex and users are demanding more and more of these applications. They expect innovative features, rapid responsiveness, and zero downtime. And problems that arise with building software, such as performance errors, recurring errors, and the inability to move fast are no longer acceptable by the user. If your application does not meet the user\u0026rsquo;s requirements, they simply move on to the competitor.\nThis means that applications need to be able to address the need for speed and agility. And the solution to this is: A Cloud Native architecture and technologies.\nCloud Native is all about changing the way you think about building and designing critical business systems. Cloud Native systems are specifically designed to respond to large scale, resilience, and rapid change. And they run in modern and dynamic environments, such as public, private, and hybrid clouds. Cloud Native applications are mostly build using one or more of these technologies: Containers, service meshes, microservices, and declarative APIs, running on immutable infrastructure.\nSome companies that have implemented Cloud Native and achieved speed, agility, and scalability are Netflix, Uber, and WeChat. They have thousands of independent microservices running in production and they deploy between hundred and thousand times a day. This architectural style enables them to quickly respond to market demand and conditions. By using a Cloud Native approach, they can instantaneously update small areas of a live, complex application, and individually scale those areas as needed.\nThe speed and agility of cloud native comes from several factors: Cloud infrastructure is key here, but there are five additional pillars that also provide the foundation for building cloud native applications:\nModern Design A widely accepted methodology for constructing cloud-based applications is the Twelve-Factor Application. It describes a set of principles and practices that developers follow to build applications that are optimized for modern cloud environments. There is big focus on portability across environments and declarative automation.\nThese principles and practices are considered as a solid foundation for building cloud native apps. The systems that are build upon these principles can deploy and scale rapidly and add features to react quickly to market changes.\nMicroservices Cloud native systems and applications embrace microservices, which is a popular style for constructing modern applications. The microservice architectural style is an approach to developing a single application as a suite of small services, each running in their own process and communicating with lightweight mechanisms, such as REST, gRPC, HTTP(S), or WebSockets.\nMicroservices can scale independently. Instead of scaling the entire application as a single unit, you scale out only those services that require more processing power or network bandwidth. Each microservice also has an autonomous lifecycle and can evolve independently and deploy frequently. You don't have to wait for a quarterly release to deploy a new features or update, but you can update small areas of a complex application with less risk of disrupting the entire system.\nContainers Containers are a great enabler of cloud native systems and applications. Microservice containerization is also placed as the first step in the Cloud-Native Trial Map \u0026ndash; released by the Cloud Native Computing Foundation. This map offers guidance for enterprises that are beginning their cloud native journey. This technique is very straightforward: you package the code, its dependencies, and the runtime into a binary called a container image. Those images are then stored inside a container registry which acts as a repository or library for the images. Those registries can be private or public and can be stored inside your own datacenter or using public cloud services. When needed, you transform the image into a running container instance. These instances can run in the cloud or in your private data center on servers that have a container runtime engine installed.\nContainers provide portability and guarantee consistency across environments. By packaging everything into a single container image, you isolate the microservice and its dependencies from the underlying infrastructure. This also eliminates the expense of pre-configuring each environment with frameworks, software libraries, and runtime engines. And by sharing the underlying operating system and host resources, containers have a much smaller footprint than a full virtual machine. This increases the number of microservices that a given host can run at one time.\nBacking Services Cloud Native applications and services depend upon several different backing services, such as data stores, monitoring, caching, and logging services, message brokers and identity services. These backing services support the Stateless principle coming from the Twelve-Factor Application. You can consume those services from a cloud provider. You could also host your own backing services, but then you would be responsible for licensing, provisioning, and managing those resources.\nCloud Native services are typically using backing services from cloud providers. This saves time, and reduces the costs, and operational risk of hosting your own services. Backend services are threated as an attached resource and are dynamically bound to a microservice. The required information to access these services, such as URLs and credentials are then stored in an external configuration store.\nAutomation The previous pillars are specifically focusing on achieving speed and agility. But that is not the complete story. The cloud environments also need to be provisioned to being able to deploy and run cloud native applications and systems. How do you rapidly deploy your apps and features? A widely accepted practice to this is Infrastructure as Code (IaC).\nUsing IaC, you can automate platform provisioning and application deployment. DevOps teams that implement IaC can deliver stable environments rapidly and at scale. By adding testing and versioning to the DevOps practices, your infrastructure and deployments are automated, consistent, and repeatable.\nYou can use tools like Azure Resource Manager, Terraform, and Azure CLI to create scripts to deploy the cloud infrastructure. This script is versioned and checked into source control as an artifact of the whole project. The script is then automatically invoked in the Continuous Integration and Continuous Delivery (CI/CD) pipelines to provision a consistent and repeatable infrastructure across system environments, such as QA, staging, and production. A service that can handle this process from the beginning to the end is Azure Pipelines which is part of Azure DevOps.\nSummary This blog introduced the five different pillars that provide the foundation for building cloud native applications.\n","permalink":"//localhost:1313/posts/2020-09-09-thinking-cloud-native/","summary":"\u003cp\u003eThese days, applications have become very complex and users are demanding more and more of these applications. They expect innovative features, rapid responsiveness, and zero downtime. And problems that arise with building software, such as performance errors, recurring errors, and the inability to move fast are no longer acceptable by the user. If your application does not meet the user\u0026rsquo;s requirements, they simply move on to the competitor.\u003c/p\u003e\n\u003cp\u003eThis means that applications need to be able to address the need for speed and agility. And the solution to this is: A Cloud Native architecture and technologies.\u003c/p\u003e","title":"Thinking Cloud Native"},{"content":"Microsoft recently introduced the Microsoft Azure Well-Architected Framework which provides customers with a set of Azure best-practices to help them build and deliver well-architected solutions on top of the Azure platform.\nThe framework consists of five pillars of architecture excellence, that can be used as guiding to improve the quality of the workloads that run on Azure. These five pillars are: Cost Optimization, Operational Excellence, Performance Efficiency, Reliability, and Security. They will be explained in more detail in the following sections.\nCost Optimization One thing to focus on when architecting cloud solutions, is to generate incremental value early in the process. To accelerate the time to market while avoiding capital-intensive solutions, the principles of Build-Measure-Learn can be applied. This is one of the central principles of Lean Startup, which helps to create customer partnerships by building with customer empathy, measuring impact on customers, and learning with customers.\nBy using this pay-as-you-go strategy in your architecture, you will invest in scaling out after customer success, instead of delivering a large investment first version. Keep a balance in your architecture between costs for first mover advantage versus \u0026ldquo;fast follow\u0026rdquo;. For this, you can use the cost calculators to estimate the initial costs and the operational costs. Finally, establish policies, budgets, and controls that set cost limits for your solution.\nFor a detailed guidance on cost optimization, you can refer to the following articles:\nPrinciples of cost optimization\nDevelop a cost model\nSet budgets and alerts\nReview the cost optimization checklist\nOperational Excellence Operational Excellence involves the operations processes that keep applications running in production. To make deployments reliable and predictable, they should be fully automated to reduce human errors. This should be a fast and repeatable process, so they don\u0026rsquo;t slow down the release of new features or bug fixes. You should also need to be able to quickly roll back or roll forward when a release has problems or bugs.\nTo accomplish this, monitoring and diagnostics are crucial. You don\u0026rsquo;t have full control over the infrastructure and operating system when using Azure solutions. Monitoring and diagnostics will give you the insights to the systems and the solutions that run on top of it. Use a common and consistent logging schema that let\u0026rsquo;s you correlate events across different systems, Azure resources, and custom applications.\nA successful monitoring and diagnostics process has several distinct phases:\nInstrumentation: Log and generate the raw data, from all the different resources and services that you are using, such as application logs, web server logs, VM logs, diagnostics built in the Azure platform, and other sources.\nCollection and storage: Collect all the raw data and consolidate it into one place.\nAnalysis and diagnosis: Analyze the data that is collected to see the overall health of the platform, services, and your applications and to troubleshoot issues.\nVisualization and alerts: Visualize the data that is analyzed to spot trends or set up alerting to alert the operation teams.\nTo get more information and further guidance about operational excellence, you can refer to the following articles:\nDesign patterns for management and monitoring\nBest practices for monitoring cloud applications\nPerformance Efficiency With performance efficiency, you make sure that your workload can scale to meet the demands placed on it by the users in an effective manner. You can achieve by implementing PaaS offering that scale automatically or implementing scaling effectively in your solutions and applications.\nApplications can scale in two different ways: horizontally (scale out) where new instances of the resource is added, such as extra VMs or database instances. You can also scale vertically (scale up), where you increase the capacity of a single resource, for example by using a larger VM size.\nHorizontal scale needs to be architected into the system. You can scale out by placing VMs behind a load balancer. The applications that run on these VMs, also need to be able to scale. This can be accomplished by designing stateless applications or by storing state and data externally. Simply adding more instances, will not guarantee that your application will scale. Scaling can also lead to more additional measures and bottlenecks.\nTherefore, you should always conduct performance and load testing to find these potential bottlenecks. You can use the following articles for this:\nDesign patterns for scalability and performance\nBest practices: Autoscaling, Background jobs, Caching, CDN, Data partitioning\nPerformance efficiency checklist Reliability Reliable workloads are both resilient and available. Resilient applications are able to return to a fully functioning state after a failure occurs. Available applications can be accessed by the users when they need to.\nIn cloud computing a different mind set is needed then in traditional application development. Cloud applications are built as distributed systems, which means they are often more complex. The costs for cloud environments are kept low through the use of commodity hardware, so occasional hardware failures must be expected. Today, users also expect systems to be available 24/7 without ever going offline.\nThis means that cloud application must be architected differently. They need to be designed to expect occasional failures and need to be able to recover from them quickly. When designing your applications to be resilient, you first must understand availability requirements. How much downtime is acceptable for this application, how much downtime will cost your business, and how much should be invested in making the application highly available.\nIn the following articles you will get more information about how you can design and build reliable workloads and applications in Azure:\nOverview of the reliability pillar\nResiliency patterns\nBest practices: Transient fault handling, Retry guidance for specific services\nSecurity Security should be embedded throughout the entire lifecycle of an application. From the design phase all the way up to the deployments and operations phase. Protection against a variety of threats, such as DDoS attacks, is already provided by the Azure platform, but you are still responsible for building security into your application and into the DevOps processes.\nSecurity areas that need to be considered for application development are:\nIdentity Management: For authenticate and authorize users, Azure Active Directory should be considered. Azure AD is a fully managed identity and access management service, and it is integrated to Azure services, Office 365, Dynamics CRM Online, Active Directory on-premises in a hybrid deployment, and many third-party SaaS applications. For consumer-facing applications, Azure AD offers Azure AD Business to Consumer, which lets users authenticate with their existing social accounts, such as Facebook, Google, LinkedIn and more, as well as creating new accounts that is managed by Azure AD.\nApplication Security: Best practices for applications, such as SSL everywhere, protecting against CSRF and XSS attacks, preventing SQL injection attacks, and so on, still apply to the cloud. You should also store your application keys and secrets in Azure Key Vault.\nProtecting the infrastructure: Control access to all the Azure resources that you deploy. Every resource has a trust relationship with the Azure AD tenant. To grant the users in your organization the correct permissions to the Azure resources that are deployed, you can use Role Based Access Control (RBAC). These permissions can be added to different scopes, to subscriptions, resource groups, or single resources.\nData encryption: When you set up high availability in Azure, make sure that you store the data in the correct geopolitical zone. Azure geo-replicated uses the concept of paired regions, which stores the replicated data in the same geopolitical region. To store cryptographic keys and secrets, you can use Azure Key Vault. You can also use Key Vault to store that are protected by hardware security modules.\nFor more information about this, you can refer to the following articles:\nIdentity Management reference architectures.\nAzure Security Center provides integrated security monitoring and policy management across your Azure subscriptions.\nAzure Security Documentation\nMicrosoft Trust Center\nWrap up Azure Well-Architected Framework provides comprehensive architecture principles and guidelines to build cost effective, secure, reliable, and manageable solutions in Azure. If you want to get started with the Azure Well-Architected Framework:\nRead the framework content, reference material, and samples available in the Azure Architecture Center.\nTake the Azure Well-Architected Review on Microsoft Assessments.\nLearn how to Build great solutions with the Microsoft Azure Well-Architected Framework on Microsoft Learn.\n","permalink":"//localhost:1313/posts/2020-08-24-microsoft-azure-well-architected-framework-five-pillars-of-architecture-excellence-copy/","summary":"\u003cp\u003eMicrosoft recently introduced the \u003ca href=\"https://docs.microsoft.com/en-us/azure/architecture/framework/\"\u003eMicrosoft Azure Well-Architected Framework\u003c/a\u003e which provides customers with a set of Azure best-practices to help them build and deliver well-architected solutions on top of the Azure platform.\u003c/p\u003e\n\u003cp\u003eThe framework consists of five pillars of architecture excellence, that can be used as guiding to improve the quality of the workloads that run on Azure. These five pillars are: Cost Optimization, Operational Excellence, Performance Efficiency, Reliability, and Security. They will be explained in more detail in the following sections.\u003c/p\u003e","title":"Microsoft Azure Well-Architected Framework - five pillars of architecture excellence"},{"content":"Most companies typically go through a set of phases the in their Microsoft cloud journey. They start with experimenting with the cloud for rapid application development. A single subscription is manually created in the Azure portal, and a set of services is quickly deployed from the portal to serve business and the developer\u0026rsquo;s needs. It is even not uncommon in this phase, for the business or for developers to use their own credit card for creating this single subscription. The main goal in this phase is mostly serving business needs quickly, creating small proof of concepts, or avoiding the lengthy and time-consuming deployment strategies of bigger organizations.\nIn the next phase, the IT department, is starting to take the first steps into the cloud and will create additional subscriptions. Mostly targeted to the different departments in the organization. They will be introducing centralized deployments and will start thinking about security and compliancy in the cloud.\nIn the third phase, the organization is embracing the cloud on a larger scale. Senior management has decided to transform IT, shift to a cloud first approach, applications and datacenters need to be migrated, hybrid environments need to be created, and all new applications need to be cloud native. This is the moment that most organizations realize that they need a proper governance model and strategy in place.\nAs cloud environments are managed on a large scale, there is a need for a solid architecture around structuring subscriptions, networking, databases, applications, security and compliance regulations and so on. It requires ownership in the organization to successfully manage a cloud platform on a large scale. It also requires a centralized entity in the organization to maintain best-practices, onboard the cloud customers and to make sure that all services that are used, are secure and compliant by default.\nAnd by implementing these technical aspects on a large scale and embedding this into the organization, people start to realize that this also involves a significant organizational and cultural change to make it a success.\nThis is where a Cloud Center of Excellence comes in \u0026hellip;\nWhat does a Cloud Center of Excellence do? A Cloud Center of Excellence (CCoE) is used to bring a diverse and knowledgeable group of experts from across the organization together to develop cloud best practices for the rest of the organization to follow. The CCoE has a support function to increase productivity throughout the organization and at the same time maintain a consistent and secure cloud platforming. It is based on Microsoft agile practices and a delivery model that provides a programmatic approach to implement, manage and operate the Microsoft Azure platform for onboarding projects and Azure workloads effectively.\nA CCoE model, requires collaboration between each of the following:\nCloud adoption\nCloud strategy\nCloud governance\nCloud platform\nCloud automation\nWhen all these aspects are addressed, all the participants can accelerate on innovation and migration, while reducing the overall costs of change and increasing business agility. When successful implemented, a CCoE will create a significant cultural shift in IT as well. Without the CCoE model, IT tends to focus on providing control and central responsibility. A successful CCoE model provides focus on freedom and delegated responsibility. This works best in a technology strategy with a self-service model that allows business units to make their own decisions. The CCoE provides a set of guidelines and established and repeatable controls, used by the business.\nKey responsibilities of a Cloud Center of Excellence The primary goal of the CCoE team is to accelerate cloud adoption through cloud native and hybrid solutions. The CCoE has the following objectives:\nBuild the modern IT organization by capturing and implementing business requirements using agile approaches.\nBuild reusable deployment packages that fully align with the security, compliance, and service management policies.\nMaintain a functional Azure platform in alignment with operational procedures.\nReview and approve the use of cloud-native tools\nOver time, standardize and automate commonly needed platform components and solutions.\nThe Cloud Center of Excellence team The CCoE team ideally consists of 3-5 people with a variety of IT backgrounds. This will bring a broad perspective and balanced set of knowledge. It should ideally include people who already have cloud experience and with day-to-day roles, such as:\nIT / Operations / IT financial manager\nSolution / Infrastructure Architect\nApplication developer\nNetwork engineer\nDatabase administrator\nSystems administrator\nExcellent way to start your cloud journey This blog will help organizations who are going through the different phases in their cloud journey and are starting to transform the whole IT department to be ready for innovation, speed and control. The Cloud Center of Excellence is an ideal model to accelerate your cloud adoption program.\n","permalink":"//localhost:1313/posts/2020-08-20-what-a-cloud-center-of-excellence-can-do-for-your-organization/","summary":"\u003cp\u003eMost companies typically go through a set of phases the in their \u003ca href=\"https://www.capgemini.com/in-en/partner/microsoft/#applications-and-infrastructure\"\u003eMicrosoft cloud journey\u003c/a\u003e. They start with experimenting with the cloud for rapid application development. A single subscription is manually created in the Azure portal, and a set of services is quickly deployed from the portal to serve business and the developer\u0026rsquo;s needs. It is even not uncommon in this phase, for the business or for developers to use their own credit card for creating this single subscription. The main goal in this phase is mostly serving business needs quickly, creating small proof of concepts, or avoiding the lengthy and time-consuming deployment strategies of bigger organizations.\u003c/p\u003e","title":"What a Cloud Center of Excellence can do for your organization"},{"content":"Most organizations embrace a hybrid and multi cloud approach for their businesses. This will give them the full benefits of their on-premises investments, the ability to innovate using cloud technologies, and the ability to avoid vendor lock-in. For the last two years, Microsoft is investing enormously in enabling seamless hybrid capabilities. They released Azure Stack, which enables a consistent cloud model, but is deployed on-premises. They enabled security threat protection for any infrastructure, which is fully powered from the cloud, and they enabled the ability to run Microsoft Azure Cognitive Services AI models anywhere. Microsoft recently released Azure Arc, which unlocks new hybrid scenarios for organizations by bringing new Azure services and management features to any infrastructure. Azure Arc extends the Azure Resource Manager capabilities to Linux and Windows servers, and Kubernetes clusters on any infrastructure across on-premises, multi-cloud, and the edge. You can use Azure Arc to run Azure data services anywhere, which includes always up-to-date data capabilities, deployment in seconds, and dynamic scalability on any infrastructure. Azure Arc for Servers is currently in preview, and that is what we are going to cover in this post.\nAzure Arc for Servers With Azure Arc for servers, you can manage machines that are hosted outside of Azure. When these types of machines are connected to Azure using Azure Arc for servers, they become **Connected Machines,**and they will be treated as native resources in Azure. Each Connected machine will get a Resource ID during registration in Azure and it will be managed as part of a Resource group inside an Azure subscription. This will enable the ability to benefit from Azure features and capabilities, such as Azure Policies, and tagging. For each machine that you want to connect to Azure, an agent package needs to be installed. Based on how recently the agent has checked in, the machine will have a status of Connected or Disconnected. If a machine has not checked-in within the past 5 minutes, it will show as Disconnected until connectivity is restored. This check-in is called a heartbeat. The Azure Resource Manager service limits are also applicable to Azure Arc for server, which means that there is a limit of 800 servers per resource group.\nSupported Operating Systems By the time of writing this post, the public preview supports the following operating systems:\nWindows Server 2012 R2 and newer Ubuntu 16.04 and 18.04 Networking During installation and runtime, the agent requires connectivity to Azure Arc service endpoints. If outbound connectivity is blocked by the firewall, make sure that the following URLs are not blocked:\nDomain Environment Required Azure service endpoints management.azure.com Azure Resource Manager login.windows.net Azure Active Directory dc.services.visualstudio.com Application Insights agentserviceapi.azure-automation.net Guest Configuration *-agentservice-prod-1.azure-automation.net Guest Configuration *.his.hybridcompute.azure-automation.net Hybrid Identity Service In the next part of this post, we are going to connect an on-premises machine in Azure using Azure Arc. For this demonstration, I have an on-premises Hyper-V environment with one Windows Server 2016 machine.\nRegister the required Resource Providers in Azure First, we need to register the required resource providers in Azure. Therefore, take the following steps:\nOpen a browser and navigate to the Azure portal at: https://portal.azure.com/\nLogin with your administrator credentials.\nOpen Cloud Shell in the top right menu, and add the following lines of code to register the Microsoft.HybridCompute and the Microsoft.GuestConfiguration resource providers:\nRegister-AzResourceProvider -ProviderNamespace Microsoft.HybridCompute Register-AzResourceProvider -ProviderNamespace Microsoft.GuestConfiguration This will result in the following output.\nNote that the resource providers are only registered in specific locations.\nIn the next part, we are going to connect the server to Azure Arc.\nConnect the machine to Azure Arc for Servers There are two different ways to connect on-premises machines to Azure Arc. You can download a script and run it manually on the server. This is the best approach when you are adding single servers to Azure Arc. You can also follow the PowerShell Quickstart for adding multiple machines using a Service Principal. You can find the quickstart here: https://docs.microsoft.com/en-us/azure/azure-arc/servers/quickstart-onboard-powershell. We are adding one machine in this demo, so we are going to follow the Portal Quickstart. To connect the machine to Azure, we need to generate the agent install script in the Azure portal. This script is going to download the Azure Connected Machine Agent (AzCMAgent) installation package, install it on the on-premises machine and register the machine in Azure Arc.\nGenerate the agent install script using the Azure portal To generate the agent install script, take the following steps:\nNavigate to the Azure portal and type Azure Arc in the search box. Or you can launch https://aka.ms/hybridmachineportal\nClick on +Add.\nSelect Add machines using interactive script:\nIn the Basicsblade, keep the default settings and click Review + generate. If you want, you can create a new resource group for your machines that are connected to Azure Arc:\nThe last page has a script generated which you can copy (or download). This script needs to be executed on the on-premises machine:\nConnect the on-premises machine to Azure Arc To connect the on-premises machine to Azure Arc, we first need install the agent on the on-premises machine. Therefore, take the following steps:\nOpen Windows PowerShell ISE as an administrator.\nPaste the script, that is generated in the previous step in PowerShell, in the window and execute it.\nThe machine will be onboarded to Azure, which can take a few minutes to complete.\nYou will receive a registration code during script execution. Navigate to https://microsoft.com/devicelogin.\nPaste in the code from PowerShell and click Next:\nYou will receive a confirmation that the device is registered in Azure Arc.\nYou can now close the browser window.\nIf you now go back to the Azure portal and refresh the page, you will see that the server is added to Azure with the Connectedstatus.\nManaging the machine in Azure Arc To manage the machine from Azure, click on the machine in the overview blade, like in the previous image. In the overview blade, you can add tags to the machine. You can also Manage access, and apply policies to the machine from here. Summary In this post, we\u0026rsquo;ve covered how to connect an on-premises machine to Azure Arc for servers. I find it extremely easy to connect the on-premises machine to Azure. By generating the script in the Azure portal, which includes downloading and installing the agent on the on-premises machine, makes it easy to connect it to Azure. Once connected, the machine can be managed as if it is a native Azure VM.\n-Sjoukje\n","permalink":"//localhost:1313/posts/2019-12-29-azure-arc-for-servers-getting-started/","summary":"\u003cp\u003eMost organizations embrace a hybrid and multi cloud approach for their businesses. This will give them the full benefits of their on-premises investments, the ability to innovate using cloud technologies, and the ability to avoid vendor lock-in.\nFor the last two years, Microsoft is investing enormously in enabling seamless hybrid capabilities. They released Azure Stack, which enables a consistent cloud model, but is deployed on-premises. They enabled security threat protection for any infrastructure, which is fully powered from the cloud, and they enabled the ability to run Microsoft Azure Cognitive Services AI models anywhere. Microsoft recently released Azure Arc, which unlocks new hybrid scenarios for organizations by bringing new Azure services and management features to any infrastructure.\nAzure Arc extends the Azure Resource Manager capabilities to Linux and Windows servers, and Kubernetes clusters on any infrastructure across on-premises, multi-cloud, and the edge. You can use Azure Arc to run Azure data services anywhere, which includes always up-to-date data capabilities, deployment in seconds, and dynamic scalability on any infrastructure. Azure Arc for Servers is currently in preview, and that is what we are going to cover in this post.\u003c/p\u003e","title":"Azure Arc for Servers: Getting started"},{"content":"Many companies fail to understand and implement Azure governance. This can result in many unwanted results, like accidentally deleting resources in your Azure environment, non-compliance with external regulations, deploying costly resources, or deploying resources in the wrong Azure regions. The solution to this is Azure governance, the ongoing process of managing, monitoring, and auditing the use of Azure resources to meet the goals and requirements of your organization. In this session we are going to look at Azure Blueprints, Azure Policy, Management Groups and the Resource Graph to effectively manage your Azure environment. Slides: https://speakerdeck.com/sjoukjezaal/the-steps-to-effective-azure-governance\n","permalink":"//localhost:1313/posts/2019-10-17-polarconf-the-slides-of-my-session-the-steps-to-ef/","summary":"\u003cp\u003eMany companies fail to understand and implement Azure governance. This can result in many unwanted results, like accidentally deleting resources in your Azure environment, non-compliance with external regulations, deploying costly resources, or deploying resources in the wrong Azure regions. The solution to this is Azure governance, the ongoing process of managing, monitoring, and auditing the use of Azure resources to meet the goals and requirements of your organization.\nIn this session we are going to look at Azure Blueprints, Azure Policy, Management Groups and the Resource Graph to effectively manage your Azure environment.\n \n\u003cstrong\u003eSlides:\u003c/strong\u003e\n\u003ca href=\"https://speakerdeck.com/sjoukjezaal/the-steps-to-effective-azure-governance\"\u003ehttps://speakerdeck.com/sjoukjezaal/the-steps-to-effective-azure-governance\u003c/a\u003e\u003c/p\u003e","title":"PolarConf: The slides of my session The steps to effective Azure governance"},{"content":"This is the second article in a series about the different solutions that Microsoft Azure has to offer for specific industries. In my first post, I covered how these industry specific solutions help organizations to accelerate their digital transformation, and why this is so important nowadays.\nIn this second part, we will look at the manufacturing, financials services and government industries.\nAzure for manufacturing In the upcoming years, instead of selling unconnected products, manufacturers will start focusing more on selling products that come with connectivity services. They will also move from selling a discrete product to selling products as a service. To accommodate global crises, manufacturers will also seek ways to be more agile and responsive to changing market demands. Manufacturers are building digital feedback loops that help companies connect with their products and customers to continuously learn, grow, and improve existing services, as well as build new ones.\nWith Azure for manufacturing, organizations can keep pace with customer needs and market trends and drive business transformation, by modernizing to a smart factory with Azure.\nAzure for manufacturing includes the following tools and services:\nAzure AI and Machine Learning: With these tools, manufacturers can capitalize on the potential of Industry 4.0 with intelligent manufacturing operations and supply chain powered by proven and responsible AI practices.\nAzure Industrial IoT: Help securely connect new and existing industrial machinery to the cloud with open manufacturing solutions that support brownfield connectivity and enable system interoperability.\nAzure Stack: Build and deploy hybrid and edge-computing intelligent manufacturing applications and run them consistently across location boundaries.\nAzure Synapse Analytics: With Azure Synapse for Analytics, manufacturers can collect, store, process, analyze, and visualize their operational and supply chain data of any variety, volume, or velocity to pave your way to smart manufacturing.\nHigh Performance computing: Rapidly iterate on product design to reduce time to market and improve product quality with scalable and secure on-demand infrastructure.\nAzure Mixed Reality: Blend your digital world and physical world to create immersive, collaborative experiences across the manufacturing floor and beyond, by for instance, using the Microsoft HoloLens 2.\nSAP on Azure: Run your IoT manufacturing and supply chain operations with SAP workloads on Azure.\nAzure Virtual Desktop: Enable a secure remote desktop experience from virtually anywhere.\nIn the next section, we are going to look at what Microsoft Azure has to offer for financial organizations.\nAzure for financial services For financial services, technology will become a transformative tool that enables everything from contactless cards, new forms of payment to defeating financial crime. AI will become the foundation for driving intelligent banking for all financial services organizations. This will also improve the ability to deliver new customer experiences, empower employees, and drive innovation in security, compliance and regulatory environments.\nWith Azure for financial services, financial organizations can transform their company with the speed and security of the cloud. They can offer new customer experiences, enhance risk management, and fight fraud.\nAzure for financial services includes the following tools and services:\nAzure AI and Machine Learning: Create financial models and build the next generation of intelligent financial services with proven, secure, and responsible AI.\nAzure Synapse Analytics: Gather, store, process, analyze, and visualize your financial data of any variety, volume, or velocity to pave your way to intelligent banking.\nHigh Performance Computing: Execute complex calculations and large datasets in seconds with scalable and highly secure on-demand infrastructure.\nAzure Virtual Desktop: Quickly deploy virtual desktops and apps to enable secure remote work.\nAzure Stack: Extend your Azure capabilities across datacenters, edge locations, and remote offices to build and run hybrid applications with more flexibility.\nAzure Security: Gain protection with built-in cloud protection and stay ahead of risks with AI-powered monitoring tools.\nIn the next section, we are going to look at the last industry of this post, which is Azure for government.\nAzure for government The need for greater efficiencies and better communication with citizens is making digital transformation initiatives crucial across local, regional, and national governments. Especially in crisis situations. We can expect that in the near future governments are going to deepen their reliance on remote access, empowering cross-agency collaboration, and delivering trusted and secure services to engage with citizens better and more safely and improve citizen public health.\nAzure for government supports to meet the needs of citizens and helps to innovate faster. And on top of that government cloud security and compliance standards.\nAzure for government includes the following tools and services:\nAzure AI and Machine Learning: With Azure AI and Machine Learning, governments can build mission-critical solutions that can analyze images, comprehend speech, and make predictions using proven, secure, and responsible AI capabilities.\nAzure Synapse Analytics: Query data on your terms, using serverless or provisioned resources at scale.\nAzure Stack: Extend Azure services and capabilities to the environment of choice. This can be from the datacenter to edge locations and remote offices.\nHigh Performance Computing: Execute complex calculations and large datasets in seconds with scalable and highly secure on-demand infrastructure.\nAzure Virtual Desktop: Easily and securely access corporate applications, data, and resources from anywhere on any device.\nSAP on Azure: Run your mission-critical SAP workloads in the cloud.\nAzure Security: Gain protection with built-in cloud protection and stay ahead of risks with AI-powered monitoring tools.\nAzure Sentinel: Make your threat detection and response smarter and faster with AI. Eliminate security infrastructure setup and maintenance and elastically scale to meet your government cloud security needs.\nAzure IoT: Securely connect assets and equipment to the cloud with solutions that unlock real-time insights and enable system interoperability.\nWrap up In this second part of the series of blogs about Azure for Industries, we have covered manufacturing, financial services and governments. Also, for these industries, Microsoft offers a comprehensive toolset.\n","permalink":"//localhost:1313/posts/2019-08-24-adapt-and-thrive-with-microsoft-azure-for-industries-part-2/","summary":"\u003cp\u003eThis is the second article in a series about the different solutions that Microsoft Azure has to offer for specific industries. In my first post, I covered how these industry specific solutions help organizations to accelerate their digital transformation, and why this is so important nowadays.\u003c/p\u003e\n\u003cp\u003eIn this second part, we will look at the manufacturing, financials services and government industries.\u003c/p\u003e\n\u003ch2 id=\"azure-for-manufacturing\"\u003eAzure for manufacturing\u003c/h2\u003e\n\u003cp\u003eIn the upcoming years, instead of selling unconnected products, manufacturers will start focusing more on selling products that come with connectivity services. They will also move from selling a discrete product to selling products as a service. To accommodate global crises, manufacturers will also seek ways to be more agile and responsive to changing market demands. Manufacturers are building digital feedback loops that help companies connect with their products and customers to continuously learn, grow, and improve existing services, as well as build new ones.\u003c/p\u003e","title":"Adapt and thrive with Microsoft Azure for Industries -- part 2"},{"content":"Adapt and thrive with Microsoft Azure for Industries \u0026ndash; part 1 The way we work and live has changed. As a result of the recent global health crisis, many organizations are accelerating their digital transformation efforts to meet the challenges that they were exposed to. Employees around the world shifted to remote work, stores needed to shift to a buy online, pick up in-store model. In manufacturing, remote capabilities became key. Digital transformation has now become a requirement for business continuity, and digital technology has universally become key to business resilience and transformation.\nThis enormous pressure on digital transformation, since the pandemic struck, has affected all industries. A recent study from the Economist and Microsoft shows that organizations that already started their digital transformation before the pandemic, were able to adjust more quickly to meet the new customer\u0026rsquo;s needs. The digital infrastructure that these industries already put in place, allowed them to not only remain competitive in the market, but also respond to societal disruption in a nimbler way. To meet the challenges of a rapidly changing economy, across all different industries, it is no longer sufficient to just adopt technology. They need to build their own technology to compete and grow. And this makes every company a technology company.\nWe\u0026rsquo;ve seen two years\u0026rsquo; worth of digital transformation in two months. From remote teamwork and learning, to sales and customer service, to critical cloud infrastructure and security\u0026mdash;we are working alongside customers every day to help them adapt and stay open for business in a world of remote everything.\u0026quot;\nSatya Nadella,\nCEO,\nMicrosoft\nOver the past year, Microsoft has been working closely with leaders in every industry to help them navigate the crisis. They equipped them with the right technology and tools to accelerate the digital transformation. Industry specific solutions are the key to ensure business resiliency and accelerating growth.\nIn this first part of a series blogs about industry solutions build on the Microsoft Azure platform, I\u0026rsquo;m going to focus on retail and healthcare solutions. We are going to focus on some use cases and the solutions and tools that Microsoft has to offer to build the different practices.\nAzure for retail Gone are the days when retailers choose when, where, and what to sell. In this changing world, retailers are being challenged to figure out how best engage with customers within new constraints. One examples of this is adapting business processes to provide BOPIS (buy-online-and-pickup-in-store- services). Also, many retailers are driving a more sustainable model proliferation of data, are including the ability to deliver remote sales and services and are addressing the need to better equip store associates with technology.\nWith Azure for retail, organizations are capable of building personalized experiences, optimize their supply chains, and reimagine multichannel retail using Microsoft Azure. This includes predictive AI, machine learning, IoT, hybrid cloud, computer vision and analytics.\nAzure for Retail includes the following tools and services:\nAzure Synapse Analytics: With Azure Synapse for Analytics, retailers can gather, store, process, analyze, and visualize data of any volume, variety, or velocity to pave the way to intelligent retail.\nAzure AI and Machine Learning: With these tools, retailers can build intelligent, personalized customer experiences and optimized systems powered by the proven, responsible, and secure AI that Microsoft Azure has to offer.\nAzure Cognitive Services: Bring AI within the reach of every developer, without the need to require machine-learning expertise.\nAzure IoT: Enabling smart retail through Azure IoT. Retailer can securely connect their assets and equipment to the retail cloud with the different IoT solutions that Azure has to offer to unlock real time insights and enable system interoperability.\nAzure Mixed Reality: Blend the digital and the physical worlds to create immersive, and collaborative experiences.\nAzure Stack: Build and run hybrid applications across datacenters, edge locations, and the cloud.\nAzure Virtual Desktop: Deploy virtual desktops and apps to enable secure remote work.\nSAP on Azure: Run your organizations retail operations with SAP workloads on Azure to increase agility, drive strategic innovation and perform at scale.\nIn the next section, we are going to look at what Microsoft Azure has to offer for healthcare organizations.\nAzure for healthcare The healthcare industry is highly regulated. In this field, digital change is driven by the need to lower the risk of delivering patient care, while doing so at scale. The rapidly changing world is pressuring healthcare organizations to evolve how they deliver patient care as well. One example of this, is that healthcare organizations are increasingly using technology platform to shift to more telehealth services. And this example is very spot on as well in the current pandemic. By meeting patients virtually, healthcare professionals can threat (more) patients while minimizing the risk of exposure to themselves and other patients.\nWith Azure for healthcare, healthcare organizations can deliver better health insights and outcomes as they enhance patient engagement, empower health team collaboration, and improve clinical informatics and operational insights. All backed by a secure and trusted cloud. This includes hybrid cloud, mixed reality, AI, and IoT\u0026mdash;to drive better health outcomes, improve security, scale faster, and enhance data interoperability.\nAzure for Healthcare includes the following tools and services:\nAzure Healthcare APIs: With Azure healthcare specific APIs, organizations can securely manage different formats of protected health data, accelerate machine learning, and enable a secure exchange of health data within a global infrastructure.\nAzure AI and Machine Learning: Deliver better healthcare outcomes with personalized, preventative care, and intelligent systems powered by proven, secure, and responsible AI.\nAzure Synapse Analytics: With Azure Synapse for Analytics, healthcare organizations can gather, store, process, analyze, and visualize clinical data of any volume, variety, or velocity to pave the way to smart healthcare.\nAzure IoT: Deliver personalized care, empower care teams and employees, and improve operational outcomes. Securely connect health devices and equipment to the cloud with healthcare solutions to unlock real-time insights and enable system interoperability.\nHigh Performance Computing: Accelerate insights in genomics precision medicine and clinical trials with near-finite high performance bioinformatics infrastructure.\nAzure Stack: Build and run hybrid applications across datacenters, edge locations, remote clinical facilities, and the cloud.\nAzure Mixed Reality: Blend the digital and the physical worlds to create immersive, and collaborative experiences, across the operating room and other health facilities.\nAzure Security: Protection from the edge to the cloud and stay ahead of risks with intelligent monitoring tools built with powerful AI.\nWrap up Microsoft offers a comprehensive toolset for the retail and healthcare industries.\nImplementing these cloud solutions on a large scale and embedding it into your organization, involves a structured approach, a cultural shift, and a need for a solid and secure architecture. This also includes setting up a Cloud Center of Excellence, and embracing cloud native technologies and the Microsoft Well Architected Framework.\n","permalink":"//localhost:1313/posts/2019-08-21-adapt-and-thrive-with-microsoft-azure-for-industries-part-1/","summary":"\u003ch1 id=\"adapt-and-thrive-with-microsoft-azure-for-industries--part-1\"\u003eAdapt and thrive with Microsoft Azure for Industries \u0026ndash; part 1\u003c/h1\u003e\n\u003cp\u003eThe way we work and live has changed. As a result of the recent global health crisis, many organizations are accelerating their digital transformation efforts to meet the challenges that they were exposed to. Employees around the world shifted to remote work, stores needed to shift to a buy online, pick up in-store model. In manufacturing, remote capabilities became key. Digital transformation has now become a requirement for business continuity, and digital technology has universally become key to business resilience and transformation.\u003c/p\u003e","title":"Adapt and thrive with Microsoft Azure for Industries -- part 1"},{"content":"My new book, Microsoft Azure Administrator – Exam Guide AZ-103, is now published! See below, the book description:\nBook Description Microsoft Azure Administrator – Exam Guide AZ-103 will cover all the exam objectives that will help you earn Microsoft Azure Administrator certification. Whether you want to clear AZ-103 exam or want hands-on experience in administering Azure, this study guide will help you achieve your objective. It covers the latest features and capabilities around configuring, managing, and securing Azure resources. Following Microsoft\u0026rsquo;s AZ-103 exam syllabus, this guide is divided into five modules. The first module talks about how to manage Azure subscriptions and resources. You will be able to configure Azure subscription policies at Azure subscription level and learn how to use Azure policies for resource groups. Later, the book covers techniques related to implementing and managing storage in Azure. You will be able to create and configure backup policies and perform restore operations. The next module will guide you to create, configure, and deploy virtual machines for Windows and Linux. In the last two modules, you will learn about configuring and managing virtual networks and managing identities. The book concludes with effective mock tests along with answers so that you can confidently crack this exam. By the end of this book, you will acquire the skills needed to pass Exam AZ-103.\nWhat You Will Learn Configure Azure subscription policies and manage resource groups Monitor activity log by using Log Analytics Modify and deploy Azure Resource Manager (ARM) templates Protect your data with Azure Site Recovery Learn how to manage identities in Azure Monitor and troubleshoot virtual network connectivity Manage Azure Active Directory Connect, password sync, and password writeback You can buy the book on Amazon!\n","permalink":"//localhost:1313/posts/2019-06-03-my-new-book-microsoft-azure-administrator-exam-gui/","summary":"\u003cp\u003eMy new book, Microsoft Azure Administrator – Exam Guide AZ-103, is now published! See below, the book description:\u003c/p\u003e\n\u003ch2 id=\"book-description\"\u003eBook Description\u003c/h2\u003e\n\u003cp\u003eMicrosoft Azure Administrator – Exam Guide AZ-103 will cover all the exam objectives that will help you earn Microsoft Azure Administrator certification. Whether you want to clear AZ-103 exam or want hands-on experience in administering Azure, this study guide will help you achieve your objective. It covers the latest features and capabilities around configuring, managing, and securing Azure resources.\nFollowing Microsoft\u0026rsquo;s AZ-103 exam syllabus, this guide is divided into five modules. The first module talks about how to manage Azure subscriptions and resources. You will be able to configure Azure subscription policies at Azure subscription level and learn how to use Azure policies for resource groups. Later, the book covers techniques related to implementing and managing storage in Azure. You will be able to create and configure backup policies and perform restore operations. The next module will guide you to create, configure, and deploy virtual machines for Windows and Linux. In the last two modules, you will learn about configuring and managing virtual networks and managing identities. The book concludes with effective mock tests along with answers so that you can confidently crack this exam.\nBy the end of this book, you will acquire the skills needed to pass Exam AZ-103.\u003c/p\u003e","title":"My new book Microsoft Azure Administrator – Exam Guide AZ-103 is now published!"},{"content":"Thanks for attending my session at DevSum in Stockholm on May 23, 2019 on Azure AD B2C: Application Security Made Easy! Hereby my slides: [DevSum: Azure AD B2C Application security made easy](\u0026quot;//www.slideshare.net/SjoukjeZaal/devsum-azure-ad-b2c-application-security-made-easy\u0026quot; \u0026ldquo;\\\u0026ldquo;DevSum:\u0026rdquo;) from SjoukjeZaal\n","permalink":"//localhost:1313/posts/2019-05-23-devsum-the-slides-of-my-session-on-azure-ad-b2c-ap/","summary":"\u003cp\u003eThanks for attending my session at DevSum in Stockholm on May 23, 2019 on Azure AD B2C: Application Security Made Easy!\nHereby my slides:\n \u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[DevSum: Azure AD B2C Application security made easy](\u0026quot;//www.slideshare.net/SjoukjeZaal/devsum-azure-ad-b2c-application-security-made-easy\u0026quot; \u0026ldquo;\\\u0026ldquo;DevSum:\u0026rdquo;)\u003c/strong\u003e  from \u003cstrong\u003e\u003ca href=\"%22https://www.slideshare.net/SjoukjeZaal%22\"\u003eSjoukjeZaal\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e","title":"DevSum - The Slides of my session on Azure AD B2C: Application Security Made Easy"},{"content":"Thanks for attending my webinar at Dear Azure on December 09, 2018 on External Collaboration with Azure B2B! Hereby my slides:\n[Azure Saturday: External Collaboration With Azure AD B2B](\u0026quot;//www.slideshare.net/SjoukjeZaal/azure-saturday-external-collaboration-with-azure-ad-b2b\u0026quot; \u0026ldquo;\\\u0026ldquo;Azure\u0026rdquo;) from SjoukjeZaal\n","permalink":"//localhost:1313/posts/2019-05-18-azure-saturday-the-slides-of-my-session-on-externa/","summary":"\u003cp\u003eThanks for attending my webinar at Dear Azure on December 09, 2018 on External Collaboration with Azure B2B!\nHereby my slides:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[Azure Saturday: External Collaboration With Azure AD B2B](\u0026quot;//www.slideshare.net/SjoukjeZaal/azure-saturday-external-collaboration-with-azure-ad-b2b\u0026quot; \u0026ldquo;\\\u0026ldquo;Azure\u0026rdquo;)\u003c/strong\u003e  from \u003cstrong\u003e\u003ca href=\"%22https://www.slideshare.net/SjoukjeZaal%22\"\u003eSjoukjeZaal\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e","title":"Azure Saturday: The Slides of my session on External collaboration with Azure AD B2B"},{"content":"Thanks for attending my session at the Intelligent Cloud Conference in Copenhagen on April 10, 2019 on Azure AD B2C: Application Security Made Easy! Hereby my slides:\n[Intelligent Cloud Conference: Azure AD B2C Application security made easy](\u0026quot;//www.slideshare.net/SjoukjeZaal/icc-azure-b2-c-application-security-made-easy\u0026quot; \u0026ldquo;\\\u0026ldquo;Intelligent\u0026rdquo;) from SjoukjeZaal\n","permalink":"//localhost:1313/posts/2019-04-11-intelligent-cloud-conference-the-slides-of-my-sess/","summary":"\u003cp\u003eThanks for attending my session at the Intelligent Cloud Conference in Copenhagen on April 10, 2019 on Azure AD B2C: Application Security Made Easy!\nHereby my slides:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[Intelligent Cloud Conference: Azure AD B2C Application security made easy](\u0026quot;//www.slideshare.net/SjoukjeZaal/icc-azure-b2-c-application-security-made-easy\u0026quot; \u0026ldquo;\\\u0026ldquo;Intelligent\u0026rdquo;)\u003c/strong\u003e  from \u003cstrong\u003e\u003ca href=\"%22https://www.slideshare.net/SjoukjeZaal%22\"\u003eSjoukjeZaal\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e","title":"Intelligent Cloud Conference: The Slides of my session on Azure AD B2C: Application Security Made Easy"},{"content":"Deel 1:\nAzure Active Directory synchronisatie\nOm een hybride omgeving op te zetten m.b.v. SharePoint 2016 en Office 365 dienen een aantal stappen te worden doorlopen. De eerste stap is het synchroniseren van je On premise Active directory omgeving met Azure Active directory.\nOm je gebruikers accounts te kunnen synchroniseren met de cloud, wordt de tool Azure AD Connect gebruikt die je hier kan dowloaden: https://www.microsoft.com/en-us/download/details.aspx?id=47594 .\nOm Azure AD Connect te kunnen gebruiken dient je server te voldoen aan een aantal eisen. Deze eisen zijn na te lezen op de volgende website: https://azure.microsoft.com/en-us/documentation/articles/active-directory-aadconnect-prerequisites/.\nAls je omgeving voldoet aan de gestelde eisen kan je Azure AD Connect installeren op de server waar Active Directory Services geinstalleerd is:\nDubbelklik op het .msi bestand. Dit opent het installatie programma;\nDe welkomst pagina wordt geopend. Vink de checkbox aan en klik op \u0026ldquo;Continue\u0026rdquo;.\nhttp://www.avepoint.com/community/avepoint-blog/5-steps-configure-hybrid-features-sharepoint-2016-beta-2/\n","permalink":"//localhost:1313/posts/2019-02-26-sharepoint-2016---office-365-hybride-deel-1/","summary":"\u003cp\u003eDeel 1:\u003c/p\u003e\n\u003cp\u003eAzure Active Directory synchronisatie\u003c/p\u003e\n\u003cp\u003eOm een hybride omgeving op te zetten m.b.v. SharePoint 2016 en Office\n365 dienen een aantal stappen te worden doorlopen. De eerste stap is het\nsynchroniseren van je On premise Active directory omgeving met Azure\nActive directory.\u003c/p\u003e\n\u003cp\u003eOm je gebruikers accounts te kunnen synchroniseren met de cloud, wordt\nde tool Azure AD Connect gebruikt die je hier kan dowloaden:\n\u003ca href=\"https://www.microsoft.com/en-us/download/details.aspx?id=47594\"\u003ehttps://www.microsoft.com/en-us/download/details.aspx?id=47594\u003c/a\u003e .\u003cbr\u003e\nOm Azure AD Connect te kunnen gebruiken dient je server te voldoen aan\neen aantal eisen. Deze eisen zijn na te lezen op de volgende website:\n\u003ca href=\"https://azure.microsoft.com/en-us/documentation/articles/active-directory-aadconnect-prerequisites/\"\u003ehttps://azure.microsoft.com/en-us/documentation/articles/active-directory-aadconnect-prerequisites/\u003c/a\u003e.\u003c/p\u003e","title":"Azure B2B SharePoint Online Solution"},{"content":"Welcome to another Azure Content Spotlight! These articles are used to highlight items in Azure that could be more visible to the Azure community. This week\u0026rsquo;s content spotlight is about the Azure Data Architecture Guide, which is published by the Azure CAT team. This guide can be used by data professionals, architects and developers, who are interested in selecting the right data services and technologies on Azure. It will give you a lot of interesting information, and guidance on how to design effective data architectures and solutions on the Azure platform. The Azure CAT team provided a series of blog posts:\nAzure Data Architecture Guide – Blog #1: Introduction Azure Data Architecture Guide – Blog #2: On-demand big data analytics Azure Data Architecture Guide – Blog #3: Advanced analytics and deep learning Azure Data Architecture Guide – Blog #4: Hybrid data architecture Azure Data Architecture Guide – Blog #5: Clickstream analysis Azure Data Architecture Guide – Blog #6: Business intelligence Azure Data Architecture Guide – Blog #7: Intelligent applications Azure Data Architecture Guide – Blog #8: Data warehousing You can also refer to the Azure Data Architectureguide.\n-Sjoukje\n","permalink":"//localhost:1313/posts/2019-01-17-azure-content-spotlight-azure-data-architecture-gu/","summary":"\u003cp\u003eWelcome to another Azure Content Spotlight! These articles are used to highlight items in Azure that could be more visible to the Azure community.\nThis week\u0026rsquo;s content spotlight is about the Azure Data Architecture Guide, which is published by the Azure CAT team. This guide can be used by data professionals, architects and developers, who are interested in selecting the right data services and technologies on Azure. It will give you a lot of interesting information, and guidance on how to design effective data architectures and solutions on the Azure platform.\nThe Azure CAT team provided a series of blog posts:\u003c/p\u003e","title":"Azure Content Spotlight – Azure Data Architecture Guide"},{"content":"Thanks for attending my webinar at Dear Azure on December 09, 2018 on External Collaboration with Azure B2B! Hereby my slides: https://www.slideshare.net/SjoukjeZaal/dear-azure-external-collaboration-with-azure-ad-b2b The code has been shared on GitHub: https://github.com/SjoukjeZaal/Events/tree/master/2018-12-09%20Dear%20Azure -Sjoukje\n","permalink":"//localhost:1313/posts/2018-12-10-dear-azure-webinar-the-slides-of-my-session-on-ext/","summary":"\u003cp\u003eThanks for attending my webinar at Dear Azure on December 09, 2018 on External Collaboration with Azure B2B!\nHereby my slides:\n\u003ca href=\"https://www.slideshare.net/SjoukjeZaal/dear-azure-external-collaboration-with-azure-ad-b2b\"\u003ehttps://www.slideshare.net/SjoukjeZaal/dear-azure-external-collaboration-with-azure-ad-b2b\u003c/a\u003e\n \nThe code has been shared on GitHub: \u003ca href=\"%22https://github.com/SjoukjeZaal/Events/tree/master/2018-12-09%20Dear%20Azure%22\"\u003ehttps://github.com/SjoukjeZaal/Events/tree/master/2018-12-09%20Dear%20Azure\u003c/a\u003e\n \n-Sjoukje\u003c/p\u003e","title":"Dear Azure Webinar: The Slides of my session on External collaboration with Azure AD B2B"},{"content":"Welcome to another Azure Content Spotlight! These articles are used to highlight items in Azure that could be more visible to the Azure community. This weeks content spotlight is about Cognitive Service Containers, which is released in preview last week. There was a high demand for the flexibility to deploy AI applications in a variety of environments. By deploying Cognitive Services in containers, data can be analyzed close to where the actual data resides. And these containers can be deployed on on-premises environments as well, so customer data (e.g., the image or text that is being analyzed) is not sent to Microsoft. For information and how to get started with Cognitive Service Containers, you can refer to the below links:\nRunning Cognitive Service containers Bringing AI to the edge Getting started with Azure Cognitive Services in containers -Sjoukje\n","permalink":"//localhost:1313/posts/2018-11-29-azure-content-spotlight-cognitive-service-containe/","summary":"\u003cp\u003eWelcome to another Azure Content Spotlight! These articles are used to highlight items in Azure that could be more visible to the Azure community.\nThis weeks content spotlight is about Cognitive Service Containers, which is released in preview last week. There was a high demand for the flexibility to deploy AI applications in a variety of environments. By deploying Cognitive Services in containers, data can be analyzed close to where the actual data resides. And these containers can be deployed on on-premises environments as well, so customer data (e.g., the image or text that is being analyzed) is not sent to Microsoft.\nFor information and how to get started with Cognitive Service Containers, you can refer to the below links:\u003c/p\u003e","title":"Azure Content Spotlight – Cognitive Service Containers"},{"content":"Thanks for attending out session at The Office 365 \u0026amp; SharePoint Connect on November 15, 2018: Innovate - Connecting bleeding edge technologies! Hereby my slides: https://www.slideshare.net/SjoukjeZaal/o365-sp-connect-2018-innovate-connecting-bleeding-edge-technologies -Sjoukje\n","permalink":"//localhost:1313/posts/2018-11-15-o365-sp-connect-2018-the-slide-of-my-session-innov/","summary":"\u003cp\u003eThanks for attending out session at The Office 365 \u0026amp; SharePoint Connect on November 15, 2018: Innovate - Connecting bleeding edge technologies!\nHereby my slides:\n \n\u003ca href=\"https://www.slideshare.net/SjoukjeZaal/o365-sp-connect-2018-innovate-connecting-bleeding-edge-technologies\"\u003ehttps://www.slideshare.net/SjoukjeZaal/o365-sp-connect-2018-innovate-connecting-bleeding-edge-technologies\u003c/a\u003e\n \n-Sjoukje\u003c/p\u003e","title":"O365 \u0026 SP Connect 2018: The slide of my session Innovate - Connecting bleeding edge technologies"},{"content":"Welcome to another Azure Content Spotlight! These articles are used to highlight items in Azure that could be more visible to the Azure community. This weeks content spotlight is all for developers that want to get started with developing AI applications or more experienced AI developers that want expand their knowledge. AI is a set of technologies that enable computers to assist and solve problems in a way that are similar to humans by perceiving, learning, and reasoning. Using the cloud and the Microsoft AI platform you can now infuse your applications with these intelligent capabilities. Microsoft offers an extensive set of services to build intelligent applications, like Azure Cognitive Services, Bot Framework and Azure Machine Learning. You can use different tooling, even open source technologies such as TensorFlow, PyTorch, or Jupyter and leverage the infrastructure capabilities that are part of Microsoft AI platform as well. To get started or expand your knowledge, you can refer to the following resources:\nDownload the free ebook: A Developer’s Guide to Building AI Applications Watch all the Ignite 2018 AI sessions Learn more about Azure AI Get started with Azure Cognitive Services -Sjoukje\n","permalink":"//localhost:1313/posts/2018-10-25-azure-content-spotlight-get-started-with-developin/","summary":"\u003cp\u003eWelcome to another Azure Content Spotlight! These articles are used to highlight items in Azure that could be more visible to the Azure community.\nThis weeks content spotlight is all for developers that want to get started with developing AI applications or more experienced AI developers that want expand their knowledge.\nAI is a set of technologies that enable computers to assist and solve problems in a way that are similar to humans by perceiving, learning, and reasoning. Using the cloud and the Microsoft AI platform you can now infuse your applications with these intelligent capabilities. Microsoft offers an extensive set of services to build intelligent applications, like Azure Cognitive Services, Bot Framework and Azure Machine Learning. You can use different tooling, even open source technologies such as TensorFlow, PyTorch, or Jupyter and leverage the infrastructure capabilities that are part of Microsoft AI platform as well.\nTo get started or expand your knowledge, you can refer to the following resources:\u003c/p\u003e","title":"Azure Content Spotlight - Get started with developing AI applications"},{"content":"Thanks for attending my session at the PolarConf on Oktober 10, 2018 on External Collaboration with Azure B2B! Hereby my slides: https://www.slideshare.net/SjoukjeZaal/external-collaboration-with-azure-b2b-118983713?qid=16289d77-0f53-47c2-9642-4571e4f3d030\u0026amp;v=\u0026amp;b=\u0026amp;from_search=8 The code has been shared on GitHub: https://github.com/SjoukjeZaal/Events/tree/master/2018-10-10%20PolarConf -Sjoukje\n","permalink":"//localhost:1313/posts/2018-10-10-polarconf-the-slides-of-my-session-on-external-col/","summary":"\u003cp\u003eThanks for attending my session at the PolarConf on Oktober 10, 2018 on External Collaboration with Azure B2B!\nHereby my slides:\n\u003ca href=\"https://www.slideshare.net/SjoukjeZaal/external-collaboration-with-azure-b2b-118983713?qid=16289d77-0f53-47c2-9642-4571e4f3d030\u0026amp;v=\u0026amp;b=\u0026amp;from\"\u003ehttps://www.slideshare.net/SjoukjeZaal/external-collaboration-with-azure-b2b-118983713?qid=16289d77-0f53-47c2-9642-4571e4f3d030\u0026amp;v=\u0026amp;b=\u0026amp;from\u003c/a\u003e_search=8\n \nThe code has been shared on GitHub: \u003ca href=\"%22https://github.com/SjoukjeZaal/Events/tree/master/2018-10-10%20PolarConf%22\"\u003ehttps://github.com/SjoukjeZaal/Events/tree/master/2018-10-10%20PolarConf\u003c/a\u003e\n \n-Sjoukje\u003c/p\u003e","title":"PolarConf – The Slides of my session on External collaboration with Azure AD B2B"},{"content":"Thanks for attending my session at the AZUG on september 20, 2018 on External Collaboration with Azure B2B! Hereby my slides: https://www.slideshare.net/SjoukjeZaal/external-collaboration-with-azure-b2b?qid=9102ff87-4d7d-4c0f-8bb2-38cc1a1d641c\u0026amp;v=\u0026amp;b=\u0026amp;from_search=9 The code has been shared on GitHub: https://github.com/SjoukjeZaal/Events/tree/master/2018-09-20%20AZUG%20BE -Sjoukje\n","permalink":"//localhost:1313/posts/2018-09-20-azug-be-the-slides-of-my-session-on-external-colla/","summary":"\u003cp\u003eThanks for attending my session at the AZUG on september 20, 2018 on External Collaboration with Azure B2B!\nHereby my slides:\n\u003ca href=\"https://www.slideshare.net/SjoukjeZaal/external-collaboration-with-azure-b2b?qid=9102ff87-4d7d-4c0f-8bb2-38cc1a1d641c\u0026amp;v=\u0026amp;b=\u0026amp;from\"\u003ehttps://www.slideshare.net/SjoukjeZaal/external-collaboration-with-azure-b2b?qid=9102ff87-4d7d-4c0f-8bb2-38cc1a1d641c\u0026amp;v=\u0026amp;b=\u0026amp;from\u003c/a\u003e_search=9\n \nThe code has been shared on GitHub: \u003ca href=\"%22https://github.com/SjoukjeZaal/Events/tree/master/2018-09-20%20AZUG%20BE%22\"\u003ehttps://github.com/SjoukjeZaal/Events/tree/master/2018-09-20%20AZUG%20BE\u003c/a\u003e\n \n-Sjoukje\u003c/p\u003e","title":"AZUG BE – The Slides of my session on External collaboration with Azure AD B2B"},{"content":"Welcome to another Azure Content Spotlight! These articles are used to highlight items in Azure that could be more visible to the Azure community. This weeks content spotlight is about App migration to Azure. The Cloud is changing the way applications are architected. Instead of monoliths, applications are decomposed into smaller services. These services communicate with each other through APIs or using asynchronous messaging. They can scale horizontally, so new instances can be added and removed easily. Deployments must be automated and monitoring is critical for gaining insights into the applications. And this all can be realized without the need to bother about the underlying infrastructure. This changing way of designing and architecting applications, introduces new architectural patterns and best practices for building apps on the Azure cloud platform. Below is a set of resources which can help software architects and developers gaining insights about the different services and resources Azure has to offer to compose your applications. It also provides different patterns and best practices on how to design your applications. A couple of Build 2018 videos:\nApp Modernization with Microsoft Azure : Build 2018 Modernizing existing .NET applications with Windows Containers and Azure cloud : Build 2018 \u0026hellip; Some online resources:\nAzure Architecture Center Migrate to the Cloud Migration checklist when moving to Azure App Service Cloud Design Patterns Azure Reference Architectures A free eBook:\nCloud Application Architecture Guide -Sjoukje\n","permalink":"//localhost:1313/posts/2018-08-30-azure-content-spotlight-migrate-apps-to-azure/","summary":"\u003cp\u003eWelcome to another Azure Content Spotlight! These articles are used to highlight items in Azure that could be more visible to the Azure community.\nThis weeks content spotlight is about App migration to Azure. The Cloud is changing the way applications are architected. Instead of monoliths, applications are decomposed into smaller services. These services communicate with each other through APIs or using asynchronous messaging. They can scale horizontally, so new instances can be added and removed easily. Deployments must be automated and monitoring is critical for gaining insights into the applications. And this all can be realized without the need to bother about the underlying infrastructure.\nThis changing way of designing and architecting applications, introduces new architectural patterns and best practices for building apps on the Azure cloud platform. Below is a set of resources which can help software architects and developers gaining insights about the different services and resources Azure has to offer to compose your applications. It also provides different patterns and best practices on how to design your applications.\nA couple of Build 2018 videos:\u003c/p\u003e","title":"Azure Content Spotlight – Migrate Apps to Azure"},{"content":"Welcome to another Azure Content Spotlight! These articles are used to highlight items in Azure that could be more visible to the Azure community The Secure DevOps Kit for Azure (AzSK) offers a set of scripts, tools, extensions, automations, etc. for dev ops teams which are using automation and are integrating security into native dev ops workflows. The following focus areas are covered:\nSecure the subscription: A secure cloud subscription provides a core foundation for development and deployment activities. Enable secure development: Developers should have the ability to write secure code and to test the secure configuration of their cloud applications. Integrate security into CICD: Ability to run Security Verification Tests (SVTs) as part of the VSTS CICD pipeline. Continuous Assurance: Treat security as a continuously varying state of a system with continious assurance. Alerting \u0026amp; Monitoring: provides solutions for individual application teams and also for central enterprise teams. Cloud Risk Governance: A telemetry framework that generates events capturing usage, adoption, evaluation results, etc. For more information, you can refer to the following sites:\nSecure DevOps Kit for Azure (AzSK) DevOpsKit-docsGetting started with the Secure DevOps Kit for Azure! Azure Friday: Getting started with the Secure DevOps Kit for Azure (AzSK) ","permalink":"//localhost:1313/posts/2018-08-16-azure-content-spotlight-secure-devops-kit-for-azur/","summary":"\u003cp\u003eWelcome to another Azure Content Spotlight! These articles are used to highlight items in Azure that could be more visible to the Azure community\nThe Secure DevOps Kit for Azure (AzSK) offers a set of scripts, tools, extensions, automations, etc. for dev ops teams which are using automation and are integrating security into native dev ops workflows. The following focus areas are covered:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSecure the subscription:  A secure cloud subscription provides a core foundation for development and deployment activities.\u003c/li\u003e\n\u003cli\u003eEnable secure development: Developers should have the ability to write secure code and to test the secure configuration of their cloud applications.\u003c/li\u003e\n\u003cli\u003eIntegrate security into CICD:  Ability to run Security Verification Tests (SVTs) as part of the VSTS CICD pipeline.\u003c/li\u003e\n\u003cli\u003eContinuous Assurance: Treat security as a continuously varying state of a system with continious assurance.\u003c/li\u003e\n\u003cli\u003eAlerting \u0026amp; Monitoring: provides solutions for individual application teams and also for central enterprise teams.\u003c/li\u003e\n\u003cli\u003eCloud Risk Governance: A telemetry framework that generates events capturing usage, adoption, evaluation results, etc.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor more information, you can refer to the following sites:\u003c/p\u003e","title":"Azure Content Spotlight – Secure DevOps Kit for Azure (AzSK)"},{"content":"Welcome to another Azure Content Spotlight! These articles are used to highlight items in Azure that could be more visible to the Azure community. This week\u0026rsquo;s content spotlight is about Azure Service Fabric Mesh, which is now in preview. Azure Service Fabric Mesh offers a fully managed service for developers to deploy microservices applications without managing the infrastructure, like storage, networking or virtual machines. It consists of clusters of thousands of machines and all of the cluster operations are hidden from the developer. You can watch the Azure Friday video about Service Fabric Mesh below: for more information, you can refer to the following websites:\nAzure Service Fabric Mesh documentation Azure Service Fabric Mesh samples -Sjoukje\n","permalink":"//localhost:1313/posts/2018-07-26-azure-content-spotlight-azure-service-fabric-mesh-/","summary":"\u003cp\u003eWelcome to another Azure Content Spotlight! These articles are used to highlight items in Azure that could be more visible to the Azure community.\nThis week\u0026rsquo;s content spotlight is about Azure Service Fabric Mesh, which is now in preview. Azure Service Fabric Mesh offers a fully managed service for developers to deploy microservices applications without managing the infrastructure, like storage, networking or virtual machines. It consists of clusters of thousands of machines and all of the cluster operations are hidden from the developer.\nYou can watch the Azure Friday video about Service Fabric Mesh below:\nfor more information, you can refer to the following websites:\u003c/p\u003e","title":"Azure Content Spotlight – Azure Service Fabric Mesh (Preview)"},{"content":"Welcome to another Azure Content Spotlight! These articles are used to highlight items in Azure that could be more visible to the Azure community.\nThis week\u0026rsquo;s content spotlight is about the Video Indexer. The Video Indexer is part of the AI offering of Microsoft and offers an application and platform built upon media AI technologies. It offers features to extract insights from video and audio files, which uses the Speech to Text APIof Cognitive Services. It creates a transcript based on the speech appearing in the video or audio file, which can be translated into an impressive amount of languages.\nWant to learn more about all the capabilities and features that the Video Indexer has to offer? You can refer to the following article on the Azure Blog:https://azure.microsoft.com/en-us/blog/get-video-insights-in-even-more-languages/, or navigate to the Video Indexer site directly:https://vi.microsoft.com/en-us/\n","permalink":"//localhost:1313/posts/2018-07-05-azure-content-spotlight-video-indexer/","summary":"\u003cp\u003eWelcome to another Azure Content Spotlight! These articles are used to highlight items in Azure that could be more visible to the Azure community.\u003c/p\u003e\n\u003cp\u003eThis week\u0026rsquo;s content spotlight is about the \u003ca href=\"%22https://vi.microsoft.com/en-us/%22\"\u003eVideo Indexer\u003c/a\u003e. The Video Indexer is part of the AI offering of Microsoft and offers an application and platform built upon media AI technologies. It offers features to extract insights from video and audio files, which uses the \u003ca href=\"%22https://azure.microsoft.com/en-us/services/cognitive-services/speech-to-text/%22\"\u003eSpeech to Text API\u003c/a\u003eof Cognitive Services. It creates a transcript based on the speech appearing in the video or audio file, which can be translated into an impressive amount of languages.\u003c/p\u003e","title":"Azure Content Spotlight - Video Indexer"},{"content":"Welcome to another Azure Content Spotlight! These articles are used to highlight items in Azure that could be more visible to the Azure community.\nThis week\u0026rsquo;s content spotlight is about Microsoft Research Open Data, a new data repository in the cloud dedicated to facilitating collaboration across the global research community. It offers datasets representing many years of data research which were used in published research studies. Microsoft Research Open Data is an outcome of the Microsoft Research Outreach Data science program and was made possible by a collaboration between many teams at Microsoft, Microsoft researchers, academic advisors, and various industry partners. The goal of this platform is to share datasets and other research related tools with Microsoft researchers and collaborators. Datasets can easily be accessed and the platform provides datasets of state-of-the-art research in areas such as natural language processing, computer vision, and domain specific sciences. This datasets can be downloaded or copied directly to a cloud-based Data Science Virtual Machine from the portal.\n","permalink":"//localhost:1313/posts/2018-06-28-azure-content-spotlight-microsoft-research-open-da/","summary":"\u003cp\u003eWelcome to another Azure Content Spotlight! These articles are used to highlight items in Azure that could be more visible to the Azure community.\u003c/p\u003e\n\u003cp\u003eThis week\u0026rsquo;s content spotlight is about \u003ca href=\"%22https://msropendata.com/%22\"\u003eMicrosoft Research Open Data\u003c/a\u003e, a new data repository in the cloud dedicated to facilitating collaboration across the global research community. It offers datasets representing many years of data research which were used in published research studies. Microsoft Research Open Data is an outcome of the Microsoft Research Outreach Data science program and was made possible by a collaboration between many teams at Microsoft, Microsoft researchers, academic advisors, and various industry partners.\nThe goal of this platform is to share datasets and other research related tools with Microsoft researchers and collaborators. Datasets can easily be accessed and the platform provides datasets of state-of-the-art research in areas such as natural language processing, computer vision, and domain specific sciences. This datasets can be downloaded or copied directly to a cloud-based Data Science Virtual Machine from the portal.\u003c/p\u003e","title":"Azure Content Spotlight - Microsoft Research Open Data"},{"content":"Welcome to another Azure Content Spotlight! These articles are used to highlight items in Azure that could be more visible to the Azure community. This week\u0026rsquo;s content spotlight is about some tips \u0026amp; tricks for Azure Cosmos DB. The first one is about Horizontal Partitioning in Azure Cosmos DB, an Azure Friday video from Channel 9. This video will cover how to partition your data in Azure Cosmos DB, will give you best practices for choosing a partition key, and how to troubleshoot bad partition key choices. For more information about Partition and scale in Azure Cosmos DB, you can refer to the following site: https://docs.microsoft.com/en-ca/azure/cosmos-db/partition-data. The second one is about Tips for using the Gremlin API with Azure Cosmos DB. Gremlin is the traversal query language for Cosmos DB graph and for writing efficient Gremlin queries, knowledge of the graph structure and the query execution plan is required. In this video you will learn how to structure a Gremlin query, and what the best practices are, and you will get tips \u0026amp; tricks to get the best performance. For more information about the Graph API for Cosmos DB, you can refer to the following website: https://docs.microsoft.com/en-ca/azure/cosmos-db/graph-introduction. You can watch the videos over here:\nhttps://channel9.msdn.com/Shows/Azure-Friday/Horizontal-Partitioning-in-Azure-Cosmos-DB https://channel9.msdn.com/Shows/Azure-Friday/Tips-for-using-the-Gremlin-API-with-Azure-Cosmos-\u0026hellip; -Sjoukje\n","permalink":"//localhost:1313/posts/2018-06-14-azure-content-spotlight-azure-cosmos-db-tips-trick/","summary":"\u003cp\u003eWelcome to another Azure Content Spotlight! These articles are used to highlight items in Azure that could be more visible to the Azure community.\nThis week\u0026rsquo;s content spotlight is about some tips \u0026amp; tricks for Azure Cosmos DB.\nThe first one is about \u003cem\u003eHorizontal Partitioning in Azure Cosmos DB\u003c/em\u003e, an Azure Friday video from Channel 9. This video will cover how to partition your data in Azure Cosmos DB, will give you best practices for choosing a partition key, and how to troubleshoot bad partition key choices. For more information about Partition and scale in Azure Cosmos DB, you can refer to the following site: \u003ca href=\"%22https://docs.microsoft.com/en-ca/azure/cosmos-db/partition-data%22\"\u003ehttps://docs.microsoft.com/en-ca/azure/cosmos-db/partition-data.\u003c/a\u003e\nThe second one is about \u003cem\u003eTips for using the Gremlin API with Azure Cosmos DB\u003c/em\u003e. Gremlin is the traversal query language for Cosmos DB graph and for writing efficient Gremlin queries, knowledge of the graph structure and the query execution plan is required. In this video you will learn how to structure a Gremlin query, and what the best practices are, and you will get tips \u0026amp; tricks to get the best performance. For more information about the Graph API for Cosmos DB, you can refer to the following website: \u003ca href=\"%22https://docs.microsoft.com/en-ca/azure/cosmos-db/graph-introduction%22\"\u003ehttps://docs.microsoft.com/en-ca/azure/cosmos-db/graph-introduction.\u003c/a\u003e\nYou can watch the videos over here:\u003c/p\u003e","title":"Azure Content Spotlight – Azure Cosmos DB Tips \u0026 Tricks"},{"content":"Welcome to another Azure Content Spotlight! These articles are used to highlight items in Azure that could be more visible to the Azure community. IoT is hot! Microsoft recently announced that they are going to invest $5 billion in the Internet of Things over the next four years. Why? Because their goal is to give every customer the ability to transform their businesses, and the world at large, with connected solutions. And why now? Because they now see the kind of increased adoption and exponential growth that analysts have been forecasting for the past years. You can refer to the full article here:https://blogs.microsoft.com/iot/2018/04/04/microsoft-will-invest-5-billion-in-iot-heres-why/ But now back to the development part of it. IoT offers tremendous opportunities for developers and the recently published IoT Interactive Development Guideoffers common application patterns and tutorials that will get you up and running fast. If you are planning on doing some IoT development or if you are just curious, you should really take a look! It offers a lot of valuable information for beginning and more experienced IoT developers. -Sjoukje\n","permalink":"//localhost:1313/posts/2018-05-31-azure-content-spotlight-azure-iot-interactive-deve/","summary":"\u003cp\u003eWelcome to another Azure Content Spotlight! These articles are used to highlight items in Azure that could be more visible to the Azure community.\nIoT is hot! Microsoft recently announced that they are going to invest $5 billion in the Internet of Things over the next four years. Why? Because their goal is to give every customer the ability to transform their businesses, and the world at large, with connected solutions. And why now? Because they now see the kind of increased adoption and exponential growth that analysts have been forecasting for the past years. You can refer to the full article here:\u003ca href=\"%22https://blogs.microsoft.com/iot/2018/04/04/microsoft-will-invest-5-billion-in-iot-heres-why/%22\"\u003ehttps://blogs.microsoft.com/iot/2018/04/04/microsoft-will-invest-5-billion-in-iot-heres-why/\u003c/a\u003e\nBut now back to the development part of it. IoT offers tremendous opportunities for developers and the recently published \u003ca href=\"%22https://content.microsoft.com/iot/developerguide%22\"\u003eIoT Interactive Development Guide\u003c/a\u003eoffers common application patterns and tutorials that will get you up and running fast. If you are planning on doing some IoT development or if you are just curious, you should really take a look! It offers a lot of valuable information for beginning and more experienced IoT developers.\n-Sjoukje\u003c/p\u003e","title":"Azure Content Spotlight – Azure IoT Interactive Development Guide"},{"content":"Modern computing today is all distributed and Azure is designed from the ground up using distributed services and resources. Examples of those services and resources are, Azure Functions, Logic Apps, Azure Services Bus, IoT Hub, and more. But, all those services and resources are operating on their own island and to let those different resources, communicate with each other, Azure Event Grid comes into place.\nAzure Event Grid offers a fully-managed intelligent event routing service, which can be used to glue all these different distributed services and resources together. It is part of the messaging landscape of Azure and it offers a cross cloud eventing platform from the Microsoft Cloud.\nAzure Event Grid simplifies the creation of event-based applications by managing all routing of events from any source, to any destination, for any application and Azure resource. Using Event Grid, you don\u0026rsquo;t have to create all these connections and integrations manually. You can subscribe to any event that is happening across the different Azure resources in your subscription, and act on that event using Azure Functions, Logic Apps, different apps and services included in Office 365, custom applications and more.\nIn Azure Event Grid, you can easily select the resource you would like to subscribe to and specify the event handler or WebHook to publish the event to. You can use filters to select specific events or use filters to route specific events to specific endpoints as well.\nAzure Event Grid Concepts Azure Event Grid uses the below concepts:\nEvents An event is a piece of information that describes something that happened inside a system or application. Each event has common information like a unique identifier, the source of the event and the time the event took place. It has relevant information about the type of the event as well. For example, an event that is created after a new file or image is being uploaded to Azure Blob Storage, has specific details about that file as well, like BlobType, which described the type of Blob. This can be a Block Blob or a Page Blob, for instance.\nEvent Sources/ Publishers The Event Source is the initiator or creator of the event. For example, Azure Blob Storage is the event source for Blob created events. The Event sources are responsible for publishing the events to Event Grid.\nBy the time of writing this post, the following Azure Services support publishing events to Event Grid:\nAzure Subscriptions and Resource Groups (management operations)\nEvent Hubs\nMedia Services\nBlob Storage\nService Bus\nStorage General-purpose v2 (GPv2)\nIoT Hub\nCustom Topics\nTopics Events are categorized into topics by the publisher. Azure Event Grid offers an endpoint where the publisher can send events to. When the event is published to Event Grid, the subscriber can decide to which topic it described to. Topics also provide a schema, so the subscribers can know how to subscribe to the event accordingly.\nEvent Subscriptions The subscription tells Event Grid where the events on a topic are sent to. It also gives information about which specific events the subscriber is interested in. It provides information about how the event needs to be delivered to the subscriber as well.\nEvent Handlers The Event Handler is the receiver of the event and it is responsible of further processing the event and take further action on the event. Azure Event Grid offers different mechanisms for delivering the event to the Event Handler. It offers a retrying mechanism for HTTP and WebHooks.\nBy the time of writing this post, the following Event handlers are available:\nEvent Hubs\nHybrid Connections\nAzure Automation\nAzure Functions\nLogic Apps\nMicrosoft Flow\nQueue Storage\nWebHooks\nSecurity Event Grid offers security for subscribing for, and publishing topics. Subscribers need to have sufficient permissions (Write permissions) on the resource or topics. For publishing events, a SAS token or key authentication is needed for the topic.\nSee below the overview image of the different Topics and Subscribers. This image is taken from the following Azure Event Grid documentation website: https://docs.microsoft.com/en-us/azure/event-grid/overview\nWrap-up In this post, I\u0026rsquo;ve described Azure Event Grid from a high level, briefly talked about how Azure is designed from the ground up using distributed components and what Azure Event Grid has to offer to let all those components communicate with each other from a single service. In the upcoming posts, we will work out some scenarios where Event Grid is used to glue the different services and resources together.\n-Sjoukje\n","permalink":"//localhost:1313/posts/2018-05-28-introduction-to-azure-event-grid/","summary":"\u003cp\u003eModern computing today is all distributed and Azure is designed from the\nground up using distributed services and resources. Examples of those\nservices and resources are, Azure Functions, Logic Apps, Azure Services\nBus, IoT Hub, and more. But, all those services and resources are\noperating on their own island and to let those different resources,\ncommunicate with each other, Azure Event Grid comes into place.\u003c/p\u003e\n\u003cp\u003eAzure Event Grid offers a fully-managed intelligent event routing\nservice, which can be used to glue all these different distributed\nservices and resources together. It is part of the messaging landscape\nof Azure and it offers a cross cloud eventing platform from the\nMicrosoft Cloud.\u003c/p\u003e","title":"Introduction to Azure Event Grid"},{"content":"This is the fifth part in the series about securing your Logic Apps. In the previous post, we\u0026rsquo;ve covered how to use the Validate JWT access Restriction Policy in API Management for securing your Logic Apps. In this post, I\u0026rsquo;m going to show you how you can add your Logic App to an App Service Environment to create an isolated and secure environment for your app to run in.\nIn this series:\nSecure access to the trigger.\nSecure your Logic App using API Management \u0026ndash; Access Restriction Policies.\nSecure your Logic App using Azure Active Directory\nSecure your Logic App using API Management \u0026ndash; Validate JWT Access Restriction Policy\nSecure your Logic App using an App Service Environment\nFor this article, I\u0026rsquo;ve used the Logic App which is created in the first post of this series.\nhttps://azure.microsoft.com/en-us/blog/introducing-app-service-environment/\nApp Service Environment An App Service Environment (ASE) offers a fully isolated and dedicated environment for securely running all kinds of apps, like Web Apps, Mobile Apps, API Apps and Logic Apps. The ASE feature is a deployment of an Azure App Service directly into the Azure Resource Manager virtual network (VNet). ASEs are created inside a subnet of the Azure Virtual Network, which enables your apps to connect securely to endpoints that are accessible only from within the VNet (this includes endpoints that are connected using VPNs and ExpressRoute circuits). ASEs are most suitable for apps that require very high scale, isolation and secure network access and high memory utilization.\nAt the time of writing, Microsoft offers two versions of the App Service Environment, v1 and v2. The former uses workers that need to be added to ASE manually. When you need more capacity, you need to add more workers to the pool. With ASEv2, this is all done automatically. Besides that, ASEv1 has support for a maximum of 50 workers, where ASEv2 supports a maximum of 50 workers. ASEv2 offers faster CPUs, twice the memory per core and SSDs. ASEv1 can be deployed in a classic virtual network and in a Resource Manager virtual network and has a different pricing model.\nIn this demo we are going to use the ASEv2. For more information about V1, you can refer to the following article: App Service Environment v1 introduction.\nCreating an App Service Environment App Service Environments are very expensive, so I strongly advice to remove them immediately after this demo\u0026hellip;\nTo create an App Service Environment, take the following steps:\nNavigate to https://portal.azure.com, Click on Create a new resource, type App Service Environment in the searchbox and create a new ASE.\nAdd the below settings (if you select the ASE pricing details, you can get an idea of the monthly costs of the ASE). You can keep the default Virtual Network Location for this demo.\nClick Create. It will take some time before the ASE is created.\nAdd the Logic App to the ASE When the deployment of the ASE is finished, next is adding the Logic App to the ASE. To do that.\nThe Logic App is added to a Consumption Plan, meaning that you only pay for the action executions that are performed by the Logic App instance are billed. So, the first step is to move the Logic App from the Consumption plan to an App Service Plan.\nAccording to the Microsoft documentation, the way to do this, is to manually redeploy the Logic App into another Service Plan. Since we have initially deployed the Logic App from Visual Studio, we will redeploy the App from there as well.\nTake the following steps to do this:\nOpen Visual Studio 2017, and select the Cloud Explorer tab. In there select the Logic App which was created in the first chapter. Right click it and select Open with Logic App Editor.\n{width=\u0026ldquo;2.0838582677165354in\u0026rdquo; height=\u0026ldquo;5.170795056867892in\u0026rdquo;}\nThis will open the Logic App instance which is deployed in Azure. Click download in the top menu top download the JSON template. Save the template somewhere on your filesystem.\n{width=\u0026ldquo;5.354166666666667in\u0026rdquo; height=\u0026ldquo;4.157694663167104in\u0026rdquo;}\nOpen the SecureLogicApp Project from Visual Studio. On the Solution Explorer tab, open the LogicApp.json file. This will automatically open the JSON Outline tab as well.\nRight click resources, and add and select Add new resource. Select App Service Plan (Server Farm) and add the following settings:\n{width=\u0026ldquo;6.268055555555556in\u0026rdquo; height=\u0026ldquo;3.995138888888889in\u0026rdquo;}\nClick Add.\nRight click the project and select Deploy -\u0026gt; New.\n{width=\u0026ldquo;6.268055555555556in\u0026rdquo; height=\u0026ldquo;3.995138888888889in\u0026rdquo;}\nSelect the appropriate Azure Subscription and Resource Group and click Edit Parameters\u0026hellip;\n{width=\u0026ldquo;4.663207567804024in\u0026rdquo; height=\u0026ldquo;4.005208880139983in\u0026rdquo;}\nFill in the parameters like the values below:\n{width=\u0026ldquo;6.135461504811898in\u0026rdquo; height=\u0026ldquo;3.067730752405949in\u0026rdquo;}\nClick Save and in the next screen click Deploy. The Logic App is now deployed inside an App Service Plan.\n{width=\u0026ldquo;6.268055555555556in\u0026rdquo; height=\u0026ldquo;3.995138888888889in\u0026rdquo;}\nConclusion Usefull links For this post, I\u0026rsquo;ve used the following links for research:\nIntroduction to the App Service Environments ","permalink":"//localhost:1313/posts/2018-05-18-securing-your-azure-logic-apps-part-5-secure-your-logic-app-using-an-app-service-environment/","summary":"\u003cp\u003eThis is the fifth part in the series about securing your Logic Apps. In\nthe previous post, we\u0026rsquo;ve covered how to use the Validate JWT access\nRestriction Policy in API Management for securing your Logic Apps. In\nthis post, I\u0026rsquo;m going to show you how you can add your Logic App to an\nApp Service Environment to create an isolated and secure environment for\nyour app to run in.\u003c/p\u003e\n\u003cp\u003eIn this series:\u003c/p\u003e","title":"Securing Your Azure Logic Apps Part 5: Secure your Logic App using an App Service Environment"},{"content":"Welcome to another Azure Content Spotlight! These articles are used to highlight items in Azure that could be more visible to the Azure community. This weeks content spotlight is all about Azure Cognitive Services. Seth Juarez\u0026rsquo;sAI Show on Channel 9 provides regular updates on all the new AI features on the Azure platform, including Cognitive Services. See below a collection of the latest video\u0026rsquo;s of that show, including video\u0026rsquo;s that cover the updates and enhancements announced at the BUILD 2018 conference:\nWhat’s New with Cognitive Services What’s New with Language Understanding Service (LUIS) What’s New with Cognitive Services Speech SDK and Speech Devices SDK Vision Cognitive Services Updates Bing Visual Search -Sjoukje\n","permalink":"//localhost:1313/posts/2018-05-17-azure-content-spotlight-what-s-new-with-cognitive-/","summary":"\u003cp\u003eWelcome to another Azure Content Spotlight! These articles are used to highlight items in Azure that could be more visible to the Azure community.\nThis weeks content spotlight is all about Azure Cognitive Services. \u003ca href=\"%22https://channel9.msdn.com/Niners/sethjuarez%22\"\u003eSeth Juarez\u0026rsquo;s\u003c/a\u003eAI Show on Channel 9 provides regular updates on all the new AI features on the Azure platform, including Cognitive Services. See below a collection of the latest video\u0026rsquo;s of that show, including video\u0026rsquo;s that cover the updates and enhancements announced at the BUILD 2018 conference:\u003c/p\u003e","title":"Azure Content Spotlight – What’s New with Cognitive Services"},{"content":"This is the fourth part in the series about securing your Logic Apps. In the previous post, I\u0026rsquo;ve described how to secure your logic app using Azure Active Directory. In this post, I\u0026rsquo;m going to show you how you can leverage the Validate JWT Access Restriction Policy for your Logic App inside the API Management Service.\nIn this series:\nSecure access to the trigger.\nSecure your Logic App using API Management \u0026ndash; Access Restriction Policies.\nSecure your Logic App using Azure Active Directory.\nSecure your Logic App using API Management \u0026ndash; Validate JWT Access Restriction Policy (this post).\nSecure your Logic App using API Management \u0026ndash; Authentication Policies.\nThe Validate JWT policy enforces existence and validity of a JSON Web Token (JWT) extracted from either a specified HTTP Header or a specified query parameter. It is based on oAuth 2.0, which is basically the standard nowadays for API\u0026rsquo;s.\nIf you use SDK\u0026rsquo;s in your code, like the Facebook, Twitter or the Office 365 JavaScript API, they will all use oAuth 2.0 and the JWT token for authentication. This policy can also be used if you want your API to be secured using Azure Active Directory. And that is exactly what we have done in the previous post, so we can use this authentication method as an example for this post as well.\nFor this article, I\u0026rsquo;ve used the Logic App which is created in the first post of this series, the API Management service which is created in the second post and the Logic App uses Azure Active Directory for authentication, which is described step-by-step in my previous post.\nValidate JWT Policy In this example, we are going to add a Validate JWT policy which checks the audience inside the JWT token. When the audience is correct, then the call is made to Azure Active Directory to authenticate the user.\nRetrieve JWT Token The first step is to retrieve the JWT token when a call is made to the Logic App which is already registered inside Azure Active Directory. This way we can investigate what is inside of the JWT token and use certain claims from the token in our policy. This way we can reject calls that don\u0026rsquo;t contain a certain claim right at the door without doing a request to Azure AD. For this example, we are going to use the \u0026ldquo;Audience\u0026rdquo; claim. We can obtain the token using Fiddler and the API Management Developer portal.\nOpen the Developer portal by navigating to the Azure API Management Service inside the Azure Portal. In there, click APIs in the left menu.\nClick Developer Portal.\nThen, click APIs in the top menu, click SecureLogicApp and click the Try It button under manual-invoke.\nOpen Fiddler.\nSwitch back to the API Management Portal where the test page for the API is opened now. Scroll down a bit and under Authorization, and from the dropdown select the OAuth 2.0 server which is configured in the previous article.\nA login window pops up. Log in with your Azure AD credentials and switch back to Fiddler. Select the GET request to the Azure AD API from fiddler which contains the following URL: /oauth2/authorizationcode/acquiretoken. Open the inspectors view and copy the Access Token which is returned from Azure AD after login in using your Azure AD credentials.\nJWT Token Details We now have obtained the Access Token and to look into it in further detail, we can use a site called \u0026ldquo;JWT\u0026rdquo; (http://jwt.io). Click the Debugger menu item in the top menu and past the Access token inside the Encoded text box.\nThe token gets decoded and on the right side you can see all the claims that are inside of it. Like mentioned before, we are going to use the audience for this example, which is \u0026ldquo;http://SecureLogicApp\u0026rdquo; in my case.\nIf you want more information about the JWT token and all the properties inside of it, you can refer to the home page of the JWT website.\nSetting up the Validate JWT Token policy Switch back to the Azure API Management Service inside the Azure Portal, and again, click APIs in the left menu.\nSelect the SecureLogicApp and click on the arrow next to Inbound Processing. Select Form-based Editor.\nClick Code View at the right side of the screen.\nAdd the below code inside the \u0026lt;inbound\u0026gt; tag to check the audience inside the JWT token from the caller of the endpoint. If the audience does not match, the API Management Service will simply block the request.\n\u0026lt;validate-jwt header-name=\u0026quot;Authorization\u0026quot; failed-validation-httpcode=\u0026quot;401\u0026quot; failed-validation-error-message=\u0026quot;Unauthorized. Access token is missing or invalid.\u0026quot;\u0026gt;\n\u0026lt;openid-config url=\u0026quot;https://login.microsoftonline.com/sjoukjelive.onmicrosoft.com/.well-known/openid-configuration\u0026quot; /\u0026gt;\n\u0026lt;required-claims\u0026gt;\n\u0026lt;claim name=\u0026quot;aud\u0026quot;\u0026gt;\n\u0026lt;value\u0026gt;http://SecureLogicApp\u0026lt;/value\u0026gt;\n\u0026lt;/claim\u0026gt;\n\u0026lt;/required-claims\u0026gt;\n\u0026lt;/validate-jwt\u0026gt;\nIt will look like below image:\nClick the Save button.\nTo test this policy, switch over to the Developer Portal of the API Management Service. Under the Authorization section, reset the OAuth2 Server to No auth, like show in the below image.\nClick the Send button and you will get a 401 Unauthorized response with the response message which is specified in the policy:\nConclusion This is the last Access Restriction Policy in this series, that you can use to secure your Logic App using API Management. The other ones are described in the second post of this series.\nThe JWT Validate Policy is, in my opinion, a very effective way of securing your API. Based on certain claims, or even certain permissions, you can block your API right at the door even without making a call to your authentication provider to check if the caller has permissions to access the API. This reduces unnecessary time and traffic, which will result in a better experience for the requestor of the API.\nSjoukje ","permalink":"//localhost:1313/posts/2018-05-17-securing-your-azure-logic-apps-part-4-secure-your-logic-app-using-api-management/","summary":"\u003cp\u003eThis is the fourth part in the series about securing your Logic Apps. In\nthe previous post, I\u0026rsquo;ve described how to secure your logic app using\nAzure Active Directory. In this post, I\u0026rsquo;m going to show you how you can\nleverage the Validate JWT Access Restriction Policy for your Logic App\ninside the API Management Service.\u003c/p\u003e\n\u003cp\u003eIn this series:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://blogs.msdn.microsoft.com/azuredev/2017/07/26/securing-your-azure-logic-apps-part-1-secure-access-to-the-trigger/\"\u003eSecure access to the\ntrigger\u003c/a\u003e.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://blogs.msdn.microsoft.com/azuredev/2017/08/16/part-2-secure-your-logic-app-using-api-management-access-restriction-policies/\"\u003eSecure your Logic App using API Management \u0026ndash; Access Restriction\nPolicies.\u003c/a\u003e\u003c/p\u003e","title":"Securing Your Azure Logic Apps Part 4: Secure your Logic App using API Management -- Validate JWT Access Restriction Policy"},{"content":"This is the third part in the series about securing your Logic Apps. In the previous post, I\u0026rsquo;ve described how to add several Access restriction policies to your API inside of the API Management Service. In this post, I was going to show how you can leverage the Validate JWT Access Restriction Policy. But when I did some research for this article, I found out that, to proper implement that policy, it is better to first secure your Logic App with Azure Active Directory using API Management. And as this is also a great topic for this series, I\u0026rsquo;ve decided to add this as well. So, a little side step is added to this series.\nIn this series:\nSecure access to the trigger.\nSecure your Logic App using API Management \u0026ndash; Access Restriction Policies.\nSecure your Logic App with Azure Active Directory using Azure API Management (this post).\nSecure your Logic App using API Management \u0026ndash; Validate JWT Access Restriction Policy.\nSecure your Logic App using API Management \u0026ndash; Authentication Policies.\nFor this article, I\u0026rsquo;ve used the Logic App which is created in the first post of this series, and the API Management service which is created in the second post.\nGrant Permissions inside Azure Active Directory The first step is to grant permissions inside the Azure Active Directory for your Logic App. The Logic App is added to Azure AD as part as the configuring and publishing process from Visual Studio. To access the Azure Directory Tenant from the Azure Portal, click Azure Active Directory in the left menu, and then App Registrations.\nInside the App Registration window, click on the SecureLogicApp. The Settings page is openend. In there, click on Required Permissions, click Add, and click Select the API.\nYou can choose between two different API\u0026rsquo;s in the next screen for authenticating your application with Azure AD. In here select Windows Azure Active Directory.\nIn the next screen, the application permissions are set. As we only want to authenticate to Azure Active Directory, you only need to select the Read Directory Data permissions. Click Select.\nClick Done.\nRegister the Azure API Management Service in Azure Active Directory Next, is to register the Azure API Management Service as an application in Azure Active Directory.\nInside the Azure AD tenant, in the Azure Portal, click App Registrations again. Then click on New Application Registrations, in the top menu.\nName the application, I\u0026rsquo;ve named mine APIMPortal, as an application pick Web app / api and fill in the API Management Service URL as the sign-on URL and append with \u0026lsquo;/signin\u0026rsquo;. In my case this is: https://msdnapimgmt.portal.azure-api.net/signin .\nAfter creation, click the Properties tab of the App. Change the App ID URI to the API Management Service URL and append with \u0026lsquo;SecureLogicApp\u0026rsquo;: https://msdnapimgmt.portal.azure-api.net/SecureLogicApp . Click the Save button.\nConfigure an API Management OAuth 2.0 Authorization Server Now, open the API Management service in the Azure Portal and click OAuth 2.0 in the left menu.\nClick the Add button.\nAdd an optional name, like OAuth2 Server. In the \u0026ldquo;Client registration page URL\u0026rdquo; box, add a placeholder URL, like http://localhost.\nNext add, the Authorization Endpoint URL and the Token Endpoint URL. These values can be retrieved from the App Endpoints Page of the Azure AD application which is registered in the previous step. For this, you have to navigate to the Azure Active Directory tenant in the Azure Portal.\nCopy the OAuth 2.0 authorization endpoint and paste it into the Authorization endpoint URL textbox.\nCopy the OAuth 2.0 token endpoint and paste it into the Token endpoint URL textbox. Add an additional body parameter to the URL with Name = resource and Value = the App ID of the SecureLogicApp. You can obtain this URL from the Azure Active Directory tenant in the Azure Portal.\nNext, specify the Client Credentials. These are the credentials for the resource you want to access, in this case the Azure API Management application which is added in the previous step. In the Azure Portal, this is called the Application ID.\nFor generating a client secret, click Keys in the left menu, add a description and select 1 year for expiration. Click the save button to save the configuration and display of the key.\nCopy the key to the client secret field in the OAuth Service creation page.\nClick Create.\nBelow the client secret is a. authorization code field. Copy the URL, switch back to the Azure AD portal and paste it in the Reply URL field of the API Management App and click Save.\nThe next step is to configure the permissions for the API Management application. Click Required Permissions and check the box for Read Directory Data.\nClick the Add button, then Select An API, and in the search box type the name of the Logic App and select it from the list and click select.\nFor the permissions, there is only one possibility, Access SecureLogicApp. Check it.\nClick the Select button, and after that, the Done button.\nEnable OAuth 2.0 Authorization in the API Management Portal Now that the OAuth 2.0 server is configured, you can enable it in the API Management portal. Switch over to the API Management portal in the Azure Portal, click APIs in the left menu and select the SecureLogicApp. Click the Settings tab.\nIn there, under the Security section, select OAuth 2.0 and below select the OAuth 2.0 server. Save your settings.\nNow, it is time to test the App. Something strange happened here, when I click the Test tab, next to the Settings tab at the top of the window, it is not possible to select the Authorization Server. But, when I open the Developer Portal, there is a possibility to select it. So, we have to do it from there. To open the Developer Portal, click the menu item at the top of the Azure Management Service. Then, click APIs, SecureLogicApp and click the Try it button under manual-invoke.\nScroll down a bit and in the Authorization section, click the OAuth 2 Server dropdown and select Authorization Code.\nAnd now comes the cool part\u0026hellip;.\nAfter selecting Authorization Code, a window pops up! Wow!! Trust the application, by clicking the Accept button.\nSorry for the Dutch language here\u0026hellip;\nAfter accepting the permissions, you get an access token and you can now test your API.\nConclusion There will be a lot of production scenario\u0026rsquo;s where you want to secure your Logic App with Azure AD. There are a lot of steps involved and it was kind of a puzzle to implement this in the \u0026rsquo;new\u0026rsquo; Azure Portal, as the documentation I\u0026rsquo;ve found regarding this topic, was all aiming at the Classic Portal. I actually did not meet my deadline for this article because of this. But finally, it worked\u0026hellip;\nThe next article will cover the Validate JWT Access Restriction Policy. So, stay tuned!\n","permalink":"//localhost:1313/posts/2018-05-16-securing-your-azure-logic-apps-part-3-secure-your-logic-app-using-azure-active-directory/","summary":"\u003cp\u003eThis is the third part in the series about securing your Logic Apps. In\nthe previous post, I\u0026rsquo;ve described how to add several Access restriction\npolicies to your API inside of the API Management Service. In this post,\nI was going to show how you can leverage the Validate JWT Access\nRestriction Policy. But when I did some research for this article, I\nfound out that, to proper implement that policy, it is better to first\nsecure your Logic App with Azure Active Directory using API Management.\nAnd as this is also a great topic for this series, I\u0026rsquo;ve decided to add\nthis as well. So, a little side step is added to this series.\u003c/p\u003e","title":"Securing Your Azure Logic Apps Part 3: Secure your Logic App with Azure Active Directory using Azure API Management"},{"content":"This is the second part in the series about securing your Logic Apps. In the previous post, I\u0026rsquo;ve provided several ways to secure your Logic App at the Trigger Level. In this post, I\u0026rsquo;m going to show how to secure your App using API Management.\nIn this series:\nSecure access to the trigger.\nSecure your Logic App using API Management \u0026ndash; Access Restriction Policies (this post).\nSecure your Logic App using API Management \u0026ndash; Validate JWT Access Restriction Policy.\nSetting up API Management Open the Azure Portal and create a new service. Search for \u0026lsquo;API Management\u0026rsquo; and click the Create button.\nEnter a name for the service, select the Azure subscription and select the resource group. I\u0026rsquo;ve picked the same resource group which I have created for the previous post. Add an organization name, an email address and pick a pricing tier. After that, click Create.\nIt takes some time to create the API Management service. After creation and activation, open it from the Azure Portal**.**\nAdding the API I\u0026rsquo;m using the Logic App which is created in the previous post to import into the publishing portal. So, copy the callback URL from the settings page of the Logic App in the settings portal.\nIn the left menu of your API Management Service, click APIs \u0026ndash; Preview.\nA wizard is opened. Select the Logic App in here.\nYou can now browse for your Logic App. Cool!\nClick the Browse button and select the Logic App that you want to import. Add a API URL suffix, I\u0026rsquo;ve named my \u0026lsquo;secure\u0026rsquo; and click Create.\nNote: If you are using the Logic App which is created in the previous post, don\u0026rsquo;t forget to remove the NotAfter tag from your Logic App code. This will cause an error when importing into the API Management Service.\nAnd there it is!!\nIn this design surface we now have an visual representation of the API, including out Logic App backend. The current context is set for all operations, so all policy changes will take place on a \u0026ldquo;global\u0026rdquo; level.\nSelect \u0026lsquo;Manual-invoke\u0026rsquo; below the already selected \u0026lsquo;All Operations\u0026rsquo;.\nInstead of empty policies, this will display some default policies that are already in place by importing the Logic App.\nAccess Restriction Policies To add authentication to this API we need to add some extra policies to the design surface.\nCheck HTTP Header The first is the Check HTTP header policy. For this we are going add an extra check to the header of the request.\nClick the arrow next to Inbound Processing and click Form-based Editor.\nOn the right, click Code View.\nSet your cursor inside the \u0026lt;inbound\u0026gt; tag and under Access Restriction Policies, click on Check HTTP header.\nThere is a code block added to the xml. Replace it with the following:\n\u0026lt;check-header name=\u0026quot;ExtraCheck\u0026quot; failed-check-httpcode=\u0026quot;401\u0026quot; failed-check-error-message=\u0026quot;Not authorized\u0026quot; ignore-case=\u0026quot;false\u0026quot;\u0026gt;\n\u0026lt;value\u0026gt;Yes\u0026lt;/value\u0026gt;\n\u0026lt;/check-header\u0026gt;\nClick Save.\nClick Test in the left menu, add an additional header to the header section, which is equivalent to the header code block which is added in the previous step, add the body text to the Request Body part and click Send.\nIf anything went well you will a similar response like displayed below:\nLimit Call Rate Per Key Next is the possibility to limit the call rate per key. This adds Throttling capabilities to your API.\nTo add this functionality to your API, open the Code View again (refer to the previous Access Restriction Policy), place the cursor in the \u0026lt;inbound\u0026gt; tag and click the Limit Call Rate Per Key. The boilerplate code is added to the xml.\nReplace the code with the following:\n\u0026lt;rate-limit-by-key calls=\u0026quot;10\u0026quot; renewal-period=\u0026quot;60\u0026quot; increment-condition=\u0026quot;@(context.Response.StatusCode == 200)\u0026quot; counter-key=\u0026quot;@(context.Request.IpAddress)\u0026quot;/\u0026gt;\nIn the above sample, the API only allows 10 requests in 1 minute from a certain IP Address.\nFor more samples about Advanced Throttling, check the following article: https://docs.microsoft.com/en-us/azure/api-management/api-management-sample-flexible-throttling\nSet Usage Quota Per Key In addition to the above, you can also set throttling based on a certain quota.\nAdd the Boilerplate code to the code view to the \u0026lt;inboud\u0026gt; tag, by clicking the Set usage quota per key link on the right.\nReplace the code with the following:\n\u0026lt;quota-by-key calls=\u0026quot;10000\u0026quot; bandwidth=\u0026quot;40000\u0026quot; renewal-period=\u0026quot;2629800\u0026quot;\nincrement-condition=\u0026quot;@(context.Response.StatusCode \u0026gt;= 200 \u0026amp;\u0026amp; context.Response.StatusCode \u0026lt; 400)\u0026quot;\ncounter-key=\u0026quot;@(context.Request.IpAddress)\u0026quot; /\u0026gt;\nThis policy restricts a single client IP address to a total of 10000 calls and 40,000 kilobytes of bandwidth per month. You can also combine these last two access restriction policies.\nRestrict Caller IPs You can also allow or deny calls from specific IP addresses by adding the following piece of xml to the \u0026lt;inbound\u0026gt; tag:\n\u0026lt;ip-filter action=\u0026quot;forbid\u0026quot;\u0026gt;\n\u0026lt;address\u0026gt;192.168.1.1\u0026lt;/address\u0026gt;\n\u0026lt;/ip-filter\u0026gt;\nOr use the following for a range of IP addresses:\n\u0026lt;ip-filter action=\u0026quot;forbid\u0026quot;\u0026gt;\n\u0026lt;address-range from=\u0026quot;192.168.1.1\u0026quot; to=\u0026quot;192.168.1.100\u0026quot;/\u0026gt;\n\u0026lt;/ip-filter\u0026gt;\nConclusion The new preview API functionality I\u0026rsquo;ve used in this post works a lot better than the previous way of adding your policies!\nAzure API Management adds a lot of functionality for authentication scenario\u0026rsquo;s. Most of the Access Restriction Policies are discussed in this post, except for the Validate JWT policy. I\u0026rsquo;m going to write about that one in a separate post, because it is basically too much to add to this post as well.\nSo, there is more to come in this series of posts\u0026hellip;\n-Sjoukje\n","permalink":"//localhost:1313/posts/2018-05-15-securing-your-azure-logic-apps-part-2-secure-your-logic-app-using-api-management/","summary":"\u003cp\u003eThis is the second part in the series about securing your Logic Apps. In\nthe previous post, I\u0026rsquo;ve provided several ways to secure your Logic App\nat the Trigger Level. In this post, I\u0026rsquo;m going to show how to secure your\nApp using API Management.\u003c/p\u003e\n\u003cp\u003eIn this series:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://blogs.msdn.microsoft.com/azuredev/2017/07/26/securing-your-azure-logic-apps-part-1-secure-access-to-the-trigger/\"\u003eSecure access to the\ntrigger\u003c/a\u003e.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSecure your Logic App using API Management \u0026ndash; Access Restriction\nPolicies (this post).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSecure your Logic App using API Management \u0026ndash; Validate JWT Access\nRestriction Policy.\u003c/p\u003e","title":"Securing Your Azure Logic Apps Part 2: Secure your Logic App using API Management -- Access Restriction Policies"},{"content":"I think Azure Logic Apps is a great way to implement scalable integrations in the Cloud. It comes with over 100 connectors for integrating applications and data in the cloud and on-premises. You can now choose to use the Logic App designer or create your apps from Visual Studio, call Azure Functions from inside of your Logic App, easily scale up and down, and all this for a fair price. I truly believe this is going to be the future of Enterprise Integration and when I get the chance, I\u0026rsquo;m using them in all my client projects.\nBut how can you make these applications secure? There are a couple of ways you can achieve this. You can:\nSecure access to the trigger.\nSecure your Logic App using API Management.\nAnd in this first post, I will show you how to secure the access to the trigger of the Logic App.\nSetting up the sample solution For this, I am going to create a Logic App Solution in Visual Studio. For more information about the prerequisites and how to create such a solution, follow the steps described here. You have to install the Azure Logic Apps Tools for Visual Studio 2017 as well.\nThis app does nothing more than respond to a HTTP request and then sending an email to my own email address using my Outlook.com account. If you don\u0026rsquo;t have a Outlook.com or Hotmail account, you can use Gmail as well.\nSo, create a new project in Visual Studio and in the Solution Explorer, right click the LogicApp.json file and select Open with Logic App Designer.\nSelect your Azure subscription and the Resource Group in the next widget screen. The Logic App designer is opened in Visual Studio after clicking OK (How cool is that!).\nSelect the HTTP Trigger, this will be added to the design canvas. Click on the action and fill in the below JSON request body:\n{\n\u0026quot;type\u0026quot;: \u0026quot;object\u0026quot;,\n\u0026quot;properties\u0026quot;: {\n\u0026quot;text\u0026quot;: {\n\u0026quot;type\u0026quot;: \u0026quot;string\u0026quot;\n}\n},\n\u0026quot;required\u0026quot;: [\u0026quot;text\u0026quot;]\n}\nClick the advanced option body and select POST from the Method dropdownlist. The request action will look the below image:\nClick the Next Step button, and then Add an action.\nIn the Search Box type, Outlook.com (or Gmail) and select the Outlook.com connector.\nThe Connector is added to the canvas. Fill in your credentials, and fill in a receiver email address, a subject and body. As a body, I\u0026rsquo;ve selected the body from the request.\nThe sample application is now ready. Right Click the project in the Solution Explorer and click Deploy 🡪 New. Fill in the correct Resource Group information in the wizard, which is opened and click Deploy.\nSecure Access to the Trigger This sample logic app fires on a HTTP Request (this can be a Webook as well). You can restrict access to the app, so that only authorized clients can fire the logic app. All requests into a logic app are encrypted and secured via SSL.\nEvery Logic App request endpoint includes a Shared Access Signature (SAS) as part of the URL, like https://prod-13.westeurope.logic.azure.com:443/workflows/\u0026lt;logic app id\u0026gt;/triggers/manual/paths/invoke?api-version=2016-06-01\u0026amp;sp=%2Ftriggers%2Fmanual%2Frun\u0026amp;sv=1.0\u0026amp;sig=xTG65EBWRwxxQXyqRUn7MDjbc4KFDm_Q-R6v9zOvmVU . The URL contains the following:\nsp: Permissions are specified here.\nsv: this is the version used to generate.\nsig: this is used for authenticating the trigger.\nThe Signature (sig) is generated using a SHA256 algorithm with a secret key on all the URL paths and properties. The secret key is never exposed and published, and your Logic App only accepts triggers that contain a valid signature created with the secret key.\nRegenerating an Access Key When deployed from Visual Studio, there is already an SAS generated automatically. If you want to regenerate the Access Key, take the following steps:\nNavigate to the Azure Portal and open the Logic App which is deployed from Visual Studio earlier. Click the Access Keys menu item under Settings. In here you can regenerate the Access Key, as shown in the image below.\nAfter generating the key, you can obtain the URL including the newly generated SAS from the Overview page under Callback url (POST).\nCallback URLs with an expiration date You can also create Callback URLs with an expiration date. This way you can restrict your app for a certain timeframe.\nYou first need to create a App Service Principal in Azure, to add to the header of the POST request. I created this using the Azure Cloud Shell (Preview). To do this, open the Azure Portal and click on the Azure Cloud Shell item. Add the following code to the Cloud Shell to create the Service Principal:\nazure ad sp create -n SecureLogicApp -p \u0026ldquo;test\u0026rdquo;\nAs you can see in the Cloud Shell, the Service Principal is created. Copy the Object Id from the Shell to the clipboard, and type the below code into the Cloud Shell:\nazure role assignment create --objectId 3a9448e9-7fd4-4ff6-a09d-744a066545ae -o Reader\nYou now have granted the service principal permissions on your subscription and you have added the service principal to the Reader role, which grants permission to read all resources in the subscription.\nTo add an expiration date to the trigger of the Logic App, open the LogicApp.json in code view in Visual Studio. Locate the triggers section and add the following to it inside the \u0026lsquo;schema\u0026rsquo; section:\n\u0026quot;NotAfter\u0026quot;: \u0026quot;2017-07-26T11:00:00.511Z\u0026quot;,\nAdjust the date and time to your needs. The triggers section will now look like below:\n\u0026quot;triggers\u0026quot;: {\n\u0026quot;manual\u0026quot;: {\n\u0026quot;type\u0026quot;: \u0026quot;Request\u0026quot;,\n\u0026quot;kind\u0026quot;: \u0026quot;Http\u0026quot;,\n\u0026quot;inputs\u0026quot;: {\n\u0026quot;schema\u0026quot;: {\n\u0026quot;NotAfter\u0026quot;: \u0026quot;2017-07-26T11:00:00.511Z\u0026quot;,\n\u0026quot;type\u0026quot;: \u0026quot;object\u0026quot;,\n\u0026quot;properties\u0026quot;: {\n\u0026quot;text\u0026quot;: {\n\u0026quot;type\u0026quot;: \u0026quot;string\u0026quot;\n}\n},\n\u0026quot;required\u0026quot;: [\n\u0026quot;text\u0026quot;\n]\n},\n\u0026quot;method\u0026quot;: \u0026quot;POST\u0026quot;\n}\n}\n},\nNow, redeploy the project to Azure.\nNote: If you call the API from Postman, don\u0026rsquo;t forget to add the Authorization token to the header.\nRestrict incoming IP addresses In addition to the Shared Access Signature, you can restrict calling a Logic App only from specific clients, like from a certain IP address or range of IP addresses.\nThis setting can be configured from the Azure Portal or from Visual Studio.\nInside the Azure Portal, from the settings menu of the Logic App select Access Control Configuration, and select Specific IP Ranges from the dropdown. Here, provide the IP address or range addresses and click the Save button:\nConclusion When you are using Azure Logic Applications in a production environment you want to secure them as best as possible. However, there are multiple ways to secure your Logic Apps. In this post, I\u0026rsquo;ve described how security can be applied to the Trigger level of the Logic App. In the next post, I will give more details about securing your Logic App using API Management.\n-Sjoukje\n","permalink":"//localhost:1313/posts/2018-05-14-securing-your-azure-logic-apps-part-1-secure-access-to-the-trigger/","summary":"\u003cp\u003eI think Azure Logic Apps is a great way to implement scalable\nintegrations in the Cloud. It comes with over 100\n\u003ca href=\"https://docs.microsoft.com/en-us/azure/connectors/apis-list\"\u003econnectors\u003c/a\u003e\nfor integrating applications and data in the cloud and on-premises. You\ncan now choose to use the Logic App designer or create your apps from\nVisual Studio, call Azure Functions from inside of your Logic App,\neasily scale up and down, and all this for a fair price. I truly believe\nthis is going to be the future of Enterprise Integration and when I get\nthe chance, I\u0026rsquo;m using them in all my client projects.\u003c/p\u003e","title":"Securing Your Azure Logic Apps Part 1: Secure access to the trigger"},{"content":"Welcome to another Azure Content Spotlight! These articles are used to highlight items in Azure that could be more visible to the Azure community. Microsoft has made a huge step in their drive to eliminate passwords for good! In the next Windows updates, they\u0026rsquo;re adding a limited preview of FIDO2 security key support. This will improve security significantly and cut the risk of phishing attacks.This new capability will give employees the ability to sign in to an Azure Active Directory-joined Windows 10 PC without a username or password. All that is needed is to insert a FIDO2compliant security key into their USB port and they will automatically be signed in to the device and get Single-sign on access to all Azure AD protected cloud resources. Check out the following video to see how this works: https://www.youtube.com/watch?v=f9lqFsWiyPg\nSjoukje ","permalink":"//localhost:1313/posts/2018-04-26-azure-content-spotlight-password-less-sign-in-to-a/","summary":"\u003cp\u003eWelcome to another Azure Content Spotlight! These articles are used to highlight items in Azure that could be more visible to the Azure community.\nMicrosoft has made a huge step in their drive to eliminate passwords for good!\nIn the next Windows updates, they\u0026rsquo;re adding a limited preview of FIDO2 security key support. This will improve security significantly and cut the risk of phishing attacks.This new capability will give employees the ability to sign in to an Azure Active Directory-joined Windows 10 PC without a username or password. All that is needed is to insert a \u003ca href=\"%22https://fidoalliance.org/fido2/%22\"\u003eFIDO2\u003c/a\u003ecompliant security key into their USB port and they will automatically be signed in to the device and get Single-sign on access to all Azure AD protected cloud resources.\nCheck out the following video to see how this works:\n\u003ca href=\"%22https://www.youtube.com/watch?v=f9lqFsWiyPg%22\"\u003ehttps://www.youtube.com/watch?v=f9lqFsWiyPg\u003c/a\u003e\u003c/p\u003e","title":"Azure Content Spotlight – Password-less Sign-in to Azure \u0026 Windows 10"},{"content":"Welcome to another Azure Content Spotlight! These articles are used to highlight items in Azure that could be more visible to the Azure community. IoT is hot! Microsoft recently announced that they are going to invest $5 billion in the Internet of Things over the next four years. Why? Because their goal is to give every customer the ability to transform their businesses, and the world at large, with connected solutions. And why now? Because they now see the kind of increased adoption and exponential growth that analysts have been forecasting for the past years. You can refer to the full article here:https://blogs.microsoft.com/iot/2018/04/04/microsoft-will-invest-5-billion-in-iot-heres-why/ But now back to the development part of it. IoT offers tremendous opportunities for developers and the recently published IoT Interactive Development Guideoffers common application patterns and tutorials that will get you up and running fast. If you are planning on doing some IoT development or if you are just curious, you should really take a look! It offers a lot of valuable information for beginning and more experienced IoT developers. -Sjoukje\n","permalink":"//localhost:1313/posts/2018-04-11-azure-content-spotlight-azure-iot-interactive-deve/","summary":"\u003cp\u003eWelcome to another Azure Content Spotlight! These articles are used to highlight items in Azure that could be more visible to the Azure community.\nIoT is hot! Microsoft recently announced that they are going to invest $5 billion in the Internet of Things over the next four years. Why? Because their goal is to give every customer the ability to transform their businesses, and the world at large, with connected solutions. And why now? Because they now see the kind of increased adoption and exponential growth that analysts have been forecasting for the past years. You can refer to the full article here:\u003ca href=\"%22https://blogs.microsoft.com/iot/2018/04/04/microsoft-will-invest-5-billion-in-iot-heres-why/%22\"\u003ehttps://blogs.microsoft.com/iot/2018/04/04/microsoft-will-invest-5-billion-in-iot-heres-why/\u003c/a\u003e\nBut now back to the development part of it. IoT offers tremendous opportunities for developers and the recently published \u003ca href=\"%22https://content.microsoft.com/iot/developerguide%22\"\u003eIoT Interactive Development Guide\u003c/a\u003eoffers common application patterns and tutorials that will get you up and running fast. If you are planning on doing some IoT development or if you are just curious, you should really take a look! It offers a lot of valuable information for beginning and more experienced IoT developers.\n-Sjoukje\u003c/p\u003e","title":"Azure Content Spotlight – Azure IoT Interactive Development Guide"},{"content":"Welcome to another Azure Content Spotlight! These articles are used to highlight items in Azure that could be more visible to the Azure community. More and more organizations are embracing the Microsoft Cloud these days and want to upload their offline data stores into Azure to benefit of all the features and capabilities Azure has to offer. Microsoft (together with partners) introduces an offline service for migrating these on-premises data easily. Organizations will only have to choose the Azure storage tier and the partners will take care of the rest. This service is suitable for all types of media and data, like SAN and NAS data, disks or tapes, or even video cassettes or 35mm, and partners can work with organizations all over the globe.\nTake a look at the following website for more information: https://azure.microsoft.com/en-us/services/storage/databox/offline-media-import/\n-Sjoukje\n","permalink":"//localhost:1313/posts/2018-04-11-azure-content-spotlight-offline-media-import-for-a/","summary":"\u003cp\u003eWelcome to another Azure Content Spotlight! These articles are used to highlight items in Azure that could be more visible to the Azure community.\nMore and more organizations are embracing the Microsoft Cloud these days and want to upload their offline data stores into Azure to benefit of all the features and capabilities Azure has to offer. Microsoft (together with partners) introduces an offline service for migrating these on-premises data easily. Organizations will only have to choose the Azure storage tier and the partners will take care of the rest. This service is suitable for all types of media and data, like SAN and NAS data, disks or tapes, or even video cassettes or 35mm, and partners can work with organizations all over the globe.\u003c/p\u003e","title":"Azure Content Spotlight – Offline media import for Azure"},{"content":"First, you need to download and install the Office 365 Developer Tools: https://www.visualstudio.com/en-us/features/office-tools-vs.aspx\nThis is the first part in the series on developing an Office 365 app. In this article I will explain how you can associate your Office 365 account with Azure AD, which is the first step in Developing Office 365 Apps.\nIn my case I\u0026rsquo;ve created an Office 365 Developer account using my MSDN Premium subscription (Ultimate will do as well). This means that you will get a Microsoft Office 365 Enterprise E3 Developer account for free. You can also register for a trial account or buy an Office 365 license. For an overview of the different Office 365 versions you can check my post: Detailed Office 365 version Comparison\nAfter you created the account, you can associate your account with your existing Microsoft Azure subscription.\nLog on to the Azure Management Portal with your existing Azure credentials.\nSelect Active Directory \u0026ndash; Directory \u0026ndash; New\nSelect Directory \u0026ndash; Custom Create\nIn the Directory Drop down, select Use existing directory and check the box I am ready to be signed out now. Then click the checkmark in the lower right corner.\nYou are signed out. Now, log in with your Office 365 credentials. You will be prompted whether you will use your directory with Azure.\nClick continue and then click sign out now.\nAgain, sign in with your Azure Credentials and you will see that the Office 365 directory is added to your subscription.\nWrap-up This is the first article in a series on Office 365 development. There will be more, so stay tuned!\n","permalink":"//localhost:1313/posts/2017-11-08-developing-office-365-apps-part-1-associate-your-office-365-account-with-azure-ad/","summary":"\u003cp\u003eFirst, you need to download and install the Office 365 Developer Tools:\n\u003ca href=\"https://www.visualstudio.com/en-us/features/office-tools-vs.aspx\"\u003ehttps://www.visualstudio.com/en-us/features/office-tools-vs.aspx\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThis is the first part in the series on developing an Office 365 app. In\nthis article I will explain how you can associate your Office 365\naccount with Azure AD, which is the first step in Developing Office 365\nApps.\u003c/p\u003e\n\u003cp\u003eIn my case I\u0026rsquo;ve created an Office 365 Developer account using my MSDN\nPremium subscription (Ultimate will do as well). This means that you\nwill get a Microsoft Office 365 Enterprise E3 Developer account for\nfree. You can also register for a trial account or buy an Office 365\nlicense. For an overview of the different Office 365 versions you can\ncheck my post: \u003ca href=\"http://puttysoft.net/index.php/2016/01/27/detailed-office-365-version-comparison/\"\u003eDetailed Office 365 version\nComparison\u003c/a\u003e\u003c/p\u003e","title":"Developing Office 365 Apps Part 1 -- Associate your Office 365 account with Azure AD"},{"content":"2 different ways of using Azure Key Vault in your Logic Apps Security is key nowadays and this means that you should secure your applications to the max. Azure Key Vault offers you a way to safely store your credentials, certificates and hardware security modules in Azure. These values can also be used inside your custom applications, like in web applications, Azure Functions, and Azure Logic Apps.\nAzure Logic Apps offers an out-of-the-box connector that you can use to retrieve values easily and use them in your Logic App. However, there is a caveat here. You cannot use Managed Identities to access the Key Vault using the connector. Managed Identities are service principals in Azure Active Directory that are used to connect different services. You can use the identity to authenticate to any service that supports Azure AD authentication, including Key Vault, without any credentials in your code.\nIn this blog, I\u0026rsquo;m going to show you two different ways of connecting to Azure Key Vault from your Logic App. You will get an overview of the differences when using both the approaches from a security perspective. The Logic App is used to create an external user in Azure AD B2B. Therefore, it needs an Azure AD app registration that has permissions to access the Microsoft Graph for creating external users in our Azure AD B2B tenant. The values for accessing the Microsoft Graph, such as the Azure AD tenant ID, App ID, and App secret are stored inside the Key Vault and retrieved in the Logic App.\nBefore starting this demo, I already created an app registration in Azure AD, added delegated permissions to access the Invitation API of the Microsoft Graph and copied the Azure AD tenant ID, App ID, and App secret to notepad. For more information on how to register an Azure AD app and set the required permissions, you can refer to this post: https://sjoukjezaal.com/azure-b2b-sharepoint-online-solution-using-powerapps-flow-and-the-graph-api/. I also already deployed a Logic App called KeyVaultConnector in a new resource group called KeyVaultConnectorGroup. I used the Recurrence trigger for the Logic App. If you want more information on how to create a Logic App, you can refer to the following post: https://docs.microsoft.com/en-us/azure/logic-apps/quickstart-create-first-logic-app-workflow.\nNow that we have everything in place, we can set up our Key Vault.\nSetting up the Key Vault To set up the Key Vault, navigate to the Azure portal, select Cloud Shell in the top right corner, select Bash and add the following line of code:\nCreate the Key Vault in the same resource group as where the Logic App is deployed (make sure you set an unique name for the Key Vault):\nAfter creation, we can add the values of the Service Principal to it. For this, navigate to the Azure Key Vault overview page in the Azure portal.\nIn the left menu, under settings select Secrets. In the top menu, click + Generate/import. There you can create the secrets for the service principal. Make sure Manual is selected for and add the values for the Service Principal:\nAfter adding the three values, the list will look like the following:\nNow that we have setup the Key Vault, we can create our Managed Identity for the Logic App.\nCreate Managed Identity for the Logic App. To create the Managed Identity, navigate to the created Logic App in the Azure portal.\nFrom there, in the left menu under Settings, select Identity.\nSet the status to On and click Save. This will create the Managed Identity for the Logic App using the exact same name as the Logic App:\\\nNow that we have created the Managed Identity for the Logic App, we can add it to the Key Vault to access the values for the Azure AD registration.\nAdd Managed Identity to Key Vault To add the Managed Identity that we created in the previous step to the Key Vault, you have to take the following steps:\nIn the Key Vault overview blade, select Access Policies in the left menu. Then select **+ Add access policy:\\\nSelect the following values:\nSecret Permissions: List, Get (this will allow the Logic App to retrieve values from the Key Vault)\nSelect Principal: KeyVaultConnector\nThen select **Add:\\\nThe Managed Identity of the Logic App now has permissions to retrieve values from the Key Vault. In the next step we are going to retrieve the values in our Logic App.\nUsing the Key Vault connector in the Logic App The out-of-the-box Key Vault connector is the easiest way of retrieving secrets, keys, certificates and other values from the Key Vault in a Logic App. In this step we are going to use this connector and see what it has to offer. Therefore, navigate to the Logic App that you have created in the Azure portal.\nClick Edit to open the Logic App designer.\nUnder the Recurrence trigger, click + New step. Search for Key Vault and select Get secret from the list:\nSpecify the name of the Key Vault.\nThere are two different options here to authenticate. The first is to sign-in to the Key Vault using your credentials. The other one is to connect with a Service Principal. When you select the latter here, you have to specify the Vault name, Client ID, Client Secret and Tenant ID. You are not allowed to select the Managed Identity here:\nSince the Managed Identity is created automatically, we don\u0026rsquo;t have access to the secret. So, we cannot use it in here. Therefore, we are going to connect with our administrator credentials for now. Click Connect with sign in and click the Sign in button.\nPick your account, in my case this is an administrator account, so I have access to the Key Vault by default. After authenticating you can select the secrets from the Key Vault:\nThis is not an ideal way of authenticating. Of course, you can use an account with least privileges, but we want to connect with our Managed Identity. By the time of writing this blog, the only way to connect to the Key Vault using the Managed Identity is by using a HTTP action and make a request to the Key Vault API. Let\u0026rsquo;s do this in the next part.\nConnect to the Key Vault using the API Delete the Azure Key Vault action from the Logic App designer and take the following steps:\nAdd a new step to the canvas.\nSearch for HTTP and select the HTTP action. Then select HTTP from the list.\\\nAdd the following values:\nMethod: GET\nURI: https://SZConnectorKeyVault.vault.azure.net/secrets/TenantID (change this if your Key Vault has a different name)\nQueries: api-version; 2016-10-01\nThen select Add new parameter and select Authentication.\nAuthentication:\nAuthentication type: Managed Identity\nManaged Identity: System Assigned Managed Identity\nAudience: https://vault.azure.net\nNow, we can use the Managed Identity to connect to the Key Vault.\nRepeat this last step until you have retrieved all three of the secrets from the Key Vault, including the App ID and the App secret. This will look like the following image:\nNow that we have our Key Vault secrets using the Managed Identity, we can store them in variables. Therefore add three new steps and select the Initialize variable actions. For each variable, add a name, such as TenantID, AppID and AppSecret of type string and add for each action add an expression to the Value field with the following expressions:\nValue 1: body('Get_TenantID')?['value']\nValue 2: body('Get_AppID')?['value']\nValue 3: body('Get_AppSecret')?['value']\nThis will look like the following image:\nNext, make a request to the Graph API. First, we need an access token from Azure AD. Add another HTTP action under the previous one and add the following values (replace the TenantID, AppID and AppSecret with the variables that were created in the previous steps):\nMethod: POST\nURI: https://login.microsoftonline.com/{'TenantID'}/oauth2/token\nHeaders: Content-Type; application/x-www-form-urlencoded\nBody: grant_type=client_credentials\u0026amp;client_id=@{'AppID'}\u0026amp;client_secret={ 'AppSecret'}\u0026amp;resource=https://graph.microsoft.com\nThis will look like the following image:\nBefore we proceed this, first save the Logic App and Run it.\nWhen the Logic App finished executing Click on the last HTTP action and copy the body from the output and paste it into Notepad.\nOpen the Logic App in edit mode again and add a new step and select the Parse JSON action. We are going to parse the access token to a variable so we can use it in the next step. Add the following values:\nContent: Select the Body from the previous step.\nSchema: For this, select Use sample payload to generate schema and paste the output from the previous step here to let the schema be generated based on the output of the body.\nAdd a new HTTP action for the request to the Microsoft Graph using the access token. Add the following values:\nMethod: POST\nURI: https://graph.microsoft.com/v1.0/invitations (the URL to the Azure AD B2B REST API\nHeaders: Authorization; Bearer {access_token}. Here select the access token from the dynamic content list and make sure there is a whitespace between the Bearer part and the access token.\nBody:\n{\n\u0026quot;inviteRedirectUrl\u0026quot;: \u0026quot;https://myapps.microsoft.com\u0026quot;,\n\u0026quot;invitedUserDisplayName\u0026quot;: \u0026quot;Sjoukje Zaal\u0026quot;,\n\u0026quot;invitedUserEmailAddress\u0026quot;: \u0026quot;sjoukje@emailaddress.com\u0026quot;,\n\u0026quot;invitedUserMessageInfo\u0026quot;: {\n\u0026quot;customizedMessageBody\u0026quot;: \u0026quot;Hey there! Check this out. I created an invitation through the Graph API\u0026quot;\n},\n\u0026quot;sendInvitationMessage\u0026quot;: true\n}\nClick Save and run the Logic App again. This will create an external user in Azure AD B2B. Wrap up We have now successfully created an external user in our Azure AD B2B tenant using the credentials that were stored inside the Azure Key Vault. We have used two different approaches for this, the former using the out-of-the-box connector which offers a lot of functionality but cannot be used in conjunction with a Managed Identity, and the latter, using the Key Vault API and the Managed Identity.\nAuthenticating using a Managed Identity is the most secure way of connecting to the Key Vault because you don\u0026rsquo;t have to login using any other credentials except the Logic App credentials and you don\u0026rsquo;t have to maintain any recycling of access tokens. This is all handled for you by Azure.\n","permalink":"//localhost:1313/posts/2017-10-25-2-ways-azure-key-vault-logic-apps/","summary":"\u003ch1 id=\"2-different-ways-of-using-azure-key-vault-in-your-logic-apps\"\u003e2 different ways of using Azure Key Vault in your Logic Apps\u003c/h1\u003e\n\u003cp\u003eSecurity is key nowadays and this means that you should secure your applications to the max. Azure Key Vault offers you a way to safely store your credentials, certificates and hardware security modules in Azure. These values can also be used inside your custom applications, like in web applications, Azure Functions, and Azure Logic Apps.\u003c/p\u003e\n\u003cp\u003eAzure Logic Apps offers an out-of-the-box connector that you can use to retrieve values easily and use them in your Logic App. However, there is a caveat here. You cannot use Managed Identities to access the Key Vault using the connector. Managed Identities are service principals in Azure Active Directory that are used to connect different services. You can use the identity to authenticate to any service that supports Azure AD authentication, including Key Vault, without any credentials in your code.\u003c/p\u003e","title":"2 Different Ways of Using Azure Key Vault in Your Logic Apps"},{"content":"Azure Active Directory is the heart of everything inside of Microsoft Azure. All Azure services are depending on it and using it for Identity Management in the Microsoft Cloud. Office 365, Intune, Exchange Online, Enterprise Mobility Suite, are all examples of Azure Services depending on Azure Active directory for both security and identity management. And when implemented right, all your custom applications, which are hosted in Azure or somehow are integrating with Azure services are using Azure Active Directory as well.\nThat\u0026rsquo;s why it is high on the agenda of Microsoft and it is constantly evolving. In this article, I want to give an overview of the different flavors of Azure Active Directory and where it stands right know, because what once started as a basic user directory, has now become something much more than that\u0026hellip;\nAzure Active Directory As pointed out before Azure AD is the heart of everything inside of Azure. Plain Azure AD, is the \u0026ldquo;root service\u0026rdquo; where you create the user accounts for your organization and which can be used for application access management. It is designed for a single tenant, so it is designed for a single organization. This doesn\u0026rsquo;t mean that it doesn\u0026rsquo;t supports a multi-domain environment, but there are differences and restrictions. Azure Active Directory has been set up using a different architecture as the Windows Server Active Directory was set up many years ago.\nCustomers that are using Office 365, Intune or Dynamics CRM online, are not always aware of the fact that they are using Azure Active Directory. It can easily be integrated with an existing Windows Server Active Directory using AAD Connect, so the on-premises identity investments that are already made by organizations, can be leveraged in the cloud as well.\nAzure Active Directory also consists of a full suite of enterprise identity management capabilities, which are now available for the smaller companies as well for a fair cost. It includes Multi-Factor Authentication, device registration, self-service password management, self-service group management, privileged account management, role based access control, application usage monitoring, rich auditing and security monitoring and alerting. It depends of the Azure AD edition if these features are included. There are three editions, Azure Active Directory Basic, Premium P1 and Premium P2. You can refer to the following article on what is included in these different editions: https://azure.microsoft.com/en-us/pricing/details/active-directory/\nAzure Active Directory B2B Azure Active Directory Business-to-Business is a fairly new service which offers collaboration capabilities for organization which are using Azure AD. With Azure AD B2C you can work safely and securely with users from other organizations.\nWith the use of this service, organizations can provide access to documents, resources and applications to their partners, while maintaining complete control over their own corporate data. Developers can use the Azure AD B2C API\u0026rsquo;s to write applications that bring two organizations together.\nPartners can use their own credentials to sign in, and there is no requirement for having an Azure AD tenant of their own. This means you don\u0026rsquo;t have to manage external accounts anymore.\nCorporate data is protected using policies which can be added at the tenant level, the application level and user level.\nThis sounds very promising and I really hope this can replace the external sharing feature of Office 365, as this is not an enterprise ready sharing mechanism in my opinion.\nYou can watch the video with more information on Azure B2B on Youtube\nAzure Active Directory B2C Azure AD Business to Consumer is a cloud identity management solution for mobile and web applications. It is a highly available and it can scale to hundreds of millions of identities. Azure AD B2C is not Azure AD, it is a developer feature which can be leveraged in custom applications.\nWith minimal configuration, Azure AD B2C offers the following authentication providers:\nSocial Accounts (like Facebook, Google, LinkedIn and more)\nEnterprise Accounts (using open standards protocols, like OpenID Connect or SAML)\nLocal Accounts (like accounts using email address and password or username and password)\nBeside these authentication providers, additional ones can be added as well through the Azure Portal. New authentication providers are constantly added by Microsoft, so if your application uses some other authentication provider, there\u0026rsquo;s a big change you can add this one to Azure B2C in the Azure portal.\nIn Azure B2C, you cannot use the employee identities which are stored in Azure AD. It is a separate product and cannot be integrated with Azure AD. So, you can\u0026rsquo;t use the features that are offered for Azure AD inside of Azure B2C. What it does offer is, MFA, sign-in reports, usage reports and audit reports.\nAzure AD Directory Services (AADDS) Azure AD Directory Services is an extension of Azure AD and it provides managed domain services such as domain join, group policy, LDAP, Kerberos/NTLM authentication that are fully compatible with Windows Server Active Directory.\nLegacy applications often depend on LDAP or Windows Integrated Authentication (NTLM or Kerberos) to authenticate users. To migrate those applications to the cloud, these dependencies on the corporate identity infrastructure need to be resolved.\nSo, instead of having to deploy VM\u0026rsquo;s with domain controllers in the cloud or deploy a site-to-site VPN connection organizations can use Azure AD Directory Services for authenticating users in hybrid scenario\u0026rsquo;s.\nFor more information you can refer to the following site: https://docs.microsoft.com/en-us/azure/active-directory-domain-services/active-directory-ds-overview\nConclusion Azure Active Directory has a lot to offer these days. Additional features can help organization to overcome the flaws in the original architecture, as it was designed in the early Azure days. You are not bound to that one Azure AD tenant anymore.\nAlso, in hybrid scenarios Azure AD has a lot to offer. Migrating to the cloud, is much easier using these features, and it is even possible to keep a part of your applications on-premises.\nHopefully, you got some more understanding of the different services and features after reading this article. And more hopefully, you are going to use them for your organization and applications in the near future.\nCheers, Sjoukje\nPs. I have big expectations of Azure AD B2B myself. So, if anyone is already using this in conjunction with Office 365, please let me know. I\u0026rsquo;m really curious if this is working as expected.\n","permalink":"//localhost:1313/posts/2017-08-03-azure-active-directory-flavors/","summary":"\u003cp\u003eAzure Active Directory is the heart of everything inside of Microsoft Azure. All Azure services are depending on it and using it for Identity Management in the Microsoft Cloud. Office 365, Intune, Exchange Online, Enterprise Mobility Suite, are all examples of Azure Services depending on Azure Active directory for both security and identity management. And when implemented right, all your custom applications, which are hosted in Azure or somehow are integrating with Azure services are using Azure Active Directory as well.\u003c/p\u003e","title":"Azure Active Directory Flavors"},{"content":"Veel bedrijven maken tegenwoordig een digitale transformatie door. Dit modewoord duikt overal op. Als ik zoek naar een aantal definities van wat digitale transformatie nu eigenlijk inhoudt, dan komt het volgende o.a. naar voren:\nDigitale transformatie betreft de impact van (digitale) technologieën en de toenemende digitalisering van bedrijfsprocessen op mens, bedrijf en zelfs maatschappij.\nOrganisaties die zich richten op de verdigitalisering van het landschap door in te spelen op de (veranderende) wensen en behoeften van klant, werknemers en maatschappij.\nWaarom digitale transformatie juist nu zo aan de orde is, wordt helder beschreven in het artikel van Pim de Wit in Computable.\nOm goed in te kunnen spelen op de wensen van de klant is het noodzakelijk om alle technologieën met elkaar te gaan verbinden. Om echt digitaal te kunnen transformeren moeten we namelijk overal en altijd kunnen beschikken over de meest recente en relevante data. Deze data is bij de meeste bedrijven versnipperd opgeslagen in losstaande applicaties met onderliggende databases. Misschien zijn er zelfs al Clouddiensten ingezet waarin ook delen van deze data opgeslagen is.\nOm over deze data te kunnen beschikken zal er een soort van integratie moeten gaan plaatsvinden om deze data met elkaar te kunnen verbinden en beschikbaar te stellen aan medewerkers en klanten.\nIntegratie m.b.v. Microsoft Azure Microsoft biedt op het gebied van integratie de volgende mogelijkheden:\nHybride Integratie;\nBizTalk;\nAPI Management;\nService Bus;\nAzure Functions.\nMicrosoft zet de laatste jaren groots in op Azure Logic Apps. Dit is een SaaS-dienst van Microsoft waarmee het mogelijk is om applicaties, verschillende SaaS-diensten en databases met elkaar te verbinden en te integreren. Daarnaast is het mogelijk om workflows in Azure te implementeren.\nMet Logic Apps is het mogelijk om hybride integraties te creëren, te integreren met je on-premises BizTalk omgeving (BizTalk beschikt over een Logic-Apps adapter), Azure Functions, API Management en de Service Bus.\nWat biedt Azure Logic Apps nu eigenlijk:\nDoor Microsoft beheerde connectors: Op dit moment zijn er meer dan honderd connectoren beschikbaar voor Logic Apps.\nOntwerptools: Logic Apps kunnen end-to-end in de browser of met andere Visual Studio-tools worden ontworpen\nSjablonen: Om snel te kunnen starten is er galerie met sjablonen die gebruikt kunnen worden de meer algemenere oplossingen te maken.\nUitbreidbaarheid: Mocht de connector die nodig is niet beschikbaar zijn, dan is het mogelijk om een eigen API-App te maken om te gebruiken als aangepast connector. Daarnaast kan er ook gebruik gemaakt worden van Azure Functions om bepaalde acties uit te voeren.\nOpschaalbaarheid: Met Azure is het mogelijk om op te schalen op momenten dat je meer rekenkracht nodig hebt. Logic Apps is gebaseerd op Azure App Services waarmee dit eenvoudig uit te voeren is.\nConclusie Microsoft Azure biedt op het gebied van (applicatie)integratie een heel scala van mogelijkheden. Het wordt ook steeds makkelijker om je on-premises omgeving en verschillende Clouddiensten (ook niet Microsoft diensten) met elkaar te integreren d.m.v. Logic Apps.\nWil je Logic Apps nu gaan inzetten binnen je organisatie of wil je meer informatie over de mogelijkheden die er zijn op het gebied van integratie? Neem dan even contact met me op. Mijn gegevens staan onderaan dit artikel.\n","permalink":"//localhost:1313/posts/2017-07-04-nl-digitale-transformatie-microsoft-azure-als-verbindende-factor/","summary":"\u003cp\u003eVeel bedrijven maken tegenwoordig een digitale transformatie door. Dit\nmodewoord duikt overal op. Als ik zoek naar een aantal definities van\nwat digitale transformatie nu eigenlijk inhoudt, dan komt het volgende\no.a. naar voren:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDigitale transformatie betreft de impact van (digitale) technologieën\nen de toenemende digitalisering van bedrijfsprocessen op mens, bedrijf\nen zelfs maatschappij.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eOrganisaties die zich richten op de verdigitalisering van het\nlandschap door in te spelen op de (veranderende) wensen en behoeften\nvan klant, werknemers en maatschappij.\u003c/p\u003e","title":"Adapt and thrive with Microsoft Azure for Industries -- part 1"},{"content":"Veel bedrijven maken tegenwoordig een digitale transformatie door. Dit modewoord duikt overal op. Als ik zoek naar een aantal definities van wat digitale transformatie nu eigenlijk inhoudt, dan komt het volgende o.a. naar voren:\nDigitale transformatie betreft de impact van (digitale) technologieën en de toenemende digitalisering van bedrijfsprocessen op mens, bedrijf en zelfs maatschappij.\nOrganisaties die zich richten op de verdigitalisering van het landschap door in te spelen op de (veranderende) wensen en behoeften van klant, werknemers en maatschappij.\nWaarom digitale transformatie juist nu zo aan de orde is, wordt helder beschreven in het artikel van Pim de Wit in Computable.\nOm goed in te kunnen spelen op de wensen van de klant is het noodzakelijk om alle technologieën met elkaar te gaan verbinden. Om echt digitaal te kunnen transformeren moeten we namelijk overal en altijd kunnen beschikken over de meest recente en relevante data. Deze data is bij de meeste bedrijven versnipperd opgeslagen in losstaande applicaties met onderliggende databases. Misschien zijn er zelfs al Clouddiensten ingezet waarin ook delen van deze data opgeslagen is.\nOm over deze data te kunnen beschikken zal er een soort van integratie moeten gaan plaatsvinden om deze data met elkaar te kunnen verbinden en beschikbaar te stellen aan medewerkers en klanten.\nIntegratie m.b.v. Microsoft Azure Microsoft biedt op het gebied van integratie de volgende mogelijkheden:\nHybride Integratie;\nBizTalk;\nAPI Management;\nService Bus;\nAzure Functions.\nMicrosoft zet de laatste jaren groots in op Azure Logic Apps. Dit is een SaaS-dienst van Microsoft waarmee het mogelijk is om applicaties, verschillende SaaS-diensten en databases met elkaar te verbinden en te integreren. Daarnaast is het mogelijk om workflows in Azure te implementeren.\nMet Logic Apps is het mogelijk om hybride integraties te creëren, te integreren met je on-premises BizTalk omgeving (BizTalk beschikt over een Logic-Apps adapter), Azure Functions, API Management en de Service Bus.\nWat biedt Azure Logic Apps nu eigenlijk:\nDoor Microsoft beheerde connectors: Op dit moment zijn er meer dan honderd connectoren beschikbaar voor Logic Apps.\nOntwerptools: Logic Apps kunnen end-to-end in de browser of met andere Visual Studio-tools worden ontworpen\nSjablonen: Om snel te kunnen starten is er galerie met sjablonen die gebruikt kunnen worden de meer algemenere oplossingen te maken.\nUitbreidbaarheid: Mocht de connector die nodig is niet beschikbaar zijn, dan is het mogelijk om een eigen API-App te maken om te gebruiken als aangepast connector. Daarnaast kan er ook gebruik gemaakt worden van Azure Functions om bepaalde acties uit te voeren.\nOpschaalbaarheid: Met Azure is het mogelijk om op te schalen op momenten dat je meer rekenkracht nodig hebt. Logic Apps is gebaseerd op Azure App Services waarmee dit eenvoudig uit te voeren is.\nConclusie Microsoft Azure biedt op het gebied van (applicatie)integratie een heel scala van mogelijkheden. Het wordt ook steeds makkelijker om je on-premises omgeving en verschillende Clouddiensten (ook niet Microsoft diensten) met elkaar te integreren d.m.v. Logic Apps.\nWil je Logic Apps nu gaan inzetten binnen je organisatie of wil je meer informatie over de mogelijkheden die er zijn op het gebied van integratie? Neem dan even contact met me op. Mijn gegevens staan onderaan dit artikel.\n","permalink":"//localhost:1313/posts/2017-07-04-digitale-transformatie-microsoft-azure-als-verbindende-factor/","summary":"\u003cp\u003eVeel bedrijven maken tegenwoordig een digitale transformatie door. Dit\nmodewoord duikt overal op. Als ik zoek naar een aantal definities van\nwat digitale transformatie nu eigenlijk inhoudt, dan komt het volgende\no.a. naar voren:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDigitale transformatie betreft de impact van (digitale) technologieën\nen de toenemende digitalisering van bedrijfsprocessen op mens, bedrijf\nen zelfs maatschappij.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eOrganisaties die zich richten op de verdigitalisering van het\nlandschap door in te spelen op de (veranderende) wensen en behoeften\nvan klant, werknemers en maatschappij.\u003c/p\u003e","title":"NL: Adapt and thrive with Microsoft Azure for Industries -- part 1"},{"content":"In my earlier post: Microsoft Identities: What\u0026rsquo;s new, I briefly talked about the new Application Portal for application registration in Azure. In this post, I want to show how you can register your application in the new portal. No more manually registering your application in Azure Active Directory using the Azure Portal, from the Microsoft Graph site, you can do this in a couple of clicks.\nSo, let\u0026rsquo;s get started.\nNavigate to https://graph.microsoft.com/.\nClick on Quick Start in the main menu.\nInside the Quick Start, you can pick the platform you\u0026rsquo;ll be building for. In this case, we are using ASP.NET MVC. Click the icon in the overview.\nThe page scrolls down after picking the platform and you can get an App Id and a secret. This is the part which registers your application in Azure Active Directory. Click the Get an App Id and Secret button.\nSign in with your Azure credentials and your application is registered. In the next screen, copy your App Secret to the clipboard and click the button to go back.\nIf everything went well, you get a confirmation that the application is registered. The application was given a default name. If you want to change that, click the \u0026lsquo;Manage your app in the Application Registration Portal\u0026rsquo; at the bottom.\nIn there, you can also set the permissions for the application.\nScroll a bit down (in the previous screen), to the Start Coding section, and paste the App secret from the clipboard into the text field. After that click on the Download the SDK based code sample.\nThe Visual Studio solution is downloaded. Unzip the package and open the solution.\nOpen the Web.config and you can see that all the settings for connecting your application to Azure Active Directory are there.\nClick F5 to run the application. Click on the log in link at the right of the screen. Accept the permissions and click the Get Email Address button.\nYour email address is retrieved from Azure Active Directory using the Microsoft Graph and filled in the text field.\nConclusion The new application portal makes it very easy to get started developing. In a few clicks your application is registered and up and running. From there you can start writing your own application code, and you don't have to think or worry about the authentication code.\nIf you want more information, you can refer to the below links:\nhttps://docs.microsoft.com/en-us/azure/active-directory/develop/active-directory-v2-app-registration\nhttps://docs.microsoft.com/en-us/azure/active-directory/develop/active-directory-appmodel-v2-overview\nhttps://docs.microsoft.com/en-us/azure/active-directory/develop/guidedsetups/active-directory-aspnetwebapp\n-Sjoukje\n","permalink":"//localhost:1313/posts/2017-06-20-registering-your-application-in-the-new-application-portal/","summary":"\u003cp\u003eIn my earlier post: \u003ca href=\"https://blogs.msdn.microsoft.com/azuredev/2017/05/24/microsoft-identities-whats-new/\"\u003eMicrosoft Identities: What\u0026rsquo;s\nnew\u003c/a\u003e,\nI briefly talked about the new Application Portal for application\nregistration in Azure. In this post, I want to show how you can register\nyour application in the new portal. No more manually registering your\napplication in Azure Active Directory using the Azure Portal, from the\nMicrosoft Graph site, you can do this in a couple of clicks.\u003c/p\u003e\n\u003cp\u003eSo, let\u0026rsquo;s get started.\u003c/p\u003e\n\u003cp\u003eNavigate to \u003ca href=\"https://graph.microsoft.com/\"\u003ehttps://graph.microsoft.com/\u003c/a\u003e.\u003c/p\u003e","title":"Registering your application in the new Application Portal"},{"content":"At Build conference, a lot of new features were released. And of course, also in the Microsoft Identity space. This article will provide an overview of all the awesome new features that were covered there.\nAzure Active Directory v2.0 Endpoint The v2.0 endpoint allows developer to write apps that accept sign in from both Microsoft Accounts and Azure AD accounts, using one single authentication endpoint.\nWhen your application makes a call to the Microsoft Graph, additional functionality and data will be available for Azure AD users. For instance, data from SharePoint and Microsoft Teams. But for reading a user\u0026rsquo;s mail, or retrieving contacts, the code is exactly the same for both Microsoft and Azure AD accounts.\nFor more information about the v2.0 endpoint, check out the overview. However, there are some limitations to be aware of.\nNew Application Registration Portal To register an app that works with the v2.0 endpoint, you must use the new application registration portal:\nhttps://apps.dev.microsoft.com\nOne Application Middleware In .Net Core 2.0 (preview), there is only one application middleware using different handlers for OpenID Connect, Cookies, etc. In the near future (approximately 1 or 2 weeks), this will be released for mobile applications as well.\nProduction-Ready MSAL Preview At Build 2016 the first developer preview of the Microsoft Authentication Library (MSAL) was released for .NET. One year later, this SDK is now production ready and enhanced with MSAL iOS, MSAL Android and MSAL JavaScript.\nMSAL is the successor of ADAL and works with Azure AD v2. This means, using this SDK, you can also log in using work \u0026amp; school accounts, personal accounts and Azure B2C.\nFor a sample using MSAL, check the following site: https://azure.microsoft.com/nl-nl/resources/samples/active-directory-dotnet-webapp-openidconnect-v2/\nMicrosoft Graph The number of data sets that are available through the Microsoft Graph are now increased:\nThe SharePoint sites API, Planner API and OneNote APIs are now available in the Microsoft Graph v1.0 endpoint.\nThe Microsoft Teams APIs, the New Insights APIs, the refreshed SharePoint list APIs and the Office Reporting APIs are now available in the Microsoft Graph beta endpoint.\nDelta Query Delta query enables applications to discover newly created, updated or deleted data without performing a full read with every request. Delta query is available for Users, Groups, Messages, Mail Folders, Calendar Events, Personal Contact, Contact Folders and Drives.\nFor more information, check the Delta Query overview.\nNew Webhooks Applications can already subscribe to and receive notifications from several resources including messages, events, contacts, group conversations and drive root items.\nNow, new subscriptions to Azure AD Users and Groups and additional support for Outlook.com resources were introduced.\nFor more information on webhooks, check the following site: https://dev.office.com/blogs/guide-to-office-at-build-2017\nCustom Data Extensions You can extend the Microsoft Graph with your custom data. For instance, you can add your favorite color to your user profile data using the Graph. This new feature is now production-ready.\nFor more information, check the following article: https://dev.office.com/blogs/adding-customer-data-to-resources-in-Microsoft-Graph-preview\nBatching Using JSON batching requests, multiple requests to the Microsoft Graph can now be combined in a single HTTP call.\nMore information will be posted in the upcoming weeks at https://developer.microsoft.com/en-us/graph/\nAzure Active Directory B2C Azure Active Directory B2C, makes it easy for customers to sign in to applications using their existing social accounts (now Twitter as well). Besides that, you can also add your own.\nFor Azure AD B2C the following new features are available:\nMSAL (on all available platforms);\nThe ASP.NET middleware (OpenId Connect) works with B2C;\nNew web app templates for Visual Studio.\nCreate custom identity policies.\nWrap Up Lots of awesome new features regarding Microsoft Identities were released at build this year. I\u0026rsquo;ve tried to describe them all in this article but in case I\u0026rsquo;ve forgotten one, please let me know. I will then add them to the above overview.\nHopefully you are just as excited about this new features as I am!\n-Sjoukje\n","permalink":"//localhost:1313/posts/2017-05-23-microsoft-identities/","summary":"\u003cp\u003eAt Build conference, a lot of new features were released. And of course,\nalso in the Microsoft Identity space. This article will provide an\noverview of all the awesome new features that were covered there.\u003c/p\u003e\n\u003ch2 id=\"azure-active-directory-v20-endpoint\"\u003eAzure Active Directory v2.0 Endpoint\u003c/h2\u003e\n\u003cp\u003eThe v2.0 endpoint allows developer to write apps that accept sign in\nfrom both Microsoft Accounts and Azure AD accounts, using one single\nauthentication endpoint.\u003c/p\u003e\n\u003cp\u003eWhen your application makes a call to the Microsoft Graph, additional\nfunctionality and data will be available for Azure AD users. For\ninstance, data from SharePoint and Microsoft Teams. But for reading a\nuser\u0026rsquo;s mail, or retrieving contacts, the code is exactly the same for\nboth Microsoft and Azure AD accounts.\u003c/p\u003e","title":"Microsoft Identities: What's new"},{"content":"Creating (web) applications which use Azure Active Directory for authentication can be quite simple. As a developer, you don\u0026rsquo;t have to know which code is added to your application for authentication. Visual Studio will handle that burden for you. But, what if something goes wrong and you suddenly have to debug your code. Or you\u0026rsquo;re just curious and want to know what happens under the hood.\nIn this post, I\u0026rsquo;m going to add the code for authenticating my MVC application to Azure manually. This will provide more insights in the different parts which are added by Visual Studio using the project templates. Insights you might need in the future when troubleshooting more complex scenarios.\nLet\u0026rsquo;s start with a little bit of background information\u0026hellip;\nOWIN, Katana and OpenId Connect In.Net Framework 3.5, Microsoft introduced WIF (Windows Identity Foundation). This was the first library entirely devoted to claims-based identity development for ASP.Net applications. WIF eventually evolved into .Net framework 4.5, which was reengineered to root all identity representations to one base class, called ClaimsPrincipal.\nHowever, as the WIF classes are still supported today, these classes are not suited for the new protocols used in Azure. WIF is strongly xml based, which makes it hard to extend. That\u0026rsquo;s why OWIN is introduced for implementing modern protocols.\nOWIN stands for Open Web Interface for .Net, a community owned specification for the creation of highly portable HTTP processing components that can be used and reused on any web server, hosting process or OS, as long as the .Net framework is available on the target platform.\nAs OWIN does not provide any classes for implementation, Katana does. \u0026ldquo;Katana\u0026rdquo; is the original code name of the project and is completely open source. The source code for all ASP.NET OWIN components is available under https://github.com/aspnet/AspNetKatana/ .\nOpenId Connect is a simple identity layer built on top of the OAuth 2.0 protocol. OpenId Connect extends OAuth2 with a new token, the ID token, that verifies the identity of the user and provides basic profile information about the user. This ID token comes in the form of a JSON Web Token (JWT Token).\nThis is just a very brief introduction on this topic. If you want more information you can refer to http://owin.org/ , https://docs.microsoft.com/en-us/aspnet/aspnet/overview/owin-and-katana/ or http://openid.net/ .\nCreating the App Begin by creating a new MVC project with no authentication targeting the .NET Framework 4.6.\n{width=\u0026ldquo;4.257048337707786in\u0026rdquo; height=\u0026ldquo;2.8333333333333335in\u0026rdquo;}\nFirst, change the project URL to HTTPS instead of the default HTTP. Go to the Project properties and set SSL Enable to true.\n{width=\u0026ldquo;2.0625in\u0026rdquo; height=\u0026ldquo;2.3153947944007in\u0026rdquo;}\nNotice that the SSL URL is now filled with the following URL: https://localhost:44355/. Copy this to the clipboard.\nIn the Solution Explorer, right click the project and choose properties. Open the Web Tab and replace the original URL with the copied URL.\n{width=\u0026ldquo;4.302083333333333in\u0026rdquo; height=\u0026ldquo;2.64250656167979in\u0026rdquo;}\nPress F5 to check if your application still works.\nInstall the Azure Authentication Packages The first thing to do is to add the required NuGet packages. Open the Package Manager Console, and add the following commands:\nInstall-Package Microsoft.Owin.Host.SystemWeb\nThis package installs the assemblies to host the OWIN middleware pipeline in your application.\nInstall-Package Microsoft.Owin.Security.Cookies\nMost redirect-based web applications request a token for the initial authentication and rely on a cookie-based session for all further interactions. The cookie middleware generates and tracks such a session. This package also brings in the Microsoft.Owin.Security as a dependency, which is a repository of classes that creates the building blocks of security-related middlewares.\nInstall-Package Microsoft.Owin.Security.OpenIdConnect\nThis package contains the OpenId Connect middleware parts. It pulls down the JWT Handler (System.IdentityModel.Protocol.Extensions) and Microsoft.IdentityModel.Protocol.Extensions. These packages are separated from the OWIN package because you can use them if you want to build a stack without OWIN.\nAdd your application to Azure AD. For adding your application to Azure AD, you can refer to the previous article I wrote called: 4 ways of adding your application to Azure Active Directory\nAfter adding your application to Azure Active Directory, copy the Application ID to the clipboard.\nSetting up Azure Authentication Switch back to your application. The next step is to add an OWIN Pipeline in front of the app and initialize the appropriate middleware in the pipeline.\nRight click the project and choose Add new item. Select the OWIN Startup Class and name the class Startup.cs and select Add.\n{width=\u0026ldquo;4.239583333333333in\u0026rdquo; height=\u0026ldquo;2.9571970691163605in\u0026rdquo;}\nEdit the class declaration to include the partial keyword. The result will look like the following:\nusing System;\nusing System.Threading.Tasks;\nusing Microsoft.Owin;\nusing Owin;\n[assembly: OwinStartup(typeof(OWINSample.Startup))]\nnamespace OWINSample\n{\npublic partial class Startup\n{\npublic void Configuration(IAppBuilder app)\n{\n// For more information on how to configure your application, visit https://go.microsoft.com/fwlink/?LinkID=316888\n}\n}\n}\nThe OWINStartup attribute will cause the Configuration method to be invoked at assembly load time. You can use this method to add all your initialization code.\nTo initialize the cookie and the OpenId Connect middleware, add a new class and call it Startup.Auth.cs. Replace the public declaration with partial.\nAdd the following using directives:\nusing Owin;\nusing Microsoft.Owin.Security;\nusing Microsoft.Owin.Security.Cookies;\nusing Microsoft.Owin.Security.OpenIdConnect;\nAdd the following identity-initialization method to the Startup class:\npublic void ConfigureAuth(IAppBuilder app) {\napp.SetDefaultSignInAsAuthenticationType(CookieAuthenticationDefaults.AuthenticationType);\napp.UseCookieAuthentication(new CookieAuthenticationOptions());\napp.UseOpenIdConnectAuthentication(\nnew OpenIdConnectAuthenticationOptions\n{\nClientId = \u0026quot;\u0026lt;Your Azure AD Application Id\u0026gt;\u0026quot;,\nAuthority = \u0026quot;https://login.microsoftonline.com/\u0026lt;your-active-directory-tenant\u0026gt;.onmicrosoft.com/\u0026quot;\n});\n}\nThe first line sets the default sign in authentication to accept cookies. The UseCookieAuthentication adds an instance of the cookie middleware to the pipeline. The UseOpenIdConnectAuthentication does the same for the OpenId Connect middleware. The order is important here. The first middleware you add will be invoked first after a request. The last one will be the one which works on the response.\nThe OpenId Connect middleware allows you to control every aspect of the authentication flow. By adding the OpenIdConnectAuthenticationOptions initialization parameters:\nClientId: This is the Application Id assigned to your app when adding it to Azure Active Directory. You have copied this value to the clipboard (or you retrieve it from the Azure Portal).\nAuthority: This is the complete url of your Azure AD tenant (You can retrieve this from the Azure Portal as well).\nAlmost there. You have to ensure that the above code is called at load time. Open the Startup.cs class again and add the following code:\npublic void Configuration(IAppBuilder app)\n{\nConfigureAuth(app);\n}\nYour application is now configured to use OpenId Connect against your Azure AD tenant for authentication!\nTest your application To test your application, add a trigger for authentication. Open the HomeController.cs and add the following directive:\nusing System.Security.Claims;\nReplace the Contact method with the following:\n[Authorize]\npublic ActionResult Contact()\n{\nstring userfirstName = ClaimsPrincipal.Current.FindFirst(ClaimTypes.GivenName).Value;\nViewBag.Message = String.Format(\u0026quot;Welcome {0}!\u0026quot;, userfirstName);\nreturn View();\n}\nPress F5. The application will open like it did earlier. Click the Contact link in top bar. You should be directed to the Azure AD Authentication Page (caused by the [Authorize]). Fill in your credentials and you should be directed back to the Contact view. If everything went well, the users first name will be displayed in the top bar of the page!\nConclusion In this post, I\u0026rsquo;ve shown what it takes to add the authentication code to your application manually. I gave some brief background information on the different parts what makes up your authentication code. Hopefully, by now, you have just enough information to debug and troubleshoot your code when anything goes wrong in the future.\nYou can download the sample app from:\n","permalink":"//localhost:1313/posts/2017-04-25-azure-admanually-create-authentication-code-part-2/","summary":"\u003cp\u003eCreating (web) applications which use Azure Active Directory for\nauthentication can be quite simple. As a developer, you don\u0026rsquo;t have to\nknow which code is added to your application for authentication. Visual\nStudio will handle that burden for you. But, what if something goes\nwrong and you suddenly have to debug your code. Or you\u0026rsquo;re just curious\nand want to know what happens under the hood.\u003c/p\u003e\n\u003cp\u003eIn this post, I\u0026rsquo;m going to add the code for authenticating my MVC\napplication to Azure manually. This will provide more insights in the\ndifferent parts which are added by Visual Studio using the project\ntemplates. Insights you might need in the future when troubleshooting\nmore complex scenarios.\u003c/p\u003e","title":"Azure AD -- Manually add authentication code to your app Part 2"},{"content":"When you are developing apps for Azure or Office 365, there comes a time that the app needs to be added to Azure Active Directory for authentication. There is written a lot about the manual approach of doing this. But maybe there are other ways to do this, and maybe can we do this even quicker?\nIn this article, I\u0026rsquo;m going to show four different ways of adding your application to Azure AD. I will cover both manually as programmatically and provide some samples on how to do this. Let\u0026rsquo;s start with the easiest one, directly from Visual Studio.\nVisual Studio The easiest option is to register your application from within Visual Studio. Take the following steps to do this.\nOpen Visual Studio and create an new application. Create a ASP.Net Web Application and choose the MVC template in the project creation wizard. Click the Change Authentication button.\nOn the next screen, there are four different options. But if you want to use Azure AD for authentication, you have to choose Work and School Accounts.\nNext you have to provide the tenant information. Select Cloud \u0026ndash; Single Organization, pick the tenant where you want to add your app and select Read Directory Data. Click Ok. And again, click OK.\nThe application is created for you and added to Azure AD with permissions to access Azure AD for authentication purposes and reading user information.\nOpen the web.config and you will see the below Azure AD related entries added to the AppSetttings:\n\u0026lt;appSettings\u0026gt;\n\u0026lt;add key=\u0026quot;ida:ClientId\u0026quot; value=\u0026quot;e0ea5425-740f-4108-b384-c365a48a32fa\u0026quot; /\u0026gt;\n\u0026lt;add key=\u0026quot;ida:AADInstance\u0026quot; value=\u0026quot;https://login.microsoftonline.com/\u0026quot; /\u0026gt;\n\u0026lt;add key=\u0026quot;ida:ClientSecret\u0026quot; value=\u0026quot;--your-client-secret--\u0026quot; /\u0026gt;\n\u0026lt;add key=\u0026quot;ida:Domain\u0026quot; value=\u0026quot;--your-domain.onmicrosoft.com--\u0026quot; /\u0026gt;\n\u0026lt;add key=\u0026quot;ida:TenantId\u0026quot; value=\u0026quot;233f4bcb-abf8-402b-989f-57d6500ed422\u0026quot; /\u0026gt;\n\u0026lt;add key=\u0026quot;ida:PostLogoutRedirectUri\u0026quot; value=\u0026quot;https://localhost:44358/\u0026quot; /\u0026gt;\n\u0026lt;/appSettings\u0026gt;\nBelow a brief explanation of the AppSettings items:\nida:ClientId: The ClientId from an OAuth 2.0 perspective.\nida:AADInstance: The base URL of the authorization server (this is always https://login.windows.net).\nida:ClientSecret: The shared secret of the app.\nida:Domain: The Azure AD reference domain name.\nida:TenantId: The tenant Id, which can be concatenated to the ida:AADInstance.\nida:PostLogoutRedirectUri: The URL to which the browsers will be redirected after logout.\nIf you now press F5, the application is started and you will be asked to fill in your Azure credentials. Accept the App permissions by clicking the Accept button.\nYou can log in to the Azure Portal and check the settings which are created for your App by Visual Studio. In the permission tab, you can also see the permissions you\u0026rsquo;ve granted in the above step. If you want to give your application additional permissions, you can set them in here.\nManually Next is adding the App manually to Azure AD.\nOpen the Azure Portal and navigate to your active directory. We are using the new Azure portal for this. Azure AD is in preview there (if you want to use the \u0026ldquo;Old\u0026rdquo; Azure Portal, you can take a look at the steps provided by Andreas in his post: Azure AD Developer Tips and Tricks \u0026ndash; Part 3). Select \u0026ldquo;More Services\u0026rdquo; at the bottom of the menu and then \u0026ldquo;Azure Active Directory\u0026rdquo;.\nIt will open up the AD of the selected tenant by default. If you have access to multiple tenants and want to select another one, click your profile button at the top right side of the screen. Here you can select a different directory.\nClick App Registrations at the left and then click the Add button.\nThe App registration wizard is opened. Fill in a name for your app. Choose an application type. You have to choose between a Web app /API or a native application. The former defines an application that has a web-based UI. The latter defines a native one, like for instance a tablet or phone application. For now, choose Web app / API.\nFor the Sign-On URL, you can fill in the exact same URL that was filled in automatically by Visual Studio in the previous section. In my case, this is: https://localhost:44358/ . Azure AD will not send tokens to URL\u0026rsquo;s that aren\u0026rsquo;t registered.\nI personally find this setting quite confusing because when you enter complete nonsense in there, the authentication still works. But you can think of this as the URL where the whole sign-in process starts. That will be the app URL. If a user is not signed in, he or she then will be send to login.windows.net. So, after you have deployed your application for production to Azure, you can change this URL to the production URL.\nClick create to add the Application.\nPowerShell Adding your application using PowerShell is very easy and results in only a couple of lines of code:\nLogin-AzureRmAccount\nSelect-AzureRmSubscription -TenantId \u0026quot;your-tenant-id\u0026quot;\nNew-AzureRmADApplication -DisplayName \u0026quot;AppAddedWithPS\u0026quot; -HomePage \u0026quot;https://localhost:44358/\u0026quot; -IdentifierUris \u0026quot;https://your-tenant-url/6958490c-21ae-4885-804c-f03b3add87ad\u0026quot;\nFirst login with your Azure credentials. This is done by calling the Login-AzureRmAccount method. Second, select the Azure AD tenant by calling the Select-AzureRmSubscription and passing it the TenantId (you can find this inside the Azure Portal under Properties:\nThe Select-AzureRmSubscription method is not mandatory. If you don\u0026rsquo;t add this, the default Azure AD tenant will be selected. So, if have only one tenant bound to your subscription, you can remove this from your script.\nThe third line calling the New-AzureRmADApplication created the application inside your Azure AD. Below an explanation of the used parameters:\nDisplayName: The name of your application.\nHomePage: The homepage of your application (I think this is a much better name than Sign-On URL, which is used in the portal.\nIdentifierUris: This is the App Id. Out-of-the box Azure AD uses the tenant URL followed by a Guid.\nWhen you execute this script, the application is created in Azure AD.\nGraph API The last approach is creating the application by using the Graph API. The Graph API is an OData 4.0 compliant REST API that provides programmatic access to directory objects in Azure AD.\nWe do this by leveraging the Graph Explorer.\nOpen your browser and go to https://graphexplorer.azurewebsites.net . You can login using your Azure credentials. One thing to note is that you have to use a school or work account to login.\nWhen navigated to the Graph Explorer set the method to POST and type the following to the query field:\nhttps://graph.windows.net/myorganization/applications/\nin the body field add the following:\n{\n\u0026quot;displayName\u0026quot;: \u0026quot;MyApplication\u0026quot;,\n\u0026quot;homepage\u0026quot;: \u0026quot; https://localhost:44358/\u0026quot;,\n\u0026quot;identifierUris\u0026quot;:\n[\n\u0026quot;https://https://your-tenant-url/6958490c-21ae-4885-804c-f03b3add87ad\u0026quot;\n]\n}\nClick the Go button. Your application is created in Azure AD.\n","permalink":"//localhost:1313/posts/2017-03-24-4-ways-of-adding-your-application-to-azure-active-directory/","summary":"\u003cp\u003eWhen you are developing apps for Azure or Office 365, there comes a time\nthat the app needs to be added to Azure Active Directory for\nauthentication. There is written a lot about the manual approach of\ndoing this. But maybe there are other ways to do this, and maybe can we\ndo this even quicker?\u003c/p\u003e\n\u003cp\u003eIn this article, I\u0026rsquo;m going to show four different ways of adding your\napplication to Azure AD. I will cover both manually as programmatically\nand provide some samples on how to do this. Let\u0026rsquo;s start with the easiest\none, directly from Visual Studio.\u003c/p\u003e","title":"4 ways of adding your application to Azure Active Directory"},{"content":"Visual Studio generates a lot of code for you, when you use the ASP.Net project templates. The code required for authentication your application to Azure AD, is added automatically for you when you run the ASP.NET Web Application wizard creating a new project. As a developer, you don\u0026rsquo;t have to know what happens under de hood. You can just fill in your Azure subscription details, generate your project and press F5\u0026hellip; That\u0026rsquo;s all.\nBut what is actually added to this project by Visual Studio? Which classed and code do you actually need to authenticate your application? And how does this all work?\nIf you want to know more, read on!\nLet\u0026rsquo;s start with creating a ASP.NET application, and ass this logic to the application ourselves.\nOpen Visual Studio and create a new Web Application. Give it a name and click OK. Then select MVC and leave the default or select Change Authentication and make sure it is set to No Authentication.\nClick OK and your project is created.\n","permalink":"//localhost:1313/posts/2017-03-24-azure-ad-manually-create-authentication-code-part-1/","summary":"\u003cp\u003eVisual Studio generates a lot of code for you, when you use the ASP.Net\nproject templates. The code required for authentication your application\nto Azure AD, is added automatically for you when you run the ASP.NET Web\nApplication wizard creating a new project. As a developer, you don\u0026rsquo;t\nhave to know what happens under de hood. You can just fill in your Azure\nsubscription details, generate your project and press F5\u0026hellip; That\u0026rsquo;s all.\u003c/p\u003e","title":"Azure AD: Manually Create Authentication Code Part 1"},{"content":"In the previous post on AMS I\u0026rsquo;ve shown how to create a new media channel, and wrote some simple code for uploading a video to AMS. In this post, I want to elaborate a bit more on the uploading of video files.\nSome Background information A best practice for uploading video files to AMS is, that each asset contains a unique instance of media content. Each asset should not contain multiple edits of a file, in order to reduce difficulties submitting encoding jobs and streaming and securing the delivery of the asset later in the workflow.\nFor instance, the trailer of your movie is stored in a different asset than the feature-length movie. In this case the trailer can have a wide viewership, but the viewing of actual movie can be restricted.\nAMS REST API Azure Media Services provides an OData REST service which built on OData v3. By using v3, you can submit HTTP request bodies in atom-pub or JSON, and you can receive the response in the same format. The Media SDK for .Net, which I am using in my samples, is a wrapper around the REST API.\nFor more information about using the REST API for uploading video assets see:\nIngesting Assets with the Media Services REST API\nIngesting Assets in Bulk with the REST API\nAMS supported video input formats The AMS SDK supports various video, image and audio file types which can be uploaded to the AMS portal. However, the AMS portal only allows formats being uploaded that are supported by the Azure Media Encoder.\nContent encoded with the following formats are allowed to be imported:\nH.264 (Baseline, Main, and High Profiles)\nMPEG-1\nMPEG-2 (Simple and Main Profile)\nMPEG-4 v2 (Simple Visual Profile and Advanced Simple Profile)\nVC-1 (Simple, Main, and Advanced Profiles)\nWindows Media Video (Simple, Main, and Advanced Profiles)\nDV (DVC, DVHD, DVSD, DVSL)\nThe following video file formats are supported for input:\n3GPP, 3GPP2 (.3gp, .3g2, .3gp2)\nAdvanced Systems Format (ASF) (.asf)\nAdvanced Video Coding High Definition (AVCHD) (.mts, .m2tf)\nAudio-Video Interleaved (AVI) (.avi)\nDigital camcorder MPEG-2 (MOD) (.mod)\nDigital video (DV) camera file (.dv)\nDVD transport stream (TS) file (.ts)\nDVD video object (VOB) file (.vob)\nExpression Encoder Screen Capture Codec file (.xesc)\nMP4 (.mp4)\nMPEG-1 System Stream (.mpeg, .mpg)\nMPEG-2 video file (.m2v)\nSmooth Streaming File Format (PIFF 1.3) (.ismv)\nWindows Media Video (WMV) (.wmv)\nFor more information on supported input formats and codecs, see: \u0026lsquo;Media Encoder Standard Formats and Codecs\u0026rsquo;\nSecuring your video when uploading to AMS AMS provides the ability to secure your video when it\u0026rsquo;s being uploaded to the AMS portal. All video files are associated with an Asset object in the AMS SDK. When you create a new Asset object for your video, you must specify an Encryption option as parameter. Each file added to the Asset will then use the Encryption option you specified.\nThe AssetCreationOptions enumeration comes in four different flavors:\nAssetCreationOptions.None\nAssetCreationOptions.StorageEncrypted\nAssetCreationOptions.CommonEncryptionProtected\nAssetCreationOptions.EnvelopeEncryptionProtected\nFor more information about protecting your content, see \u0026lsquo;AMS Protecting content overview\u0026rsquo;.\nSo, enough background information for now. Let\u0026rsquo;s do some coding!!\nSetting up your Application For more information about how to set up an AMS account and retrieving your account key, as well as some more information on setting up the Visual Studio solution, refer to my previous post. For this sample, I will use the same account and key which are used in the previous post.\nIn this sample, we are going to use a more professional approach. Where in the previous sample, the video files where only uploaded, this time we will encode the video file, add a policy, a locator and secure the upload.\nSo, the following tasks are performed by the sample code:\nCreate an empty Asset\nCreate an AssetFile instance which is associated to the Asset\nCreate an AccesPolicy that defines the access permissions and the duration of the Asset\nCreate a Locator instance that provides access to the Asset. The Locator provides an access point by creating Url. This way the account key does not have to be shared, the files can be accessed using a custom Url.\nOpen Visual Studio and create a new C# console application. Install the windowsazure.mediaservices.extensions NuGet Package, add a reference to the System.Configuration assembly and add the appropriate App Settings to the App.config.\nOpen the program.cs file and add the following using statements to it:\nusing System.Configuration;\nusing System.Threading;\nusing System.IO;\nusing Microsoft.WindowsAzure.MediaServices.Client;\ninside the class, add the following code:\nclass Program\n{\n// Read values from the App.config file.\nprivate static readonly string _mediaServicesAccountName =\nConfigurationManager.AppSettings[\u0026quot;MediaServicesAccountName\u0026quot;];\nprivate static readonly string _mediaServicesAccountKey =\nConfigurationManager.AppSettings[\u0026quot;MediaServicesAccountKey\u0026quot;];\nprivate static CloudMediaContext _context = null;\nprivate static MediaServicesCredentials _cachedCredentials = null;\nstatic void Main(string[] args)\n{\n_cachedCredentials = new MediaServicesCredentials(_mediaServicesAccountName,\n_mediaServicesAccountKey);\n_context = new CloudMediaContext(_cachedCredentials);\nIAsset inputAsset = CreateAssetAndUploadSingleFile(@\u0026quot;C:\\SampleVideo.mp4\u0026quot;, AssetCreationOptions.StorageEncrypted);\nEncodeToAdaptiveBitrateMP4Set(inputAsset);\n}\nstatic public IAsset CreateAssetAndUploadSingleFile(string singleFilePath, AssetCreationOptions assetCreationOptions)\n{\nif (!File.Exists(singleFilePath))\n{\nConsole.WriteLine(\u0026quot;File does not exist.\u0026quot;);\nreturn null;\n}\nvar assetName = Path.GetFileNameWithoutExtension(singleFilePath);\n//create a new input asset\nIAsset inputAsset = _context.Assets.Create(assetName, assetCreationOptions);\nvar assetFile = inputAsset.AssetFiles.Create(Path.GetFileName(singleFilePath));\nConsole.WriteLine(\u0026quot;Created assetFile {0}\u0026quot;, assetFile.Name);\n//create a 30-day read and list access policy\nvar policy = _context.AccessPolicies.Create(\nassetName,\nTimeSpan.FromDays(30),\nAccessPermissions.Read | AccessPermissions.List);\n//create a SAS locator to the asset\nvar locator = _context.Locators.CreateLocator(LocatorType.Sas, inputAsset, policy);\nConsole.WriteLine(\u0026quot;Upload {0}\u0026quot;, assetFile.Name);\nassetFile.Upload(singleFilePath);\nConsole.WriteLine(\u0026quot;Done uploading {0}\u0026quot;, assetFile.Name);\nlocator.Delete();\npolicy.Delete();\nreturn inputAsset;\n}\nstatic public IAsset EncodeToAdaptiveBitrateMP4Set(IAsset asset)\n{\n// Declare a new job.\nIJob job = _context.Jobs.Create(\u0026quot;Media Encoder Standard Job\u0026quot;);\n// Get a media processor reference, and pass to it the name of the\n// processor to use for the specific task.\nIMediaProcessor processor = GetLatestMediaProcessorByName(\u0026quot;Media Encoder Standard\u0026quot;);\n// Create a task with the encoding details, using a string preset.\n// In this case \u0026quot;H264 Multiple Bitrate 720p\u0026quot; preset is used.\nITask task = job.Tasks.AddNew(\u0026quot;My encoding task\u0026quot;,\nprocessor,\n\u0026quot;H264 Multiple Bitrate 720p\u0026quot;,\nTaskOptions.None);\n// Specify the input asset to be encoded.\ntask.InputAssets.Add(asset);\n// Add an output asset to contain the results of the job.\n// This output is specified as AssetCreationOptions.None, which\n// means the output asset is not encrypted.\ntask.OutputAssets.AddNew(\u0026quot;Output asset\u0026quot;,\nAssetCreationOptions.None);\njob.StateChanged += new EventHandler\u0026lt;JobStateChangedEventArgs\u0026gt;(JobStateChanged);\njob.Submit();\njob.GetExecutionProgressTask(CancellationToken.None).Wait();\nreturn job.OutputMediaAssets[0];\n}\nprivate static void JobStateChanged(object sender, JobStateChangedEventArgs e)\n{\n//Eventhandler for monitoring the job state\nConsole.WriteLine(\u0026quot;Job state changed event:\u0026quot;);\nConsole.WriteLine(\u0026quot; Previous state: \u0026quot; + e.PreviousState);\nConsole.WriteLine(\u0026quot; Current state: \u0026quot; + e.CurrentState);\nswitch (e.CurrentState)\n{\ncase JobState.Finished:\nConsole.WriteLine();\nConsole.WriteLine(\u0026quot;Job is finished. Please wait while local tasks or downloads complete...\u0026quot;);\nbreak;\ncase JobState.Canceling:\ncase JobState.Queued:\ncase JobState.Scheduled:\ncase JobState.Processing:\nConsole.WriteLine(\u0026quot;Please wait...\\n\u0026quot;);\nbreak;\ncase JobState.Canceled:\ncase JobState.Error:\n// Cast sender as a job.\nIJob job = (IJob)sender;\n// Display or log error details as needed.\nbreak;\ndefault:\nbreak;\n}\n}\nprivate static IMediaProcessor GetLatestMediaProcessorByName(string mediaProcessorName)\n{\nvar processor = _context.MediaProcessors.Where(p =\u0026gt; p.Name == mediaProcessorName).\nToList().OrderBy(p =\u0026gt; new Version(p.Version)).LastOrDefault();\nif (processor == null)\nthrow new ArgumentException(string.Format(\u0026quot;Unknown media processor\u0026quot;, mediaProcessorName));\nreturn processor;\n}\n}\nBy executing this code, the video will be uploaded to the AMS account, a 30-day access policy is created for the file, a locator Url is created on which the video can be displayed and the video is encoded using a Media Encoder.\nWhen you navigate to the AMS portal, you can see that there are two assets created.\nClick the \u0026lsquo;Output Asset\u0026rsquo;, scroll down and you will see that the original file is encoded and that there are multiple files now. From here, you can publish the asset.\nSummary This concludes this series of posts about Azure Media Services. Below is a list with reference materials used for this article. They provide a good starting point for building your own Azure Media Services solutions:\nBuilding an On-Demand Video Service with Microsoft Azure Media Services\nAzure Media Services overview and common scenarios\n","permalink":"//localhost:1313/posts/2017-02-24-azure-media-services-uploading-and-encoding-your-video-files/","summary":"\u003cp\u003eIn \u003ca href=\"https://blogs.msdn.microsoft.com/azuredev/2017/02/01/a-first-look-at-azure-media-services/\"\u003ethe previous\npost\u003c/a\u003e\non AMS I\u0026rsquo;ve shown how to create a new media channel, and wrote some\nsimple code for uploading a video to AMS. In this post, I want to\nelaborate a bit more on the uploading of video files.\u003c/p\u003e\n\u003ch2 id=\"some-background-information\"\u003eSome Background information\u003c/h2\u003e\n\u003cp\u003eA best practice for uploading video files to AMS is, that each asset\ncontains a unique instance of media content. Each asset should not\ncontain multiple edits of a file, in order to reduce difficulties\nsubmitting encoding jobs and streaming and securing the delivery of the\nasset later in the workflow.\u003c/p\u003e","title":"Azure Media Services: Uploading and encoding your video files"},{"content":"Who are you, and what do you do?\nMy name is Sjoukje Zaal and I live in The Netherlands. I work as a Principal Architect \u0026amp; Lead Productivity for Ordina, an IT company in the Benelux (Belgium, The Netherlands, and Luxemburg). Sixteen years ago, I started my career as a web designer. After that I worked as a software developer for several years and for the last 6 years I work as an Architect.\nIn my daily job, I work with customers on Azure, Office 365 and Microsoft integration solutions. Besides that, I spend a lot of time on participating in the Microsoft community.\nHow do you contribute to the community?\nI am a co-founder for SP\u0026amp;C NL, a Dutch community website with articles about Azure, Office 365 and SharePoint. I \u0026rsquo;m also very active in the MSDN community, moderating the Azure, BizTalk, SharePoint and Office 365 developer forums, answering questions and writing Wiki articles and blog articles for these topics. Besides that, I am also involved in the Azure Advisors Yammer group.\nHow long have you been answering questions on MSDN Forums? What have you learned from that experience?\nI started answering questions back in 2008, only for SharePoint. I took a break for a couple of years when I was focusing on other technologies, like Azure and BizTalk. The last couple of years I am more active again. Answering questions gives me the ability to become much better at these technologies, learning new things and share my learnings and experience with others.\nWhy do you contribute to TechNet Wiki articles? What kind of articles have you written, and why?\nThe TechNet Wiki articles have helped me a lot in my career with troubleshooting and solving issues. As I became more experienced I wanted to give something back to the community. When a question is asked multiple times in the MSDN forum, I find the solution, write it down and create a Wiki article about it. This way it can help a lot of people solving issues and gaining new insights. So far, my articles are mainly about Office 365, Azure Logic Apps and BizTalk.\nTell me about your involvement with Azure Advisors. How do you give feedback and impact Azure tools and services?\nMy involvement mainly focusses on reading and answering questions that are asked by Azure customers inside the group. Microsoft employees are also part of this group, so you can join Skype meetings as well and stay updated on news, issues and updates.\nWho has impressed you in the Azure development community, and why?\nI don\u0026rsquo;t have a particular person but I\u0026rsquo;m impressed by anyone who takes the time and effort to make contributions to the community. It helped me throughout my career and will help a lot of others in the future.\nDo you have any tips for new community members?\nYou learn as you go along\u0026hellip;\n","permalink":"//localhost:1313/posts/2017-02-08-azure-dev-interview/","summary":"\u003cp\u003e\u003cstrong\u003eWho are you, and what do you do?\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eMy name is Sjoukje Zaal and I live in The Netherlands. I work as a\nPrincipal Architect \u0026amp; Lead Productivity for Ordina, an IT company in the\nBenelux (Belgium, The Netherlands, and Luxemburg). Sixteen years ago, I\nstarted my career as a web designer. After that I worked as a software\ndeveloper for several years and for the last 6 years I work as an\nArchitect.\u003c/p\u003e","title":"Azure Dev Interview"},{"content":"Back in the early (Azure) days, I did a project in which we used the Microsoft Expression Media Encoder SDK for encoding video's from inside a Azure Worker Role. We stored those video's in Azure Blob Storage and used IIS Smooth Streaming \u0026amp; Silverlight for playing those video's.\nBut now there is Azure Media Services, and this will make it all a lot easier and better! Let's take a look at what Azure Media Services has to offer from a developer perspective.\nIntroduction Microsoft Azure Media Services (AMS) allows you tot build end to end media distribition solutions that can upload, encode, package, and stream media to multiple platforms and devices.\nBelow features are included:\nStudio-grade encoding at cloud scale.\nOne player for all your playback needs.\nGreater discoverability and accessibility of media through media intelligence.\nContent protection and encryption delivered by AES and PlayReady.\nOn-demand and live video streaming with integrated Content Delivery Network capabilities.\nBefore you can get start developing your first AMS solution you need the following:\nA Media Services account in an Azure subscription.\nVisual Studio 2015 / 2013 / 2012.\n.Net Frameword 4.5.\nCreate a Media Services Account A Media Services account gives you access to a set of cloud-based services available in Azure. It does'nt stores the actual media content, only the media metadata and media processing jobs. Take the following steps to create a new Media Services account.\nNavigate to the Azure Portal and login.\nClick Web + Mobile -\u0026gt; Media Services.\n{width=\u0026ldquo;4.59375in\u0026rdquo; height=\u0026ldquo;5.0in\u0026rdquo;}\nEnter an accountname, select a Resourcegroup or create a new one and select a storage account or create a new one. Click OK. {width=\u0026ldquo;1.8333333333333333in\u0026rdquo; height=\u0026ldquo;5.0in\u0026rdquo;}\nYour AMS account is being created now!\nAfter the account creation navigate to the new Media Service. A Default endpoint is created for you, which is in the stopped state. Turn this on to enable streaming, by clicking the Start button.\nFor your application, you need the Account key to connect to your Media Services account. Go to Account Keys and copy the Primary Key to your text-editor.\nCreate your Visual Studio Project The AMS SDK can be installed using NuGet. So, open Visual Studio and create a new C# Console Application.\nWhen the project is created, click Tools -\u0026gt; NuGet Package Manager -\u0026gt; Manage NuGet Packages for Solution.\nIn the search box, type windowsazure.mediaservices.extensions, select the package and click install.\nAdd a reference to the System.Configuration assembly to your project. This assembly is used to add the Media Services account name and the account key (which you\u0026rsquo;ve copied earlier) to the App.config.\nAdd the below code to the App.Config file:\n\u0026lt;appSettings\u0026gt;\n\u0026lt;add key=\u0026quot;MediaServicesAccountName\u0026quot; value=\u0026quot;Media-Services-Account-Name\u0026quot; /\u0026gt;\n\u0026lt;add key=\u0026quot;MediaServicesAccountKey\u0026quot; value=\u0026quot;Media-Services-Account-Key\u0026quot; /\u0026gt;\n\u0026lt;/appSettings\u0026gt;\nReplace the values with your Account Name and the Account Key.\nAdd the following using statements to the program.cs file:\nusing System.Configuration;\nusing System.Threading;\nusing System.IO;\nusing Microsoft.WindowsAzure.MediaServices.Client;\nInside your class, add the following code:\nclass Program\n{\n// Read values from the App.config file.\nprivate static readonly string _mediaServicesAccountName =\nConfigurationManager.AppSettings[\u0026quot;MediaServicesAccountName\u0026quot;];\nprivate static readonly string _mediaServicesAccountKey =\nConfigurationManager.AppSettings[\u0026quot;MediaServicesAccountKey\u0026quot;];\nprivate static CloudMediaContext _context = null;\nprivate static MediaServicesCredentials _cachedCredentials = null;\nstatic void Main(string[] args)\n{\ntry\n{\n_cachedCredentials = new MediaServicesCredentials(\n_mediaServicesAccountName,\n_mediaServicesAccountKey);\n_context = new CloudMediaContext(_cachedCredentials);\nIAsset inputAsset =\nUploadFile(@\u0026quot;C:\\SampleVideo.mp4\u0026quot;, AssetCreationOptions.None);\n}\ncatch (Exception exception)\n{\nexception = MediaServicesExceptionParser.Parse(exception);\nConsole.Error.WriteLine(exception.Message);\n}\nfinally\n{\nConsole.ReadLine();\n}\n}\nstatic public IAsset UploadFile(string fileName, AssetCreationOptions options)\n{\nIAsset inputAsset = _context.Assets.CreateFromFile(\nfileName,\noptions,\n(af, p) =\u0026gt;\n{\nConsole.WriteLine(\u0026quot;Uploading '{0}' - Progress: {1:0.##}%\u0026quot;, af.Name, p.Progress);\n});\nConsole.WriteLine(\u0026quot;Asset {0} created.\u0026quot;, inputAsset.Id);\nreturn inputAsset;\n}\nWhen you run this code, the video gets uploaded to the storage account of your Media Services. If you open the Media Services account in the Azure Portal you can see that it\u0026rsquo;s there.\nConclusion\nIn this post we\u0026rsquo;ve created a Azure Media Service Account and created a simple application for uploading a video into AMS. It works smooth and fast in my opinion. In upcoming posts I will explore some more AMS development functionalities.\n","permalink":"//localhost:1313/posts/2017-01-13-a-first-look-at-azure-media-services/","summary":"\u003cp\u003eBack in the early (Azure) days, I did a project in which we used the\nMicrosoft Expression Media Encoder SDK for encoding video's from inside\na Azure Worker Role. We stored those video's in Azure Blob Storage and\nused IIS Smooth Streaming \u0026amp; Silverlight for playing those video's.\u003c/p\u003e\n\u003cp\u003eBut now there is Azure Media Services, and this will make it all a lot\neasier and better! Let's take a look at what Azure Media Services has\nto offer from a developer perspective.\u003c/p\u003e","title":"A first look at Azure Media Services"},{"content":"In this wiki article I give an overview on how to set up your development environment for Office 365.\nOffice 365 Developer Tenant Before you can start coding, you need an Office 365 developer tenant. You could sign up for a 1-year free developer subscription on: http://dev.office.com . Navigate to the website, follow the instructions and sign up. You will receive a welcome email with a link to sign up for an Office 365 development tenant. First Release will be enabled already for this tenant.\nYou could also create an Office 365 developer tenant using your MSDN subscription (Premium and Ultimate).\nThe Office 365 development tenant is for developing and testing purposes only.\nVisual Studio For a development environment, you can choose between Visual Studio Code or Visual Studio 2015. Visual Studio Code is the free open source tool Microsoft developed. Check the download section for downloading VSCode.\nFor most development tasks VSCode will be suitable. however, if you want to use the Office Development tools for Visual Studio, you need to install Visual Studio 2015 with update 2. The Office Development tools will provide some project templates that give you a head start on developing SharePoint Add-ins or Office Add-ins. Besides Office 365 development, Visual Studio will provide project templates targeting the old on premise development model as well (SharePoint and VSTO solutions). So, you should focus on the templates displayed in the image below:\n{width=\u0026ldquo;6.5in\u0026rdquo; height=\u0026ldquo;4.489583333333333in\u0026rdquo;}\nIn VSCode you have to write almost everything from scratch. There are some helpful open source tools, but it will not give you the same development experience that Visual Studio 2015 will give.\nOffice 365 Developer Patterns \u0026amp; Practices Tools Besides the Microsoft tools there is a community project called Office 365 Developer Patterns \u0026amp; Practices (PnP). The focus of PnP is to provide training, guiding, articles and solutions to support the community.\nOne of the key elements PnP is offering is a framework which makes it easy to develop Office 365 solutions. This is an open source library on GitHub called SharePoint PnP Core . You can include this library in all your Visual Studio projects using a NuGet package. After installing this NuGet package you can call common CSOM/REST operations using Extension Methods.\nThe PnP Core team also provides the PnP Remote Provisioning Engine. With this engine you can provision SharePoint on premise and SharePoint Online artifact using SharePoint CSOM instead of the \u0026lsquo;old\u0026rsquo; SharePoint feature framework.\nAnother solution which is provided by the PnP Core Team is the PnP Powershell Cmdlets. This solution contains a library of PowerShell commands that allows you to perform provisioning and management actions towards SharePoint on premise and SharePoint Online.\nYou can download a .msi setup package from GitHub to install the PnP Powershell Cmdlets. If your operating system is Windows 10 or Windows Server 2016 (or have PowerShell 3.0 installed), you can install the Cmdlets from within your PowerShell Command line.\nSharePoint Online:\nInstall-Module SharePointPnPPowerShellOnline -AllowClobber\nSharePoint 2016\nInstall-Module SharePointPnPPowerShell2016 -AllowClobber\nSharePoint 2013\nInstall-Module SharePointPnPPowerShell2013 -AllowClobber\nSharePoint Framework On May 4, 2016 Microsoft Announced the SharePoint Framework, which allows you to develop client side web parts and applications using JavaScript.\nFor development using the SharePoint Framework, you can use any kind of code editor as long as you can use it to write JavaScript, TypeScript and Node.js.\nTo install the latest version of the Node.js runtime, navigate to the Node.js website. After installing, update the NPM package manager to the latest version using the node.js command line. Type the following command:\nnpm install -g npm\nWrap-up This concludes setting up your development environment for Office 365. Below is a list for further reading. You should definitely check these out. There is a lot more information on PnP and the SharePoint Framework there.\nFurther Reading https://dev.office.com/sharepoint/docs/spfx/set-up-your-developer-tenant\nSharePoint / Office 365 Dev Patterns \u0026amp; Practices (PnP)\nSharePoint PnP Core\nPnP Provisioning Framework\nPnP Provisioning Engine\nIntroduction to Office 365 Dev PnP Provisioning Engine\nWhat you should learn to prepare for developing solutions on the SharePoint Framework\nDownloads Visual Studio Code\nOffice Development Tools for Visual Studio 2015\nSharePoint PnP Core\nPnP Remote Provisioning Engine\nPnP Powershell Cmdlets\nNode.js\n","permalink":"//localhost:1313/posts/2016-12-13-this-is-the-first-article-in-a-series-on-office-365-development/","summary":"\u003cp\u003eIn this wiki article I give an overview on how to set up your\ndevelopment environment for Office 365.\u003c/p\u003e\n\u003ch2 id=\"office-365-developer-tenant\"\u003eOffice 365 Developer Tenant\u003c/h2\u003e\n\u003cp\u003eBefore you can start coding, you need an Office 365 developer tenant.\nYou could sign up for a 1-year free developer subscription on:\n\u003ca href=\"http://dev.office.com\"\u003ehttp://dev.office.com\u003c/a\u003e . Navigate to the website, follow the\ninstructions and sign up. You will receive a welcome email with a link\nto sign up for an Office 365 development tenant. First Release will be\nenabled already for this tenant.\u003c/p\u003e","title":"Office 365: Setting up your development environment"},{"content":"Office included in Office 365 ProPlus, can be deployed to your users in two different ways. First is, by using the user driven approach and second, the IT driven approach. This article explains how to use both ways to deploy Office to your users..\nUser Driven Approach\nThe first approach is user driven, the end user can login to the Office 365 portals and install Office from the portal. As a user I can choose which version of Office I want to deploy. As an administrator, there are a couple of settings in the portal which you can set to restrict the installation process for your users. You can exclude Office and Skype for Business and SharePoint designer (for some versions of Office 365 Visio and Project are added to this list as well). But that\u0026rsquo;s all. If you want complete control over the installation process, you have to use the IT driven approach.\n{width=\u0026ldquo;3.9375in\u0026rdquo; height=\u0026ldquo;4.145764435695538in\u0026rdquo;}\nIT Driven Approach\nWhen using the IT driven approach, the installation material is installed on a file server. Users can go to this file server and install Office from there. For this type of installation, the Office Deployment tool for Click-to-Run is used. There are two different versions of the Office Deployment Tool available:\nOffice Deployment Tool (Office 2013 version)\nOffice Deployment Tool (Office 2016 version)\nDownload the tool and save it to the file server from where Office is installed. The tool consists of two files. Double click the setup file, accept the EULA, click Continue and browse to the file share. The files now will be extracted to the folder.\nThe file share will now have 2 files, a setup file and a configuration file. The setup will run the Click-to-Run setup for us and will be using the information that you add to the configuration file. In the configuration file you can have complete control over the installation. In this file you can control the following:\nDetermine what Office programs are installed\nWhere Office 365 ProPlus is installed from\nHow and when Office 365 is updated\nDetermine which version of Office is installed\nChoose which languages are available for installation\nThe configuration file which is added to the file share is a sample file. To create your own configuration file, you can use the Deployment Tool Configuration XML Editor(https://officedev.github.io/Office-IT-Pro-Deployment-Scripts/XmlEditor.html ). This is a Github project, which basically creates the configuration file for you, depending on the values you add to the editor.\n{width=\u0026ldquo;6.291666666666667in\u0026rdquo; height=\u0026ldquo;3.2083333333333335in\u0026rdquo;}\nThe setup.exe file has two capabilities:\n/Download: this allows you to download any Office version from the Microsoft Content Delivery Network.\n/Configure: allows you to determine the installation settings.\n/Download When using the /Download method, the Office version is downloaded from the CDN. The Office setup is downloaded into the same folder from where you run the setup file. You add your custom configuration file tot his folder as well (you can also add multiple config files to the folder, this way you can split up the configuration file and separate the downloading part from the configuration part).\nBelow you see a sample configuration file:\n\u0026lt;Configuration\u0026gt;\n\u0026lt;Add OfficeClientEdition=\u0026quot;32\u0026quot; SourcePath=\u0026quot;\\\\FileShare\\Folder\u0026quot; Version=\u0026ldquo;15.0.4420.1017\u0026rdquo; Channel=\u0026quot;Current\u0026quot;\u0026gt;\n\u0026lt;Product ID=\u0026quot;VisioProRetail\u0026quot;\u0026gt;\n\u0026lt;Language ID=\u0026quot;en-us\u0026quot; /\u0026gt;\n\u0026lt;/Product\u0026gt;\n\u0026lt;Product ID=\u0026quot;O365ProPlusRetail\u0026quot;\u0026gt;\n\u0026lt;Language ID=\u0026quot;en-us\u0026quot; /\u0026gt;\n\u0026lt;/Product\u0026gt;\n\u0026lt;/Add\u0026gt;\n\u0026lt;Updates Enabled=\u0026quot;TRUE\u0026quot; /\u0026gt;\n\u0026lt;Display AcceptEULA=\u0026quot;TRUE\u0026quot; Level=\u0026quot;None\u0026quot; /\u0026gt;\n\u0026lt;Logging Level=\u0026quot;Standard\u0026quot; Path=\u0026quot;%temp%\u0026quot; /\u0026gt;\n\u0026lt;!--Silent install of 32-Bit Office 365 ProPlus with Visio with Updates and Logging enabled--\u0026gt;\n\u0026lt;/Configuration\u0026gt;\nOpen Command Prompt and navigate to the installation folder. Type: \u0026ldquo;Setup.exe /Download \u0026lt;Name_ConfigFile\u0026gt;.xml\u0026rdquo;. The Office version you specified in the configuration file, is now downloaded to the installation folder.\n/Configure Once downloaded, Office can be installed. This is handled by the /configure parameter.\nAgain, open Command Prompt and navigate to the installation folder. Type: \u0026ldquo;Setup.exe /Configure \u0026lt;Name_ConfigFile\u0026gt;.xml\u0026rdquo;. Office will now be installed.\nSummary We now download and installed Office manually. You can also automate this process using an installer file or you use System Center for automated deployment.\n","permalink":"//localhost:1313/posts/2016-11-08-2-ways-to-install-office-for-office-365-users/","summary":"\u003cp\u003eOffice included in Office 365 ProPlus, can be deployed to your users in\ntwo different ways. First is, by using the user driven approach and\nsecond, the IT driven approach. This article explains how to use both\nways to deploy Office to your users..\u003c/p\u003e\n\u003cp\u003eUser Driven Approach\u003cbr\u003e\nThe first approach is user driven, the end user can login to the Office\n365 portals and install Office from the portal. As a user I can choose\nwhich version of Office I want to deploy. As an administrator, there are\na couple of settings in the portal which you can set to restrict the\ninstallation process for your users. You can exclude Office and Skype\nfor Business and SharePoint designer (for some versions of Office 365\nVisio and Project are added to this list as well). But that\u0026rsquo;s all. If\nyou want complete control over the installation process, you have to use\nthe IT driven approach.\u003c/p\u003e","title":"2 ways to install Office for Office 365 Users"},{"content":"Office 365 provides the ability to add your own domain. When setting up Office 365 Microsoft gives you a onmicrosoft.com domain out-of-the-box. If you want to use your own domain, and for most companies this is preferred, you can add your own domain using the setup wizard in the Office 365 admin center. This will provide you the ability to use your own email in Office 365 or login using to Office 365 and Skype for Business, using your own domain.\nOpen the Office 365 portal and go to the admin center. From the admin center, click \u0026lsquo;Go to Setup\u0026rsquo; to open the setup wizard.\nIn the next screen, check the \u0026lsquo;I already own a domain, and click Next.\nNow you have to prove that you own the domain. In order to do that, login to your domain registar and add the record which is generated by Office 365 in the next screen.\nIf you not already own a domain but need to buy one, return to the admin center, choose Domains, choose buy new domain and follow the steps.\nAfter that, click Verify.\nOffice 365 will connect to the domain registar and validate if the text record is there. If all went well, you go to the next step, which is adding users.\nFor now, click Next. You can add users from the admin center later.\nIn the next screen, you can install apps. Skip this step, you can do this later as well. Click Next.\nThe last step, gives you the ability to migrate your email messages. For now, click don\u0026rsquo;t migrate email messages and again, click Next.\nFor setting up DNS, there are two possibilities. First is Setup my online services for me, which Microsoft Recommends. If you already have an existing website and you only want to migrate email to Office 365, don\u0026rsquo;t choose this setting. If you do (and I accidently did\u0026hellip;), your website becomes inaccessible. So, if you have already have an existing website, choose the second one, I\u0026rsquo;ll manage my own DNS records and click Next.\nIn the next section, Office 365 will provide you with the records which need to be added to your DNS. Login to your domain registar and add the records. You can click verify after you\u0026rsquo;ve added them all or after adding a few of them and Office 365 will check if they are valid.\nThis completes the steps for adding a custom domain to Office 365.\nYou can refer to the following articles for more information:\nAdd users and domain to Office 365\nChange nameservers at any domain registrar to set up Office 365\nSet up your domain in Office 365\n","permalink":"//localhost:1313/posts/2016-11-08-adding-a-custom-domain-to-office-365/","summary":"\u003cp\u003eOffice 365 provides the ability to add your own domain. When setting up\nOffice 365 Microsoft gives you a onmicrosoft.com domain out-of-the-box.\nIf you want to use your own domain, and for most companies this is\npreferred, you can add your own domain using the setup wizard in the\nOffice 365 admin center. This will provide you the ability to use your\nown email in Office 365 or login using to Office 365 and Skype for\nBusiness, using your own domain.\u003c/p\u003e","title":"Adding a custom domain to Office 365"},{"content":"Certificates in BizTalk 2013 Part 2: How To Configure the BizTalk WCF Adapter to use the SSL Certificates In the previous post I explained hot how install SSL certificates in BizTalk to make a secure connection to a third party (web) service. In this article I will explain how you can configure the WCF Adapter to use these installed certificates to encrypt the message that is send to the service.\nCreating the Send Port First, we need a send port. You can create a new BizTalk application in the BizTalk adminstration console or use a excisiting one.\nRicht click on Send Ports \u0026ndash; new \u0026ndash; Static one-way send port.\nThe Send Port properties are displayes. Fill in/select the following values:\nName \u0026ndash; for instance WebServiceSendPort\nType \u0026ndash; WCF-BasisHttp\nSend Pipeline \u0026ndash; PassThruTransmit\nClick the Configure Button\nThe transport properties screen is displayed. Fill in the Service Url, and the SOAP Action header. Click the Security tab and fill in/select the following values:\nSecurity mode = Transport\nTransport Client Credential Type = Certificate\nUnder Client Certificate, click the browse Button. Select the installed private certificate and click Ok.\nUnder Server Certificate, click the browse Button. Select the installed public certificate and click Ok.\nThe transport property pane will look like the following figure:\nClick Ok, and again Ok. The Send Port is now configured.\nSummary This is the last part of the series where I explained how to use SSL certificates in BizTalk to create a secure connection to a third party service.\n","permalink":"//localhost:1313/posts/2016-11-08-biztalk-2013-configure-wcf-adapter-ssl-certificates/","summary":"\u003ch2 id=\"certificates-in-biztalk-2013-part-2-how-to-configure-the-biztalk-wcf-adapter-to-use-the-ssl-certificates\"\u003eCertificates in BizTalk 2013 Part 2: How To Configure the BizTalk WCF Adapter to use the SSL Certificates\u003c/h2\u003e\n\u003cp\u003eIn the previous post I explained hot how install SSL certificates in BizTalk to make a secure connection to a third party (web) service. In this article I will explain how you can configure the WCF Adapter to use these installed certificates to encrypt the message that is send to the service.\u003c/p\u003e","title":"Certificates in BizTalk 2013 Part 2: How To Configure the BizTalk WCF Adapter to use the SSL Certificates"},{"content":"When I installed the Cumulative Update for Workflow Manager 1.0 I received a Microsoft.Workflow.Common.FatalException error. The Workflow Manager Backend Service stopped working and when I tried to re-start the service it crashed.\nNavigating to the workflow management site gave me a 500 internal server error.\nI checked the Event Viewer and the following error message showed up:\nMicrosoft.Workflow.Common.FatalException: An unrecoverable error occurred. For diagnostic purposes, this English message is associated with the failure: 'A required Workflow Manager configuration 'WorkflowServiceScopeSnapshotProcessBatchSize' is not present. Please add this configuration value.'. ---\u0026gt; System.IO.InvalidDataException: A required Workflow Manager configuration 'WorkflowServiceScopeSnapshotProcessBatchSize' is not present. Please add this configuration value.\nat Microsoft.Workflow.Common.AsyncResult.End[TAsyncResult](IAsyncResult result)\nat Microsoft.Workflow.Service.WorkflowServiceBackendHost.OnStartCompleted(IAsyncResult result)\n--- End of inner exception stack trace ---\nat Microsoft.Workflow.Common.Fx.\u0026lt;\u0026gt;c__DisplayClass2.\u0026lt;FailFast\u0026gt;b__0()\nat System.Threading.ExecutionContext.RunInternal(ExecutionContext executionContext, ContextCallback callback, Object state, Boolean preserveSyncCtx)\nat System.Threading.ExecutionContext.Run(ExecutionContext executionContext, ContextCallback callback, Object state, Boolean preserveSyncCtx)\nat System.Threading.ExecutionContext.Run(ExecutionContext executionContext, ContextCallback callback, Object state)\nat System.Threading.ThreadHelper.ThreadStart()\nIt seems that there are some configuration values missing from the Workflow Resource Management Database. Add the below values to the WorkflowServiceConfig table:\nWorkflowServiceScopeSnapshotProcessBatchSize : 20\nWorkflowServiceScopeSnapshotProcessLoopInterval : 00:05:00\nWorkflowServiceSuspendedInstanceRetentionDuration : 30.00:00:00\nWorkflowServiceMaxInstanceCompressedSizeKB : 5120\nRestart the Workflow Manager Backend Service and navigate to the Workflow Management Site. You should be seeing the below information on the page:\nSjoukje ","permalink":"//localhost:1313/posts/2016-11-08-error-after-installing-cumulative-update-for-workflow-manager/","summary":"\u003cp\u003eWhen I installed the Cumulative Update for Workflow Manager 1.0 I\nreceived a Microsoft.Workflow.Common.FatalException error. The Workflow\nManager Backend Service stopped working and when I tried to re-start the\nservice it crashed.\u003c/p\u003e\n\u003cp\u003eNavigating to the workflow management site gave me a 500 internal server\nerror.\u003c/p\u003e\n\u003cp\u003eI checked the Event Viewer and the following error message showed up:\u003c/p\u003e\n\u003cp\u003eMicrosoft.Workflow.Common.FatalException: An unrecoverable error\noccurred. For diagnostic purposes, this English message is associated\nwith the failure: 'A required Workflow Manager configuration\n'WorkflowServiceScopeSnapshotProcessBatchSize' is not present. Please\nadd this configuration value.'. ---\u0026gt; System.IO.InvalidDataException:\nA required Workflow Manager configuration\n'WorkflowServiceScopeSnapshotProcessBatchSize' is not present. Please\nadd this configuration value.\u003c/p\u003e","title":"Microsoft.Workflow.Common.FatalException error after installing Cumulative Update for Workflow Manager 1.0"},{"content":"In this series of posts I will take a deep dive on creating Workflows in SharePoint 2013. As I am focusing on Microsoft integration in my daily work and had to create some workflows for the SharePoint 2013 platform in the last months, I have spent a lot of time doing research on this subject.\nWorkflow has changed a lot in SharePoint 2013 (in a positive way) and in upcoming posts I will provide you with some product insights as well as samples you can use in your own projects. I will stick to Microsoft Best Practices and give some more information on (workflow) Architecture, reusability and security regarding this topic.\nI will cover the following topics over the series:\nSharePoint 2013 Workflows\nSharePoint 2013 Workflows Part 1: Workflow Manager 1.0 Architecture\nSharePoint 2013 Workflows Part 2: Installing and Configuring The Workflow Manager.\nSharePoint 2013 Workflows Part 3: Using the SharePoint 2013 REST API in a Visual Studio 2013 Workflow.\nSharePoint 2013 Workflows Part 5: Using Secure WCF Services in a Visual Studio workflow.\nIn the first post of this series I will start off by discussing the Workflow Manager.\nSharePoint 2013 Workflow Part 1: Workflow Manager Architecture This is the first part in a series of posts about SharePoint 2013 Workflows. In this post we will discuss the architecture and improvements of the new Workflow Model in SharePoint 2013.\nNote that Office 365 is using this same Workflow Architecture only Microsoft handles all the installation and configuration regarding the Workflow Manager.\nWorkflow Architecture Besides the SharePoint 2010 workflow model, which is still part of the SharePoint 2013 installation, Microsoft introduced a new Workflow architecture in SharePoint 2013 (and Office 365).\nSharePoint 2013 is now using Workflow Manager 1.0 which acts as the host for the workflow runtime, is hosting the latest version of Windows Workflow Foundation (WF 4.5) and unlike the 2010 Workflow model, is totally decoupled from SharePoint using OAuth secured WCF Services. SharePoint 2013 is configured to send all related tasks to the workflow manager to execute workflows using these services.\nWorkflow Manager 1.0 consists of 3 components:\nWorkflow Frontend\nThis is a web service that receive calls from an external application, SharePoint in our case. Calls that are made by SharePoint to the Workflow Service are then logged in the Service Bus.\nService Bus\nThe Service bus uses the Pub/Sub model and will publish the message which is send to the service to all the subscribers.\nWorkflow Backend\nThe Workflow Backend service is a Windows Service and it is responsible for processing the actual Workflow. It communicates with SharePoint by using the SharePoint REST API.\nFigure 1: Workflow Manager Architecture (copied from MSDN)\nHow it works After installing and configuring the workflow Manager a Workflow Manager farm is created within SharePoint which contains all of the necessary databases to store workflows and services to communicate with the Workflow Manager Frontend Service. The Workflow Manager farm is configured by SharePoint as an App.\nWhen a workflow is published, SharePoint only stores a copy of the workflow, the actual workflow is going to be send to the workflow manager farm which will handle the execution of the workflow. By the time the workflow needs to be executed, SharePoint tells the Workflow Manager farm to execute the workflow and adds some extra context information to the call, like which start-up parameters to use and what SharePoint list it has to execute on. This call is send to the Workflow Manager Frontend and will be queued in the Workflow Manager Service Bus until the Workflow Manager Bacnkend process is picking it up to execute.\nWorkflows that are executed by the Workflow Manager Backend will need to communicate with SharePoint on several occasions, for instance to send emails, store items in lists, creating tasks and retrieve other relevant information for the workflow. This communication is done by using the SharePoint REST API and it uses S2S high trust authentication.\nImprovements Workflow Manager uses Windows Workflow Foundation 4.5. This means that you can only create declarative workflows in SharePoint 2013 and you are not allowed anymore to add custom code to your workflow.\nWorkflow Manager includes a HTTP Send activity which can make Web Service calls and REST / OData calls. The business logic will now need to be added to a custom web service which can be called from the HTTP Send activity. This activity is also added to SharePoint Designer. In my upcoming posts I will provide you with some samples on how to do this.\nBy moving the business logic from out of SharePoint to custom services, you can reuse this logic in multiple applications (like mobile apps, BizTalk, custom applications, etc.) and It can be hosted on premise or for instance, in Windows Azure.\nWorkflow Manager 1.0 can also be installed on multiple servers, which don\u0026rsquo;t need to have a SharePoint installation on it. This means that you can easily scale out your workflow environment and you don\u0026rsquo;t have to deal with additional SharePoint licensing costs.\nSummary Microsoft has made a lot of great improvements with this new Workflow Model. The Workflow Manager is scalable and robust and because it uses the most recent version of the workflow model, it can integrate with almost everything.\nBelow are some interesting sites about workflow manager:\nMSDN: http://msdn.microsoft.com/en-us/library/jj193528%28v=azure.10%29.aspx\nWorkflow Team Blog: http://blogs.msdn.com/b/workflowteam/\nFor more information on Workflow Foundation 4.5 I recommend the book Pro WF 4.5: http://www.amazon.com/Pro-WF-4-5-Bayer-White/dp/143024383X or you can visit:\nhttp://msdn.microsoft.com/en-us/library/hh305677%28v=vs.110%29.aspx\nSharePoint 2013 Workflows Part 2: Installing and Configuring The Workflow Manager In this post I will guide you through installing and configuring the Workflow Manager for SharePoint 2013.\nThis is the second post in the following series:\nSharePoint 2013 Workflows: Introduction\nSharePoint 2013 Workflows Part 1: Workflow Manager 1.0 Architecture\nSharePoint 2013 Workflows Part 2: Installing and Configuring The Workflow Manager.\nSharePoint 2013 Workflows Part 3: Using the SharePoint 2013 REST API in a Visual Studio 2013 Workflow.\nSharePoint 2013 Workflows Part 5: Using Secure WCF Services in a Visual Studio workflow.\nBefore starting the installation of the Workflow Manager, make sure that at least the SharePoint 2013 March Public Update (KB 2767999) is installed on toy SharePoint Server: http://support.microsoft.com/default.aspx?scid=kb;EN-US;2767999. This update solves some bug fixes and added extra support for the Workflow Manager and workflow development in Visual Studio.\nInstalling the Workflow Manager It is best to install the Workflow Manager using the Web Platform Installer tool. This will automatically install all the prerequisites like Service Bus 1.0.\nYou can download Workflow Manager 1.0 from here: http://www.microsoft.com/en-us/download/details.aspx?id=35375.\nYou can also download the Web Platform Installer Tools Command Line for offline installation:\nhttp://www.iis.net/learn/install/web-platform-installer/web-platform-installer-v4-command-line-webpicmdexe-rtw-release\nWhen workflow manager is installed, the Web Platform Installer will also install the workflow Manager Client 1.0.\nSharePoint uses this client to communicate with the Workflow Manager.\nIf the Workflow Manager is installed on a separate server, this client will still need to be installed (manually) on the SharePoint Server. You can download the Workflow Manager Client 1.0 from the same website as the Workflow Manager.\nThe Web Platform installer will also automatically install the Workflow Manager Tools 1.0 for Visual Studio if Visual Studio is already installed on the system.\nRun the Workflow Manager installer. This will launch the Web Platform Installer and click \u0026ldquo;Install\u0026rdquo; and follow the installation steps.\nFigure 1: Installing Workflow Manager 1.0 using the Web Platform installer.\nConfiguring the Workflow Manager After the installation the configuring screen is automatically opened. This configuration wizard creates the databases and Workflow Manager farm and provisions the Service Bus. It also provides a PowerShell which can be used to configure other server installations as well.\nFigure 2: Configuring Workflow Manager 1.0.\nFor this installation choose Configure Workflow Manager With Custom Settings and in the next screen enter the name of the SQL Server Instance, you can change the database name or leave the default if you like, and you can test the connection.\nFigure 3: Configuring Workflow Manager 1.0.\nIf you scroll down you can configure the service account. Best Practice is to create a new Service account for the workflow manager.\nFigure 4: Configuring Workflow Manager 1.0.\nIn the next section add a certification Generation Key and confirm. You need this key if you want to join extra servers to the workflow farm so memorize it!\nFigure 5: Configuring Workflow Manager 1.0.\nScroll down again and in the next section you can configure the ports. Leave the default settings and for developing purposes you can check the Allow Workflow Management over HTTP on this Computer box.\nFigure 6: Configuring Workflow Manager 1.0.\nClick on the right arrow to go to the next screen to configure the Service Bus.\nHere you can provide the database settings for the Service Bus. I used the default settings here for my development environment.\nFigure 7: Configuring Workflow Manager 1.0.\nHere you can provide the Service Account Settings. You can choose the same Service Account credentials as you used for the Workflow Manager or add different credentials. The same for the Certificate key, I used the same key as I entered for the Workflow Manager.\nFigure 8: Configuring Workflow Manager 1.0.\nLeave the default ports in the next section and click the right arrow.\nFigure 9: Configuring Workflow Manager 1.0.\nThe next screen provides you with an overview of the settings. Review them and Click Apply. You can also download the PowerShell Script here for installing.\nIf everything went right you will see the following screen after the installation:\nFigure 10: Configuring Workflow Manager 1.0.\nAfter installing and configuring the workflow manager and creating the SharePoint Manager Farm you need to connect both environments to communicate with each other.\nThis is done by running a PowerShell script on the SharePoint 2013 farm. It configures the SharePoint farm to send all workflow related tasks to the Workflow Manager Frontend Service.\nOpen the SharePoint Management Console and type the following Powershell cmdlet:\nFor communicating over HTTP (only recommended for a development environment)\nRegister-SPWorkflowService \u0026ndash;SPSite http://sp-siteurl \u0026ndash;WorkflowHostUri http://workflowhostURI:12291 \u0026ndash;AllowOAuthHttp\nFor communicating over HTTPS\nRegister-SPWorkflowService \u0026ndash;SPSite http:// sp-siteurl \u0026ndash;WorkflowHostUri https://workflowhostURI:12290\nTo Retrieve the Workflowhost Url open IIS, look for the Workflow Management Site check the settings.\nAfter running this command the Workflow Manager is connected to the SharePoint farm and you can begin to create custom Workflows.\nMicrosoft released a Workflow Manager Best Practice analyzer which scans your Workflow Environment and provides best practices on installing and configuring. For more information see this site: http://msdn.microsoft.com/library/azure/jj730571%28v=azure.10%29.aspx\nUpdating Workflow Manager 1.0 The Workflow Manager Service and the Service Bus are released prior to the RTM release of SharePoint 2013. Since then, some things changed and some bugs where found and resolved in below updates.\nInstall the Azure Services February 2013 Cumulative Updates:\nCumulative Update for Service Bus 1.0 (KB2799752) : http://www.microsoft.com/en-us/download/confirmation.aspx?id=36794\nCumulative Update for Workflow Manager 1.0 (KB2799754) : http://www.microsoft.com/en-us/download/details.aspx?id=36800\nSummary In this article I explained how to install and configure the Workflow Manager 1.0. The steps are already explained in several other articles on the internet, but in most cases some settings and steps are left out of it, so I wasn\u0026rsquo;t able to connect the workflow manager to the SharePoint farm.\nAlso running the SharePoint updates before installing the Workflow Manager and installing the Windows Azure February updates afterwards is a critical step, which will solve a lot of problems using the SharePoint 2013 Workflow Manager and creating Workflows.\nIn my next post I will actually create a SharePoint 2013 Workflow\u0026hellip;\u0026hellip;\nSharePoint 2013 Workflows Part 3: Using the SharePoint 2013 REST API in a Visual Studio 2013 Workflow In this post I will create a Custom Workflow in Visual Studio that uses the SharePoint REST API to create a new list item in a SharePoint list site.\nThis is the second post in the following series:\nSharePoint 2013 Workflows: Introduction\nSharePoint 2013 Workflows Part 1: Workflow Manager 1.0 Architecture\nSharePoint 2013 Workflows Part 2: Installing and Configuring The Workflow Manager.\nSharePoint 2013 Workflows Part 3: Using the SharePoint 2013 REST API in a Visual Studio 2013 Workflow.\nSharePoint 2013 Workflows Part 4: Using Secure WCF Services in a Visual Studio workflow.\nIn this scenario a List item is created in a vacation requests list. After creating the request item, the workflow is automatically triggered to create a copy of this item in the Approved Vacation Requests list.\nBoth the Vacation Request list and the workflow are created as an app in SharePoint which means they are stored in the App web. The Approved Vacation list is a list created using a Farm solution and is stored in the host web.\nThe workflow in the app needs to create a list item in the Host Web under the workflow manager context and not the user context. In this case you don\u0026rsquo;t need to give users permissions to create the list items, you only have the give the app the appropriate permissions. I think this is one of the biggest advantages using apps and that\u0026rsquo;s why I include this in my post.\nIn a real world scenario, the request probably will need to be approved or rejected by a manager, but for this demo this step is not required because I only want to show how to use the REST API in a workflow and to create a new list item in the host web.\nSetting Up the Visual Studio Project Create a new Visual Studio Project, choose the App for SharePoint project and give it a name.\nSelect SharePoint-Hosted app and fill in a SharePoint Development Site Url.\nAfter the project is created, add a new list to the project. Call the list Vacation Requests.\nCreate another SharePoint project choose SharePoint Solutions -\u0026gt; Empty SharePoint Project and call it HostWebApprovedVacationRequests. Deploy it as a Farm solution Project. Add a new list to the new Project and call it ApprovedVacationRequests. This list will be deployed to the host web.\nDeploy the Solution.\nAdd the below fields to both the lists.\nChange the start page for the App to point to the default Url of the Vacation Request list. Open the AppManifest.xml and change the start page.\nThe App will create the list in the Host web and not in the App web so you have to change the App permissions so it will have full control on the host web.\nThe last step for setting up the project is to add a workflow. Name it CreateListItemWorkflow.\nIn the next screen select List Workflow, click next and select the Vacation Requests list to associate the workflow to. Create a new History and Task list.\nSelect the option that the workflow automatically needs be started when an item is created.\nThe workflow is created and the below items are added to the project.\nCreating the Workflow So now the project is created the next step is to create the actual Workflow. First I will add a few variables I will need throughout the project.\nAdd the below variables. The first are for the listitem fields, the SecurityDigest and the WMSecurityToken are both used for authenticating to the REST API.\nYou can get a reference of the SecurityToken type from the below Assembly.\nGet ListItem Properties Add an Sequence activity to the Root Activity and call it Get Vacation Request Properties. Add a SetUserStatus Activity to the sequence and add the description: Collecting Data from the List Item. The SetUserStatus will create an internal status property which will be displayed on the Workflow Status Page.\nAdd a LookUpSPListItem activity below the SetUserStatus Activity and create a new variable ListRequestItemProperties of the type DynamicValue. This type is referenced in the Microsoft Activity Assembly under Microsoft Activities -\u0026gt; DynamicValue.\nThe DynamicValue is a new data type which understands JSON, so when you make a call to a WCF using the HTTP Send Activity, it return s an object of the type DynamicValue. You can then use an activity to extract data from this variables using an XPATH Expression.\nRename the LookUpSPListItem to Get List Item Request Properties and add the following properties:\nItemId = (currentitem)\nListId = (currentlist)\nResult = ListRequestItemProperties\nAdd an GetDynamicValueProperties Activity, call it Extract ListItem Properties, and add the ListRequestItemProperties as the source in the property pane. Click on the ellipsis button next to properties and add the items to the properties box like the items in the below figure:\nAdd a WriteToHistory Activity and add the message provided below:\nRetrieve the Root Site Collection Url Add a new variable and call it HostWebUrl. In the next step we are going to obtain the Url of the Web where the Approved Vacation Requests list is stored**.**\nAdd a new sequence below the Get Vacation Request Properties sequence and call it Get Host Url. Add a SetUserStatus Activity to it and add the description \u0026ldquo;Retrieving Host Url\u0026rdquo;.\nAdd the variables in the below figure:\nThese variables are scoped at the Get Host Url because they are only needed for this sequence.\nTo retrieve the Host Web Url a couple of calls need to be made to the REST API. First we need the Url of the App web, then we are going to use the REST API to get the properties of the current app web hosting SPWeb (Host Web), then we retrieve the server relative url of this Host web.\nAdd a WebUri Activity to the sequence and call it GetAppWebUrl. Select the AppWebUrl as the result in the property pane.\nAdd a HTTPSend Activity and call it GetHostingWebProperties to get the properties of the current site collection. Add the following properties:\nMethod: POST\nRequest Headers: Add the following request header:\nAccept: \u0026ldquo;application/json;odata=verbose\u0026rdquo; Uri: AppWebUrl + \u0026quot;/api/web/parentweb\u0026quot;\nResponseContent: GetHostSiteCollectionPropertiesResponse\nThen add a GetDynamicValueProperty\u0026lt;T\u0026gt; activity to extract the Server Relative Url from the response, select the type String, and call it Retract Host Web Relative Url. Set the following properties:\nPropertyName: \u0026ldquo;d/ServerRelativeUrl\u0026rdquo;\nResult: [HostWebServerRelativeUrl]{.mark}\nSource: [GetHostWebPropertiesResponse]{.mark}\nAdd a HTTPSend Activity, call it Get Host SiteCollection Properties and add the following properties:\nMethod: GET\nRequest Headers: Add the following request header:\nAccept: \u0026ldquo;application/json;odata=verbose\u0026rdquo; Uri: AppWebUrl + [\u0026quot;_api/site\u0026quot;]{.mark}\nResponseContent: [GetHostSiteCollectionPropertiesResponse]{.mark}\nAdd a GetDynamicValueProperty\u0026lt;T\u0026gt; activity to extract the absolute Url of the Host Site Collection, , select the type String, and call it Extract Host Site Collection Property and add the following properties:\nPropertyName: \u0026ldquo;d/Url\u0026rdquo;\nResult: [HostWebSiteCollectionUrl]{.mark}\nSource: [GetHostSiteCollectionPropertiesResponse]{.mark}\nFinally add a Assign Activity to concat the HostWebSiteCollectionUrl and the [HostWebServerRelativeUrl]{.mark} and store that in the HostWebUrl Variable.\nAdd the following properties:\nDisplayname: Get The Host Web Absolute Url\nTo: HostWebUrl\nValue: [HostWebSiteCollectionUrl + HostWebServerRelativeUrl]{.mark}\nAdd a WriteToHistory activity and add the following message:\n\u0026ldquo;The Host Web root url is: \u0026quot; + HostWebUrl\nSecurity Digest To submit an HTTP Post to the SharePoint REST API we need a Security Digest. The next step in the workflow is to obtain this.\nAs I pointed out before, the workflow in the app needs to create a list item in the Host Web under the workflow manager context. The user may not have permissions to create a list item. To create a list item as the workflow manager we need to pass along a OAuth Security token.\nSo to get this token add the GetS2SSecurityToken Activity below the Get Host Url Sequence. Add the Following Properties:\nDisplayname: Get Worfklow Manager Token\nResult: [WMSecurityToken]{.mark}\nNow that we have the token we can obtain the security Digest. Add a new Sequence and call it Get SP Security Digest. Add a new Variable and call it GetSecurityDigestReponse, type DynamicValue.\nAdd an HTTPSend Activity and add the following properties:\nMethod: POST\nRequest Headers: Add the following request header:\nAccept: \u0026ldquo;application/json;odata=verbose\u0026rdquo;\nContent-Length: \u0026ldquo;0\u0026rdquo;\nSecurityToken: [WMSecurityToken]{.mark}\nUri: [HostWebUrl + \u0026quot;_api/contextinfo\u0026quot;]{.mark}\nResponseContent: [GetSecurityDigestReponse]{.mark}\nAdd a GetDynamicValueProperty\u0026lt;T\u0026gt; activity of type String and add the following properties:\nDisplayName: Get SP Security Digest\nPropertyName: \u0026ldquo;d/GetContextWebInformation/FormDigestValue\u0026rdquo;\nResult: [SecurityDigest]{.mark}\nSource: [GetSecurityDigestReponse]{.mark}\nFor Debugging add a WriteToHistoryList Activity to write the digest to the histtorylist. Add the following message:\n[\u0026quot;SP Security Digest: \u0026quot; + SecurityDigest]{.mark}\nIt is not recommended to add this line of code in production\u0026hellip; ☺\nCreate the new List Item With the digest obtained we can create the new List Item.\nAdd a new Sequence to the Root and call it Create List item. Create the following variables:\nCreateListRequestPayLoad\nCreateListResponseBody\nAdd a SetUserStatus Activity and the following description: [\u0026quot;Create a list item in the Host Web.\u0026quot;]{.mark}\nTo Create the new list item we need to create the payload we are going to send to the REST API using JSON. Add a BuildDynamicValue Activity and add the following properties:\nDisplayName: Create new List Item Payload\nResult: [CreateListRequestPayLoad]{.mark}\nProperties:\nAdd an HTTPSend Activity and add the following properties:\nDisplayName: Create List Item\nMethod: POST\nRequest Content: [CreateListRequestPayLoad]{.mark}\nRequest Headers: Add the following request header:\nAccept: \u0026ldquo;application/json;odata=verbose\u0026rdquo;\nContent-Type: \u0026ldquo;application/json;odata=verbose\u0026rdquo;\nX-RequestDigest: [SecurityDigest]{.mark}\nSecurityToken: [WMSecurityToken]{.mark}\nUri: [HostWebUrl + \u0026quot;_api/web/lists/GetByTitle(\u0026lsquo;ApprovedVacationRequests')/items\u0026quot;]{.mark}\nResponseContent: [CreateListResponseBody]{.mark}\nDeploying the Workflow Now the workflow is created so we are going to deploy it to SharePoint. If you did not already deployed the Farm feature do it now.\nSet the Worfklow Projects as the Start-Up project and click F5.\nAdd a new Vacation Request to the list and\n","permalink":"//localhost:1313/posts/2016-11-08-sharepoint-2013-workflows/","summary":"\u003cp\u003eIn this series of posts I will take a deep dive on creating Workflows in\nSharePoint 2013. As I am focusing on Microsoft integration in my daily\nwork and had to create some workflows for the SharePoint 2013 platform\nin the last months, I have spent a lot of time doing research on this\nsubject.\u003c/p\u003e\n\u003cp\u003eWorkflow has changed a lot in SharePoint 2013 (in a positive way) and in\nupcoming posts I will provide you with some product insights as well as\nsamples you can use in your own projects. I will stick to Microsoft Best\nPractices and give some more information on (workflow) Architecture,\nreusability and security regarding this topic.\u003c/p\u003e","title":"SharePoint 2013 Workflows -- Introduction"},{"content":"This is the first part in a series on how to use SSL Certificates in your BizTalk Application to secure your data transfer when connecting to third party (web) services. In this post I\u0026rsquo;m going to explain how to install the certificates in the BizTalk servers certificate store.\nSSL Certificates SSL certificates contain a private (.pfx) and a public key (.cer) which will need to be installed in the appropriate certification stores. (For more information on SSL Certificates you can read the following article: http://www.tldp.org/HOWTO/SSL-Certificates-HOWTO/x64.html.)\nFor test scenarios you can create your own certificate using the Makecert.exe tool which is part of the .Net Framework. (For production environments SSL certificates need to be purchased from a Certificate Authority (CA).\nBut in the case of using SSL certificates in a BizTalk application, the certificates are probibly provided by the third party to which the application connects to. In that case you can skip the next part and go to the part where the certificates are installed in the appropriate stores.\nCreating SSL Certificates for this demo I\u0026rsquo;ve used the following article for creating my SSL certificate: http://www.jayway.com/2014/09/03/creating-self-signed-certificates-with-makecert-exe-for-development/http://www.jayway.com/2014/09/03/creating-self-signed-certificates-with-makecert-exe-for-development/\nI\u0026rsquo;ve made some minor changes to the input paramaters for the Makecert tool. My cmd file consists of the following parameters:\n+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+ | 1 | makecert.exe ^ | | | | | 2 | -n \u0026quot;CN=SjoukjeZaal\u0026quot; ^ | | | | | 3 | -r ^ | | | | | 4 | -pe ^ | | | | | 5 | -a sha512 ^ | | | | | 6 | -len 4096 ^ | | | | | 7 | -cy authority ^ | | | | | 8 | -sv SjoukjeZaal.pvk ^ | | | | | 9 | SjoukjeZaal.cer | | | | | 10 | pvk2pfx.exe ^ | | | | | 11 | -pvk SjoukjeZaal.pvk ^ | | | | | 12 | -spc SjoukjeZaal.cer ^ | | | | | 13 | -pfx SjoukjeZaal.pfx ^ | | | | | 14 | -po Test123 | +===================================+===================================+\nAfter following the steps from the article the below certificates are created:\nType Description\nsjoukjezaal.cer Root certificate\nsjoukjezaal.pfx Client certificate with private key. (the password for this file is \u0026ldquo;Test123″). Installing the certificates in the appropriate certificate stores Next is installing the certificates.\nClick Start, click Run and select mmc.exe to open the Microsoft Management Console.\nClick the File Menu and select Add/Remove Snap-in.\nSelect Certificates and click the Add button.\nNow you have to make a selection in which store you want to install the certificates.\n![](./Certificates in BizTalk 2013 Part 1_ How To Install SSL Certificates in the Certificate store/e5ce42b85bf81d0fb357204b9cba67368664e592.png \u0026ldquo;BizTalk-Certificates-SSL1\u0026rdquo;){width=\u0026ldquo;3.125in\u0026rdquo; height=\u0026ldquo;2.28125in\u0026rdquo;}\nThe below table shows in which store to install the SSL certificates so BizTalk can use it to encrypt the messages:\nThe In-Process Host account described in the table below can be obtained by opening the BizTalk Administration Console -\u0026gt; Host Instances and then check under which account the Host-Instance is running. For installing the certificate, log on to the server with the BizTalk Service account, open MMC and choose My User Account.\n+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+ | Certificate | Store | +===================================+==========================================================================+ | sjoukjezaal.cer | - Trusted Root CA (Local machine) | | | | | | - Trusted People (Local machine) | | | | | | - Other People (Local machine) | | | | | | - Trusted Publisher (BizTalk Service Account \u0026ndash; In-Process Host account) | +\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+ | sjoukjezaal.pfx | - Personal (local machine) | | | | | | - Personal (Logged in user) | | | | | | - Personal (BizTalk Service Account \u0026ndash; In-Process Host account). | +\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+\nSummary In this article I explained how to install SSL certificates which can be used in your BizTalk Application to make a secure call to a third party (web) service. In the second part of this series I will explain how you can configure the BizTalk Send Port to use the certificates to create a secured channel using the WCF Adapter.\n\u0026ndash; Sjoukje\n","permalink":"//localhost:1313/posts/2016-11-07-biztalk-2013-install-ssl-certificates-store_1/","summary":"\u003cp\u003eThis is the first part in a series on how to use SSL Certificates in your BizTalk Application to secure your data transfer when connecting to third party (web) services. In this post I\u0026rsquo;m going to explain how to install the certificates in the BizTalk servers certificate store.\u003c/p\u003e\n\u003ch2 id=\"ssl-certificates\"\u003eSSL Certificates\u003c/h2\u003e\n\u003cp\u003eSSL certificates contain a private (.pfx) and a public key (.cer) which will need to be installed in the appropriate certification stores. (For more information on SSL Certificates you can read the following article: \u003ca href=\"http://www.tldp.org/HOWTO/SSL-Certificates-HOWTO/x64.html\"\u003ehttp://www.tldp.org/HOWTO/SSL-Certificates-HOWTO/x64.html.)\u003c/a\u003e\u003c/p\u003e","title":"Certificates in BizTalk 2013 Part 1: How To Install SSL Certificates in the Certificate store"},{"content":"Afgelopen maart heeft Microsoft Azure App Services gelanceerd. App Services is een samenvoeging van verschillende services n.l., Web Apps (wat voorheen Azure Websites was), Mobile Apps (wat gebaseerd is op Azure Mobile Services) en 2 nieuwe services, Logic Apps en API Apps.\nMet deze nieuwe services biedt Microsoft een breed scala aan mogelijkheden. Met Web Apps en Mobile Apps kunnen ontwikkelaars web applicaties en mobiele applicaties bouwen. Met Logic Apps is het mogelijk om integratieoplossingen te ontwikkelen en deze onderdelen kunnen onderling met elkaar communiceren d.m.v. API Apps.\nHieronder leg ik kort uit wat de mogelijkheden van deze nieuwe services zijn.\nWeb Apps Azure Web Apps vervangt de bestaande Azure Websites en bevat alle onderdelen die Azure websites ook boden. Dit bevat o.a.:\nOndersteuning voor .NET, Node.js, Java, PHP en Python code\nIngebouwde functie voor automatisch schalen en taakverdeling\nHoge beschikbaarheid met automatische patches\nDoorlopende implementaties met Git, TFS, GitHub, BItBucket en Visual Studio Online\nVirtuele netwerk ondersteuning en hybride connecties met on-premise netwerken en databases\nAls je al gebruik maakt van Azure Websites dan zal je zien dat alle instances in de Azure Management Portal hernoemd zijn naar Web Apps.\nKijk hier voor meer informatie over Web Apps: http://azure.microsoft.com/nl-nl/services/app-service/web/\nMobile Apps Azure Mobile Services blijft bestaan als een losstaande service en wordt nog volledig ondersteund door Microsoft.\nAzure Mobile Apps bevat de basisonderdelen van Azure Mobile Services en daarnaast een aantal nieuwe onderdelen zoals:\nIngebouwde functie voor automatisch schalen en taakverdeling\nDoorlopende implementaties met Git, TFS, GitHub, BItBucket en Visual Studio Online\nVirtuele netwerk ondersteuning en hybride connecties met on-premise netwerken en databases\nTraffic Manager ondersteuning (automatische wereldwijde schaling)\nAzure Mobile Apps is op dit moment nog in public preview. Kijk hier voor meer informatie: http://azure.microsoft.com/nl-nl/services/app-service/mobile/\nLogic Apps Logic Apps is een nieuwe services die gebaseerd is op de technologie van BizTalk Services (BizTalk Services blijft bestaan als een losstaande service en wordt nog volledig ondersteund door Microsoft). Hiermee is het mogelijk om integratieoplossingen te bouwen.\nJe kan hiermee b.v. een workflow configureren die automatisch uitgevoerd wordt bij het aanroepen van een API, of die op ingestelde tijden een record wegschrijft naar een database. Binnen die workflow is het dan mogelijk om bepaalde taken uit te voeren, zoals het ophalen van een record uit CRM / Salesforce, een item aanmaken in Office 365, een email te verzenden of een bericht te posten op Facebook, Twitter of Yammer.\nWorkflows kunnen gedefinieerd worden met een JSON bestand of m.b.v de Logic Designer in de nieuwe Azure Portal.\nHieronder zie je een screenshot van de nieuwe Logic Designer. Hierin is een workflow geconfigureerd die iedere minuut op Twitter zoekt naar een tweet over Azure. Via Twilio worden er dan sms berichten gestuurd met de inhoud van de tweet.\nAzure Logic Apps bevat op dit moment de onderstaande connectoren:\nLogic Apps is op dit moment nog in public preview. Kijk hier voor meer informatie over Logic Apps: http://azure.microsoft.com/nl-nl/services/app-service/logic/\nAPI Apps Naast de al bestaande mogelijkheid om API\u0026rsquo;s te hosten in Azure Web Apps is het nu mogelijk om een API App te bouwen. Dit biedt de volgende extra mogelijkheden:\nDe SDK om de API aan te roepen kan automatisch gegeneerd worden\nAPI Apps kunnen in Logic Apps gebruikt worden\nDe API App kan in publieke galerieën (waaronder de Marketplace) en in particuliere organisatie-galerieën gedeeld en verkocht worden (Dit is nog niet mogelijk in de preview versie).\nAPI Apps kunnen automatisch geüpdatet worden. (Dit is nog niet mogelijk in de preview versie).\nAPI Apps kunnen in .NET, Node.js, Java, PHP en Python code gebouwd worden. Achter de schermen wordt er dan een Web App gecreëerd met speciale metadata.\nOok API Apps is op dit moment nog in public preview. Kijk hier voor meer informatie over API Apps: http://azure.microsoft.com/nl-nl/services/app-service/api/\nMeer informatie Wil je meer informatie over de nieuwe Azure App Services of gaan beginnen met het bouwen van Apps, kijk dan op de onderstaande sites:\nAzure App Services: http://azure.microsoft.com/en-us/services/app-service/\nAzure App Service Architecture: https://www.youtube.com/watch?v=jMtNOzUKhDQ\nLogic Apps: http://channel9.msdn.com/events/Ignite/2015/BRK1450\nAzure App Service Logic Apps with Josh Twist: http://channel9.msdn.com/Shows/Azure-Friday/Azure-App-Service-Logic-Apps-with-Josh-Twist\nConfigure a Web API project as an API app: http://azure.microsoft.com/en-us/documentation/articles/app-service-dotnet-create-api-app-visual-studio/\nAzure App Services pricing: http://azure.microsoft.com/en-us/pricing/details/app-service/\nAzure Preview Portal: https://portal.azure.com\n","permalink":"//localhost:1313/posts/2015-06-08-azure-app-services/","summary":"\u003cp\u003eAfgelopen maart heeft Microsoft Azure App Services gelanceerd. App Services is een samenvoeging van verschillende services n.l., Web Apps (wat voorheen Azure Websites was), Mobile Apps (wat gebaseerd is op Azure Mobile Services) en 2 nieuwe services, Logic Apps en API Apps.\u003c/p\u003e\n\u003cp\u003eMet deze nieuwe services biedt Microsoft een breed scala aan mogelijkheden. Met Web Apps en Mobile Apps kunnen ontwikkelaars web applicaties en mobiele applicaties bouwen. Met Logic Apps is het mogelijk om integratieoplossingen te ontwikkelen en deze onderdelen kunnen onderling met elkaar communiceren d.m.v. API Apps.\u003c/p\u003e","title":"Azure App Services"},{"content":"Last week I had to connect to an Oracle Database using SSIS 2008. I had a couple of issues installing the Oracle Client tools, the Microsoft Connector for Oracle Provided by Attunity and the configuration on the client side (my SSIS server) and the server side (the Oracle Database server).\nBesides that, it took me an awful lot of time reading the amount of information written on this subject before actually finding the right solution for making the connection.\nThis post is for explaining the steps regarding the installation as well as pointing to the right sites for downloading the tools and the blog posts which provide invaluable content for installing, configuring the SSIS and Oracle environment and troubleshooting the connection.\nInstall the Attunity Connector Note that both servers are part of the same domain.\nTo install the Oracle Client tools and the Microsoft Connector for Oracle by Attunity browse to the following site: http://msdn.microsoft.com/en-us/library/ee470675%28v=sql.100%29.aspx\nNote that you have to install the Attunity Connector first. By installing the Oracle client tools before the Connector, you will eventually get errors creating the connection in SSIS.\nIn my case I was installing the Client tools on a 64 bits Windows 2008 R2 server. BIDS 2008 is running in 32-bit mode so I had to install both the 32 bit client tools as well as the 64 bits client tools.\nYou can download the Oracle connector from this website: www.microsoft.com/en-us/download/details.aspx?id=29284. Download the 64bits OraAdapter and follow the installation guide provided by MSDN.\nInstall the Oracle Client Tools After installing the connector open the Oracle Database and execute the following statement to retrieve the Oracle Database version. You need this version number to download the Client Tools:\n1 SELECT version FROM V$INSTANCE\nFor downloading the 32 bits client tools browse to: http://www.oracle.com/technetwork/topics/winsoft-085727.html\nFor the 64bits tools browse to: http://www.oracle.com/technetwork/topics/winx64soft-089540.html\nChoose the instant client package for the right Oracle database version and download the zip files.\nUnzip the installation packages and run \u0026ldquo;setup.exe\u0026rdquo;.\nFigure 1: installing the Oracle client \u0026ndash;Choose the type of installation\nChoose to install the Adminstrator version, this will give you the possibility to ping the Oracle Database server and to create a \u0026ldquo;tnsnames.ora\u0026rdquo; file for setting the connecting from within the Attunity Oracle Source Adapter. Click \u0026ldquo;Next\u0026rdquo;.\nIn the next screen leave the default language (you can add additional languages if you like) and click \u0026ldquo;Next\u0026rdquo;.\nFigure 2: installing the Oracle client \u0026ndash; Specify the Oracle base path\nThe above screenshot shows the Oracle Base path and the software location where the client tools will be installed. Leave the default and click \u0026ldquo;Next\u0026rdquo;. This will start the installer and a Path Environment Variable will be created which is pointed to the Software location showed in the above screenshot.\nRepeat the installation steps for installing the 64 bits client tools.\nCreate the TNS_Admin Environment Variable For pointing to the \u0026ldquo;tnsnames.ora\u0026rdquo; file that we will be creating in the next step, first there needs to be an Environment Variable in place.\nCreate a new environment variable and call it \u0026ldquo;TNS_Admin\u0026rdquo;. In the Variable Value text box add the path where the \u0026ldquo;tnsnames.ora\u0026rdquo; file is saved. This path is the same as the software location path you entered installing the product with \u0026ldquo;\\network\\admin \u0026quot; added to it at the end.\nThe Settings for the variable will be:\nVariable name = TNS_ADMIN\nVariable value = C:\\app\\Administrator\\product\\11.2.0\\client_1\\ network\\admin\nFigure 3: Creating the TNS_ADMIN environment variable\nCreate the tnsnames.ora Navigate to the TNS_Admin path you specified above and copy the tnsnames.ora sample file from the \u0026ldquo;Sample\u0026rdquo; folder to this folder. The alias specified in this file will be used by the Attunity Oracle Source Adapter to communicate with the Oracle Database server. Open the file and add the following code to it:\n+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;+ | 1 | \u0026lt;alias\u0026gt; = | | | | | 2 | (DESCRIPTION = | | | | | 3 | (ADDRESS = (PROTOCOL = TCP)(HOST = \u0026lt;Oracle Database Server IP Adress\u0026gt;)(PORT = 1521)) | | | | | 4 | (CONNECT_DATA = | | | | | 5 | (SERVER = DEDICATED) | | | | | 6 | (SERVICE_NAME = \u0026lt;alias\u0026gt;) | | | | | 7 | ) | | | | | 8 | ) | | | | | 9 | LISTENER_\u0026lt;alias\u0026gt; = | | | | | 10 | (ADDRESS = (PROTOCOL = TCP)(HOST = \u0026lt;Oracle Database Server IP Adress\u0026gt;)()(PORT = 1521)) | +===================================+==========================================================================================+\nReplace the \u0026lt;alias\u0026gt; with a more appropriate name and save the file.\nSetting up the Listener.ora After setting up the above file the listener.ora file on the Oracle Database server will need some changes as well. I assume the client tools for the server are already installed on the database server.\nOn the Oracle Database server navigate to the folder where the client tools are installed (something like \u0026ldquo;C:\\app\\\u0026hellip;.\\product\\11.2.0\u0026rdquo; and open the \u0026ldquo;dbhome_1\u0026rdquo; folder. Navigate to \u0026ldquo;NETWORK\\ADMIN\u0026rdquo; and open the Listener.ora file and add the following code to it:\n+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-+ | 1 | LISTENER = | | | | | 2 | (DESCRIPTION_LIST = | | | | | 3 | (DESCRIPTION = | | | | | 4 | (ADDRESS = (PROTOCOL = IPC)(KEY = EXTPROC1521)) | | | | | 5 | (ADDRESS = (PROTOCOL = TCP)(HOST = \u0026lt;computername.domainname\u0026gt;)(PORT = 1521)) | | | | | 6 | ) | | | | | 7 | ) | +===================================+===============================================================================+\nChecking the Connection After setting up the listener file we can check if we have a connection.\nOn the Oracle Database server open \u0026ldquo;cmd.exe\u0026rdquo; and type \u0026ldquo;lsnrctl reload\u0026rdquo;. After that, type \u0026ldquo;lsnrctl status\u0026rdquo;. You should get the following information:\nFigure 4: Checking the Oracle listener.\nIf the connection is in place there should be a instance with the state \u0026ldquo;Ready\u0026rdquo;.\nNext is to check if the SSIS server can connect to the Oracle Listener. Open \u0026ldquo;cmd.exe\u0026rdquo; on the SSIS server and type \u0026ldquo;tnsping \u0026lt;alias\u0026gt;\u0026rdquo; (the alias provided in the tnsnames.ora)\nIf the connection is made you should receive the following information:\nFigure 5: Checking the TNS Connection\nTroubleshooting If you have any problems setting up the connection with the Oracle listener (like I had\u0026hellip;) the following sites will give a lot of information:\nhttps://community.oracle.com/thread/371929?tstart=0\nhttp://edstevensdba.wordpress.com/2011/03/19/ora-12514/\nhttps://community.oracle.com/thread/363885?tstart=0\nConnecting to Oracle using the Attunity Adapters Open BIDS 2008 and follow the instructions provided by the MSDN article: http://msdn.microsoft.com/en-us/library/ee470675%28v=sql.100%29.aspx .\nYou first need to add the Oracle Source and Oracle Destination Adapter to the toolbox. After that, you can set up a connection using these adapters. Note that, when creating the actual connection, the \u0026lt;alias\u0026gt; from the tnsnames.ora should be added to the \u0026ldquo;TNS Service Name\u0026rdquo;.\nIn this blog I demonstrated how to set up a connection with an Oracle Database from SSIS 2008. I did had problems creating the actual connection between the servers, but the articles provided in the Troubleshooting section of this post, gave me a lot of samples and information to set up the tnsnames.ora and the listener.ora file. The Attunity Connector does a great job creating the connection and querying the oracle data.\n\u0026ndash; Sjoukje\n","permalink":"//localhost:1313/posts/2014-11-08-connecting-oracle-database-ssis-2008/","summary":"\u003cp\u003eLast week I had to connect to an Oracle Database using SSIS 2008. I had a couple of issues installing the Oracle Client tools, the Microsoft Connector for Oracle Provided by Attunity and the configuration on the client side (my SSIS server) and the server side (the Oracle Database server).\u003c/p\u003e\n\u003cp\u003eBesides that, it took me an awful lot of time reading the amount of information written on this subject before actually finding the right solution for making the connection.\u003c/p\u003e","title":"Connecting to an Oracle Database using SSIS 2008"},{"content":"In this series of posts I will focus on leveraging the Out-of-the-Box SharePoint REST API in visual studio workflows.\nI will provide some samples on how to use this REST API, which includes making calls to the REST Services as well as retrieving Security Tokens to authenticate to SharePoint.\nBesides that I will create some Custom Activities which can be downloaded and used in your own SharePoint Workflow projects.\nThe following topics are included in this series:\nSharePoint 2013 Workflows - Using The SharePoint 2013 REST API: Introduction\nSharePoint 2013 Workflows \u0026ndash; Retrieving a Security Digest for your Workflow\nSharePoint 2013 Workflows - Using The SharePoint 2013 REST API Part 2: Creating a List item In this post I will create a Custom Workflow in Visual Studio that uses the SharePoint REST API to create a new list item in a SharePoint site.\nIn this scenario a List item is created in a vacation requests list. After creating the request item, the workflow is automatically triggered to create a copy of this item in the Approved Vacation Requests list.\nBoth the lists and the workflow are created as an app in SharePoint. For creating an item in the Approved Vacation list the SharePoint REST API is leveraged in the custom Workflow.\nIn a real world scenario, the request probably will need to be approved or rejected by a manager, but for this demo this step is not required because I only want to show how to use the REST API in a workflow.\nSetting Up the Visual Studio Project Create a new Visual Studio Project, choose the App for SharePoint project and give it a name.\nSelect SharePoint-Hosted app and fill in a SharePoint Development Site Url.\nAfter the project is created, add a new list to the project. Call the list VacationRequests. Add another list to the project and call it ApprovedVacationRequests.\nAdd the below fields to both the Vacation Request and the ApprovedVacationRequests list.\nChange the start page for the App to point to the default Url of the Vacation Request list. Open the AppManifest.xml and change the start page.\nThe last step for setting up the project is to add a workflow. Name it CreateListItemWorkflow.\nIn the next screen select List Workflow, click next and select the Vacation Requests list to associate the workflow to. Create a new History and Task list.\nSelect the option that the workflow automatically needs be started when an item is created.\nThe workflow is created and the below items are added to the project.\nCreating the Workflow So now the project is created the next step is to create the actual Workflow. First I will add a few variables I will need throughout the project:\nRequest Title : Type String\nFirst_Date_Of_Leave : Type String\nLast_Date_Of_Leave : Type String\nAdditional_Information : Type String\nGet ListItem Properties Add an Sequence activity to the Root Activity and call it Get Vacation Request Properties. Add a SetUserStatus Activity to the sequence and add the description: Collecting Data from the List Item. The SetUserStatus will create an internal status property which will be displayed on the Workflow Status Page.\nAdd a LookUpSPListItem activity below the SetUserStatus Activity and create a new variable ListRequestItemProperties of the type DynamicValue. This type is referenced in the Microsoft Activity Assembly under Microsoft Activities -\u0026gt; DynamicValue.\nThe DynamicValue is a new data type which understands JSON, so when you make a call to a WCF using the HTTP Send Activity, it return s an object of the type DynamicValue. You can then use an activity to extract data from this variables using an XPATH Expression.\nRename the LookUpSPListItem to Get List Item Request Properties and add the following properties:\nItemId = (currentitem)\nListId = (currentlist)\nResult = ListRequestItemProperties\nAdd an GetDynamicValueProperties Activity, call it Extract ListItem Properties and add the ListRequestItemProperties as the source in the property pane. Click on the ellipsis button next to properties and add the items to the properties box like the items in the below figure:\nAdd a WriteToHistory Activity and add the message provided below:\nCreate the new List Item Now we can create a new list item in the Approved Vacation Request list using the REST API.\nAdd a new Sequence to the Root and call it Create List item. Create the following variables:\nCreateListRequestPayLoad : Type DynamicValue\nCreateListResponseBody: Type DynamicValue\nAppWebUrl : Type String\nAdd a WebUri Activity to the sequence and call it GetAppWebUrl. Select the AppWebUrl as the result in the property pane.\nAdd a SetUserStatus Activity and the following description: [\u0026quot;Create a list item in the]{.mark} Approved Vacation List\u0026quot;.\nTo Create the new list item we need to create the payload we are going to send to the REST API using JSON. Add a BuildDynamicValue Activity and add the following properties:\nDisplayName: Create new List Item Payload\nResult: [CreateListRequestPayLoad]{.mark}\nProperties\nAdd an HTTPSend Activity and add the following properties:\nDisplayName: Create List Item\nMethod: POST\nRequest Content: [CreateListRequestPayLoad]{.mark}\nRequest Headers: Add the following request header:\nAccept: \u0026ldquo;application/json;odata=verbose\u0026rdquo;\nContent-Type: \u0026ldquo;application/json;odata=verbose\u0026rdquo;\nUri: [AppWebUrl + \u0026quot;_api/web/lists/GetByTitle(\u0026lsquo;ApprovedVacationRequests')/items\u0026quot;]{.mark}\nResponseContent: [CreateListResponseBody]{.mark}\nFigure 21: Workflow Model.\nDeploying the Workflow Now that the workflow is finished we can deploy it to SharePoint. Click F5. Add a new Vacation Request to the list and Click Save.\n","permalink":"//localhost:1313/posts/2014-11-08-sharepoint-2013-workflows-using-sharepoint-2013-rest-api-introduction.md/","summary":"\u003cp\u003eIn this series of posts I will focus on leveraging the Out-of-the-Box SharePoint REST API in visual studio workflows.\u003c/p\u003e\n\u003cp\u003eI will provide some samples on how to use this REST API, which includes making calls to the REST Services as well as retrieving Security Tokens to authenticate to SharePoint.\u003c/p\u003e\n\u003cp\u003eBesides that I will create some Custom Activities which can be downloaded and used in your own SharePoint Workflow projects.\u003c/p\u003e\n\u003cp\u003eThe following topics are included in this series:\u003c/p\u003e","title":"SharePoint 2013 Workflows - Using The SharePoint 2013 REST API: Introduction"},{"content":"In this post I will guide you through installing and configuring the Workflow Manager for SharePoint 2013.\nBefore starting the installation of the Workflow Manager, make sure that at least the SharePoint 2013 March Public Update (KB 2767999) is installed on toy SharePoint Server. This update has some bug fixes and added extra support for the Workflow Manager and workflow.\nInstalling the Workflow Manager It is best to install the Workflow Manager using the Web Platform Installer tool. This will automatically install all the prerequisites like Service Bus 1.0.\nYou can download Workflow Manager 1.0 from here: http://www.microsoft.com/en-us/download/details.aspx?id=35375\nYou can also download the Web Platform Installer Tools Command Line for offline installation: http://www.iis.net/learn/install/web-platform-installer/web-platform-installer-v4-command-line-webpicmdexe-rtw-release\nWhen workflow manager is installed, the Web Platform Installer will also install the workflow Manager Client 1.0. SharePoint uses this client to communicate with the Workflow Manager.\nIf the Workflow Manager is installed on a separate server, this client will still need to be installed (manually) on the SharePoint Server. You can download the Workflow Manager Client 1.0 from the same website as the Workflow Manager.\nThe Web Platform installer will also automatically install the Workflow Manager Tools 1.0 for Visual Studio if Visual Studio is already installed on the system.\nRun the Workflow Manager installer. This will launch the Web Platform Installer and click \u0026ldquo;Install\u0026rdquo; and follow the installation steps.\nFigure 1: Installing Workflow Manager 1.0 using the Web Platform installer.\nConfiguring the Workflow Manager After the installation the configuring screen is automatically opened. This configuration wizard creates the databases and Workflow Manager farm and provisions the Service Bus. It also provides a PowerShell which can be used to configure other server installations as well.\nFigure 2: Configuring Workflow Manager 1.0.\nFor this installation choose Configure Workflow Manager With Custom Settings and in the next screen enter the name of the SQL Server Instance, you can change the database name or leave the default if you like, and you can test the connection.\nFigure 3: Configuring Workflow Manager 1.0.\nIf you scroll down you can configure the service account. Best Practice is to create a new Service account for the workflow manager.\nFigure 4: Configuring Workflow Manager 1.0.\nIn the next section add a certification Generation Key and confirm. You need this key if you want to join extra servers to the workflow farm so memorize it!\nFigure 5: Configuring Workflow Manager 1.0.\nScroll down again and in the next section you can configure the ports. Leave the default settings and for developing purposes you can check the Allow Workflow Management over HTTP on this Computer box.\nFigure 6: Configuring Workflow Manager 1.0.\nClick on the right arrow to go to the next screen to configure the Service Bus.\nHere you can provide the database settings for the Service Bus. I used the default settings here for my development environment.\nFigure 7: Configuring Workflow Manager 1.0.\nHere you can provide the Service Account Settings. You can choose the same Service Account credentials as you used for the Workflow Manager or add different credentials. The same for the Certificate key, I used the same key as I entered for the Workflow Manager.\nFigure 8: Configuring Workflow Manager 1.0.\nLeave the default ports in the next section and click the right arrow.\nFigure 9: Configuring Workflow Manager 1.0.\nThe next screen provides you with an overview of the settings. Review them and Click Apply. You can also download the PowerShell Script here for installing.\nIf everything went right you will see the following screen after the installation:\nFigure 10: Configuring Workflow Manager 1.0.\nAfter installing and configuring the workflow manager and creating the SharePoint Manager Farm you need to connect both environments to communicate with each other.\nThis is done by running a PowerShell script on the SharePoint 2013 farm. It configures the SharePoint farm to send all workflow related tasks to the Workflow Manager Frontend Service.\nOpen the SharePoint Management Console and type the following Powershell cmdlet:\nFor communicating over HTTP (only recommended for a development environment)\n1 Register-SPWorkflowService \u0026ndash;SPSite http://sp-siteurl \u0026ndash;WorkflowHostUri http://workflowhostURI:12291 \u0026ndash;AllowOAuthHttp\nFor communicating over HTTPS\n1 Register-SPWorkflowService \u0026ndash;SPSite http://sp-siteurl \u0026ndash;WorkflowHostUri https://workflowhostURI:12290\nTo Retrieve the Workflowhost Url open IIS, look for the Workflow Management Site check the settings.\nAfter running this command the Workflow Manager is connected to the SharePoint farm and you can begin to create custom Workflows.\nMicrosoft released a Workflow Manager Best Practice analyzer which scans your Workflow Environment and provides best practices on installing and configuring. For more information see this site: http://msdn.microsoft.com/library/azure/jj730571%28v=azure.10%29.aspx\nUpdating Workflow Manager 1.0 The Workflow Manager Service and the Service Bus are released prior to the RTM release of SharePoint 2013. Since then, some things changed and some bugs where found and resolved in below updates.\nInstall the Azure Services February 2013 Cumulative Updates:\nCumulative Update for Service Bus 1.0 (KB2799752)\nCumulative Update for Workflow Manager 1.0 (KB2799754)\nSummary In this article I explained how to install and configure the Workflow Manager 1.0. The steps are already explained in several other articles on the internet, but in most cases some settings and steps are left out of it, so I wasn\u0026rsquo;t able to connect the workflow manager to the SharePoint farm.\nAlso running the SharePoint updates before installing the Workflow Manager and installing the Windows Azure February updates afterwards is a critical step, which will solve a lot of problems using the SharePoint 2013 Workflow Manager and creating Workflows.\nThis was the last article in this series on Workflows in SharePoint 2013. In my upcoming posts on this topic I will focus on leveraging the SharePoint REST API in Visual Studio Workflows. I will provide some workflow examples as well as a couple of custom activities which you can use in your own workflows.\n\u0026ndash; Sjoukje\n","permalink":"//localhost:1313/posts/2014-11-08-sharepoint-2013-workflows-install-configure-workflow-manager-copy/","summary":"\u003cp\u003eIn this post I will guide you through installing and configuring the Workflow Manager for SharePoint 2013.\u003c/p\u003e\n\u003cp\u003eBefore starting the installation of the Workflow Manager, make sure that at least the \u003ca href=\"http://support.microsoft.com/default.aspx?scid=kb;EN-US;2767999\"\u003eSharePoint 2013 March Public Update (KB 2767999)\u003c/a\u003e is installed on toy SharePoint Server. This update has some bug fixes and added extra support for the Workflow Manager and workflow.\u003c/p\u003e\n\u003ch2 id=\"installing-the-workflow-manager\"\u003eInstalling the Workflow Manager\u003c/h2\u003e\n\u003cp\u003eIt is best to install the Workflow Manager using the Web Platform Installer tool. This will automatically install all the prerequisites like Service Bus 1.0.\u003c/p\u003e","title":"SharePoint 2013 Workflows Part 2: Installing and Configuring The Workflow Manager"},{"content":"This is the first part in a series of posts about SharePoint 2013 Workflows. In this post we will discuss the architecture and improvements of the new Workflow Model in SharePoint 2013.\nWorkflow Architecture Besides the SharePoint 2010 workflow model, which is still part of the SharePoint 2013 installation, Microsoft introduced a new Workflow architecture in SharePoint 2013 (and Office 365).\nSharePoint 2013 is now using Workflow Manager 1.0 which acts as the host for the workflow runtime, is hosting the latest version of Windows Workflow Foundation (WF 4.5) and unlike the 2010 Workflow model, is totally decoupled from SharePoint using OAuth secured WCF Services. SharePoint 2013 is configured to send all related tasks to the workflow manager to execute workflows using these services.\nWorkflow Manager 1.0 consists of 3 components:\nWorkflow Frontend\nThis is a web service that receive calls from an external application, SharePoint in our case. Calls that are made by SharePoint to the Workflow Service are then logged in the Service Bus.\nService Bus\nThe Service bus uses the Pub/Sub model and will publish the message which is send to the service to all the subscribers.\nWorkflow Backend\nThe Workflow Backend service is a Windows Service and it is responsible for processing the actual Workflow. It communicates with SharePoint by using the SharePoint REST API.\nFigure 1: Workflow Manager Architecture (copied from MSDN)\nNote that Office 365 is using this same Workflow Architecture only Microsoft handles all the installation and configuration regarding the Workflow Manager.\nHow it works After installing and configuring the workflow Manager a Workflow Manager farm is created within SharePoint which contains all of the necessary databases to store workflows and services to communicate with the Workflow Manager Frontend Service. The Workflow Manager farm is configured by SharePoint as an App.\nWhen a workflow is published, SharePoint only stores a copy of the workflow, the actual workflow is going to be send to the workflow manager farm which will handle the execution of the workflow. By the time the workflow needs to be executed, SharePoint tells the Workflow Manager farm to execute the workflow and adds some extra context information to the call, like which start-up parameters to use and what SharePoint list it has to execute on. This call is send to the Workflow Manager Frontend and will be queued in the Workflow Manager Service Bus until the Workflow Manager Bacnkend process is picking it up to execute.\nWorkflows that are executed by the Workflow Manager Backend will need to communicate with SharePoint on several occasions, for instance to send emails, store items in lists, creating tasks and retrieve other relevant information for the workflow. This communication is done by using the SharePoint REST API and it uses S2S high trust authentication.\nImprovements Workflow Manager uses Windows Workflow Foundation 4.5. This means that you can only create declarative workflows in SharePoint 2013 and you are not allowed anymore to add custom code to your workflow.\nWorkflow Manager includes a HTTP Send activity which can make Web Service calls and REST / OData calls. The business logic will now need to be added to a custom web service which can be called from the HTTP Send activity. This activity is also added to SharePoint Designer. In my upcoming posts I will provide you with some samples on how to do this.\nBy moving the business logic from out of SharePoint to custom services, you can reuse this logic in multiple applications (like mobile apps, BizTalk, custom applications, etc.) and It can be hosted on premise or for instance, in Windows Azure.\nWorkflow Manager 1.0 can also be installed on multiple servers, which don\u0026rsquo;t need to have a SharePoint installation on it. This means that you can easily scale out your workflow environment and you don\u0026rsquo;t have to deal with additional SharePoint licensing costs.\nSummary Microsoft has made a lot of great improvements with this new Workflow Model. The Workflow Manager is scalable and robust and because it uses the most recent version of the workflow model, it can integrate with almost everything.\nBelow are some interesting sites about workflow manager:\nMSDN: http://msdn.microsoft.com/en-us/library/jj193528%28v=azure.10%29.aspx\nWorkflow Team Blog: http://blogs.msdn.com/b/workflowteam/\nI recommend the book Pro WF 4.5: http://www.amazon.com/Pro-WF-4-5-Bayer-White/dp/143024383X or you can visit: http://msdn.microsoft.com/en-us/library/hh305677%28v=vs.110%29.aspx\nIn my next post I will install and configure Workflow Manager 1.0 which can be used in the later samples of this series.\n\u0026ndash; Sjoukje\n","permalink":"//localhost:1313/posts/2014-11-07-sharepoint-2013-workflow-manager-architecture/","summary":"\u003cp\u003eThis is the first part in a series of posts about SharePoint 2013 Workflows. In this post we will discuss the architecture and improvements of the new Workflow Model in SharePoint 2013.\u003c/p\u003e\n\u003ch2 id=\"workflow-architecture\"\u003eWorkflow Architecture\u003c/h2\u003e\n\u003cp\u003eBesides the SharePoint 2010 workflow model, which is still part of the SharePoint 2013 installation, Microsoft introduced a new Workflow architecture in SharePoint 2013 (and Office 365).\u003c/p\u003e\n\u003cp\u003eSharePoint 2013 is now using Workflow Manager 1.0 which acts as the host for the workflow runtime, is hosting the latest version of Windows Workflow Foundation (WF 4.5) and unlike the 2010 Workflow model, is totally decoupled from SharePoint using OAuth secured WCF Services. SharePoint 2013 is configured to send all related tasks to the workflow manager to execute workflows using these services.\u003c/p\u003e","title":"SharePoint 2013 Workflow Part 1: Workflow Manager Architecture"},{"content":"Today I found a very handy tool to check your computer or server security settings: The Microsoft Baseline Security Analyzer. This tool determines if Windows administrative vulnerabilities are present, If weak passwords are being used and it checks if there are any vulnerabilities in IIS and SQL Server.\nAfter installing the tool you can scan your server or computer. After completion it provides you with a detailed report and instructions on how to make your environment more secure.\nFore more information on how to use this tool check the following site: http://blogs.technet.com/b/security/archive/2012/10/22/microsoft-free-security-tools-microsoft-baseline-security-analyzer.aspx?Redirected=true\nYou can download the MBSA from here: http://www.microsoft.com/en-us/download/details.aspx?id=7558.\n– Sjoukje\n","permalink":"//localhost:1313/posts/2014-06-24-microsoft-baseline-security-analyzer/","summary":"\u003cp\u003eToday I found a very handy tool to check your computer or server security settings: The Microsoft Baseline Security Analyzer. This tool determines if Windows administrative vulnerabilities are present, If weak passwords are being used and it checks if there are any vulnerabilities in IIS and SQL Server.\u003c/p\u003e\n\u003cp\u003eAfter installing the tool you can scan your server or computer. After completion it provides you with a detailed report and instructions on how to make your environment more secure.\u003c/p\u003e","title":"Microsoft Baseline Security Analyzer"},{"content":"If you want to debug your SharePoint 2013 on-premise workflows you need to allow the workflow manager to access your system through the firewall.\nTake the following steps to allow the workflow manager to access your system:\nIn Control Panel, choose System and Security and then Windows Firewall. In the Control Panel Home list, choose the Advanced Settings link. In the left pane of Windows Firewall, choose Inbound Rules. In the Inbound Rules list, choose Workflow Manager Tools 1.0 for Visual Studio 2012 – Test Service Host. In the Actions list, choose Enable Rule. On the properties page of your SharePoint project, choose the SharePoint tab, and then select the Enable Workflow debugging check box. ","permalink":"//localhost:1313/posts/2014-06-03-sharepoint-2013-on-premise-workflow-debugging-in-visual-studio-2013/","summary":"\u003cp\u003eIf you want to debug your SharePoint 2013 on-premise workflows you need to allow the workflow manager to access your system through the firewall.\u003c/p\u003e\n\u003cp\u003eTake the following steps to allow the workflow manager to access your system:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIn Control Panel, choose System and Security and then Windows Firewall.\u003c/li\u003e\n\u003cli\u003eIn the Control Panel Home list, choose the Advanced Settings link.\u003c/li\u003e\n\u003cli\u003eIn the left pane of Windows Firewall, choose Inbound Rules.\u003c/li\u003e\n\u003cli\u003eIn the Inbound Rules list, choose Workflow Manager Tools 1.0 for Visual Studio 2012 – Test Service Host.\u003c/li\u003e\n\u003cli\u003eIn the Actions list, choose Enable Rule.\u003c/li\u003e\n\u003cli\u003eOn the properties page of your SharePoint project, choose the SharePoint tab, and then select the Enable Workflow debugging check box.\u003c/li\u003e\n\u003c/ul\u003e","title":"SharePoint 2013 On-Premise Workflow Debugging in Visual Studio 2013"},{"content":"Microsoft released a new version of the Identity Developer Training Kit, updated for WIF 4.5 and for Visual Studio 2012 RC . You can download it here.\n","permalink":"//localhost:1313/posts/2012-07-08-download-the-windows-identity-foundation-developer-training-kit/","summary":"\u003cp\u003eMicrosoft released a new version of the Identity Developer Training Kit, updated for WIF 4.5 and for Visual Studio 2012 RC . You can download it \u003ca href=\"https://web.archive.org/web/20120728021300/http://www.microsoft.com/en-us/download/details.aspx?id=14347\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","title":"Download the Windows Identity Foundation Developer Training Kit"},{"content":"Leveraging Azure Marketplace Data in SharePoint Part 1: Consuming Azure Marketplace Data in BCS\nIn this series of posts:\nPart 1: Consuming Azure Marketplace Data in BCS (this article).\nPart 2: Using the Secure Store Service for Azure Marketplace Authentication in BCS.\nWindows Azure Marketplace data is a service by Microsoft that hosts WCF services in Windows Azure. Organizations and individuals can consume that data via a subscription model. These services expose data using REST services which can be leveraged in SharePoint using BCS.\nFor this example we are going to use US Crime data statistics Service (DATA.gov). By using BCS we can consume the Azure WCF Service and display this data in SharePoint via an External List.\nFor achieving the above we are going to take the following steps:\nCreate an Azure Marketplace account and consume the data.\nCreate a Custom .Net Connector to leverage this data in BCS.\nUse the Secure Store Service for Azure Marketplace authentication (part 2).\nIn the first part of this series we are going to register for an Azure Marketplace account so we can subscribe to a service. After this, we are going to create a BCS Custom .Net Connector for adding that data to SharePoint\u0026rsquo;s BCS. In the next part of this series we are going to use Secure Store Service for Azure Marketplace Authentication.\nAzure Marketplace Data\nTo get started navigate to [https://datamarket.azure.com/]{.underline} and register for an account by using your Windows Live ID. Click the Windows Live Sign in the upper right corner, add your information, accept the license agreement and click register. Get a developer account, search for the US Crime Data Statistics Service and add it to your account. (some data sets cost money, so be aware). After you found the data feed click on it for more details. Then click the Sign Up button on the right. After this the data feed will be added to your account. Click the \u0026ldquo;My Account\u0026rdquo; button in the top navigation and click on \u0026ldquo;My Data\u0026rdquo; in the left navigation. You will see the newly added subscription on the page. Click on the title of the service which sends you to the details page of the Crime Service. Click on \u0026ldquo;Explore the dataset\u0026rdquo;. A new window is opened and here you can filter the service data using the web browser. Add \u0026ldquo;Washington\u0026rdquo; to the \u0026ldquo;City\u0026rdquo; Textbox and click on \u0026ldquo;Run Query\u0026rdquo;.\nClick on the \u0026ldquo;Develop\u0026rdquo; Button next to the Build Query window. This URL contains the address of the service together with the filter we\u0026rsquo;ve added earlier in the Query Box. You can use the whole URL if you like but you can also use the root service URL and filter the data using LINQ in the custom .Net Connector. At the top of the screen locate the Service Root URL and copy it. Create a Custom .Net Connector for Connecting to the Azure Service\nAfter registering for an Azure Marketplace account we are going to create a custom .Net Connector to connect the data feed with SharePoint. The build in WCF connector is not suitable for this scenario because the marketplace feed expects the developer key for consuming the service. So in this case a custom connector needs to be developed using Visual Studio.\nFor this example we are going to create a .Net Assembly Connector. This type of connector is used when the External System Schema is fixed, like the data schema of the Crime data feed.\nOpen Visual Studio and create a new project.\nChoose the \u0026ldquo;Business Data Connectivity Model\u0026rdquo; as the project type. Call it \u0026ldquo;USCrimeDataConnector\u0026rdquo; (or call it anything you like) and click \u0026ldquo;OK\u0026rdquo;.\nChoose the SharePoint Server URL on which you\u0026rsquo;re going to debug and click Finish.\nRename the default BDCModel and call it \u0026ldquo;CrimeDataModel\u0026rdquo;.\nWe start by creating an External List for the Azure Crime Data. Right click the existing Entity1 and select Delete.\nSelect Entity1.cs and EntityService1.cs in the Solution Explorer and delete them.\nRight click the canvas and select Add -\u0026gt; Entity. Right click the new Entity and select Properties. In the properties window set the Name to CrimeData.\nRight click the CrimeData entity and select Add -\u0026gt; Identifier.\nSelect the Identifier and set the Name to Id using the Properties Window.\nAdd a ReadList method to the CrimeData Entity. Right click the CrimeData Entity and select Add -\u0026gt; Method. Rename the method to ReadList. In the BDC Method Details pane locate the ReadList Method and expand its parameters. Click the dropdown in \u0026lt;Add a Parameter\u0026gt; and choose Create Parameter. Set the following properties in the properties window:\nName to ReturnParameter\nParameterDirection to Return.\nIn the BDC Method Details pane locate the Instances node, select \u0026lt;Add a Method Instance\u0026gt; and choose Create Finder Instance. Set the following properties in the Properties Window:\nName to ReadList\nDefault to True\nDefaultDisplay Name to Read List\nReturn Parameter name to returnParameter.\nOpen the BDC Explorer Window, expand the ReadList message and select the returnParameterTypeDescriptor. Set the following properties in the Properties Window:\nName = CrimeDataList\nTypeName = System.Collections.Generic.IEnumerable`1[[USCrimeDataConnector.CrimeDataModel.CrimeData, CrimeDataModel]]\nIsCollection = True.\nIn the BDC Explorer, right click CrimeDataList and select Add Type Descriptor. Set the following properties in the Properties Window:\nName = CrimeData\nTypeName = USCrimeDataConnector.CrimeDataModel.CrimeData, CrimeDataModel.\nIn the BDC Explorer, right click CrimeData and select Add Type Descriptor. Set the following properties in the Properties Window:\nName = Id\nTypeName = System.Int32\nIdentifier = Id.\nAdd 3 more type descriptors and set the following properties (same as above):\nName = City\nTypeName = System.String\nName = State\nTypeName = System.String\nName = Year\nTypeName = System.Int32\nThe next step is to define the ReadItem method. Right click the CrimeData Enitity in the canvas and select Add -\u0026gt; Method. Rename the method to ReadItem.\nSwitch to the BDC Method Details Pane and select the ReadItem node. Click the dropdown in \u0026lt;Add a Parameter\u0026gt; and choose Create Parameter. Set the following properties in the properties window:\nName = ReturnParameter\nParameterDirection = Return.\nAdd another parameter and set the following properties:\nName = Id\nParameterDirection = In.\nIn the ReadItem method instances node add a new Create Finder instance and set the following properties:\nName = ReadItem.\nType = Specific Finder\nDefault = True\nDefaultDisplayName = ReadItem\nReturn Parameter = ReturnParameter\nIn the BDC Explorer Window locate the ReadItem parameters and expand them both.\nSelect idTypeDescriptor under the ReadItem\u0026rsquo;s id parameter and set the following values in the Properties window:\nName = CrimeDataId.\nTypeName = System. Int32.\nIdentifier = Id.\nRight Click CrimeData under ReadList -\u0026gt; ReturnParameter -\u0026gt; CrimeDataList -\u0026gt; CrimeData and select Copy.\nRight Click ReturnParameter under ReadItem and select Paste.\nClick Yes.\nLocate the Model and rename it from BDCModel1 to CrimeDataModel. Repeat this for the LobSystem and the LobSystemInstance.\nthe BDC Explorer Window will look like the following figure:\nThe BDC Model is ready. The next step is adding the Azure Marketplace Service Reference. Switch to the Solution Explorer and a Service Reference.\nAdd the Azure Marketplace URL to the Address box and Call the Service CrimeDataServiceReference. Click OK.\nSwitch back to the Solution Explorer and add a new class to the project. Call it CrimeData.\nAdd the following code to the CrimeData class:\n+\u0026mdash;+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;+ | 1 | public class CrimeData { | | | | | 2 | public int Id { get; set; } | | | | | 3 | public string City { get; set; } | | | | | 4 | public string State { get; set; } | | | | | 5 | public int Year { get; set; } | | | | | 6 | | | | | | 7 | } | +===+====================================+\nAdd a new class to the project and call it CrimeDataService. Add the following code to the CrimeDataService class: +\u0026mdash;-+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;+ | 1 | public partial class CrimeDataService { | | | | | 2 | | | | | | 3 | private string _url = \u0026quot;[https://api.datamarket.azure.com/data.gov/Crimes/]{.underline}\u0026quot;; | | | | | 4 | | | | | | 5 | private string _liveID = \u0026quot;{Your LiveID}\u0026quot;; | | | | | 6 | | | | | | 7 | private string _accountID = \u0026quot;{Your AccountKey}\u0026quot;; | | | | | 8 | | | | | | 9 | private static CrimeDataServiceReference.datagovCrimesContainer _context; | | | | | 10 | | | | | | 11 | public CrimeDataService() { | | | | | 12 | | | | | | 13 | _context = new CrimeDataServiceReference.datagovCrimesContainer(new Uri(_url)) Credentials = new NetworkCredential(_liveID, _accountID) | | | | | 14 | | | | | | 15 | }; | | | | | 16 | | | | | | 17 | /// The underlying connection was closed: Could not establish trust relationship for the SSL/TLS secure channel. | | | | | 18 | | | | | | 19 | /// ---\u0026gt; System.Security.Authentication.AuthenticationException: The remote certificate is invalid according to the validation procedure. | | | | | 20 | | | | | | 21 | ServicePointManager.ServerCertificateValidationCallback += (sender1, certificate, chain, sslPolicyErrors) =\u0026gt; true; | | | | | 22 | | | | | | 23 | } | | | | | 24 | | | | | | 25 | public static IEnumerable\u0026lt;CrimeData\u0026gt; ReadList() { | | | | | 26 | | | | | | 27 | try { | | | | | 28 | | | | | | 29 | var crimeData = (from c in _context.CityCrime | | | | | 30 | | | | | | 31 | where c.City == \u0026quot;Washington\u0026quot; | | | | | 32 | | | | | | 33 | select new CrimeData { | | | | | 34 | | | | | | 35 | Id = c.ROWID, | | | | | 36 | | | | | | 37 | City = c.City, | | | | | 38 | | | | | | 39 | State = c.State, | | | | | 40 | | | | | | 41 | Year = c.Year | | | | | 42 | | | | | | 43 | }).ToList(); | | | | | 44 | | | | | | 45 | return crimeData; | | | | | 46 | | | | | | 47 | } catch (Exception ex) { | | | | | 48 | | | | | | 49 | SPDiagnosticsService.Local.WriteTrace(0, new SPDiagnosticsCategory(\u0026quot;Azure BCS connector: failed to fetch read list\u0026quot;, TraceSeverity.Unexpected, EventSeverity.Error), TraceSeverity.Unexpected, ex.Message, ex.StackTrace); | | | | | 50 | | | | | | 51 | } | | | | | 52 | | | | | | 53 | return null; | | | | | 54 | | | | | | 55 | } | | | | | 56 | | | | | | 57 | \u0026amp;amp;amp;amp;nbsp; | | | | | 58 | | | | | | 59 | public static CrimeData ReadItem(int Id) { | | | | | 60 | | | | | | 61 | try { | | | | | 62 | | | | | | 63 | var item = _context.CityCrime.Where(x =\u0026gt; x.ROWID == Id).ToList().First(); | | | | | 64 | | | | | | 65 | var crimeData = new CrimeData { | | | | | 66 | | | | | | 67 | Id = item.ROWID, | | | | | 68 | | | | | | 69 | City = item.City, | | | | | 70 | | | | | | 71 | State = item.State, | | | | | 72 | | | | | | 73 | Year = item.Year | | | | | 74 | | | | | | 75 | }; | | | | | 76 | | | | | | 77 | return crimeData; | | | | | 78 | | | | | | 79 | } catch (Exception ex) { | | | | | 80 | | | | | | 81 | SPDiagnosticsService.Local.WriteTrace(0, new SPDiagnosticsCategory(\u0026quot;Azure BCS connector: failed to fetch read item\u0026quot;, TraceSeverity.Unexpected, EventSeverity.Error), TraceSeverity.Unexpected, ex.Message, ex.StackTrace); | | | | | 82 | | | | | | 83 | } | | | | | 84 | | | | | | 85 | return null; | | | | | 86 | | | | | | 87 | } | +====+==============================================================================================================================================================================================================================+\nPress F5;\nAfter deploying the external Content Type we first need to set the permissions in the BDC Service Application. Browse to Central Administration. Go to Application Management -\u0026gt; Service Applications and click the BDC Service application. Select the CrimeData ECT and click Set Object Permissions.\nAdd yourself and assign all the permissions.\nNext is creating an external list for the CrimeData ECT. Creating an external list can be done by using SharePoint Designer or the browser. We will use the browser for this sample.\nBrowse to the SharePoint site, click on Site Actions -\u0026gt; View All Site Content -\u0026gt; Create.\nChoose External List and Click Create.\nName the List CrimeData, click on select Select External Content Type and choose the CrimeData external content type from the dialog. Click the Create button.\nAfter creating the External list verify that the Azure Marketplace CrimeData is visible in the page.\nClick on one of the list items to see the details.\n","permalink":"//localhost:1313/posts/2012-02-02-leveraging-azure-marketplace-data-in-sharepoint-part-1-consuming-azure-marketplace-data-in-bcs-copy/","summary":"\u003cp\u003e\u003cstrong\u003eLeveraging Azure Marketplace Data in SharePoint Part 1: Consuming Azure Marketplace Data in BCS\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eIn this series of posts:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003ePart 1: Consuming Azure Marketplace Data in BCS (this article).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ePart 2: Using the Secure Store Service for Azure Marketplace Authentication in BCS.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWindows Azure Marketplace data is a service by Microsoft that hosts WCF services in Windows Azure. Organizations and individuals can consume that data via a subscription model. These services expose data using REST services which can be leveraged in SharePoint using BCS.\u003c/p\u003e","title":"SharePoint 2013 Workflow Part 1: Workflow Manager Architecture"},{"content":"Leveraging Azure Marketplace Data in SharePoint Part 1: Consuming Azure Marketplace Data in BCS\nWindows Azure Marketplace data is a service by Microsoft that hosts WCF services in Windows Azure. Organizations and individuals can consume that data via a subscription model. These services expose data using REST services which can be leveraged in SharePoint using BCS.\nFor this example we are going to use US Crime data statistics Service (DATA.gov). By using BCS we can consume the Azure WCF Service and display this data in SharePoint via an External List.\nFor achieving the above we are going to take the following steps:\nCreate an Azure Marketplace account and consume the data.\nCreate a Custom .Net Connector to leverage this data in BCS.\nUse the Secure Store Service for Azure Marketplace authentication (part 2).\nIn the first part of this series we are going to register for an Azure Marketplace account so we can subscribe to a service. After this, we are going to create a BCS Custom .Net Connector for adding that data to SharePoint\u0026rsquo;s BCS. In the next part of this series we are going to use Secure Store Service for Azure Marketplace Authentication.\nAzure Marketplace Data\nTo get started navigate to [https://datamarket.azure.com/]{.underline} and register for an account by using your Windows Live ID. Click the Windows Live Sign in the upper right corner, add your information, accept the license agreement and click register. Get a developer account, search for the US Crime Data Statistics Service and add it to your account. (some data sets cost money, so be aware). After you found the data feed click on it for more details. Then click the Sign Up button on the right. After this the data feed will be added to your account. Click the \u0026ldquo;My Account\u0026rdquo; button in the top navigation and click on \u0026ldquo;My Data\u0026rdquo; in the left navigation. You will see the newly added subscription on the page. Click on the title of the service which sends you to the details page of the Crime Service. Click on \u0026ldquo;Explore the dataset\u0026rdquo;. A new window is opened and here you can filter the service data using the web browser. Add \u0026ldquo;Washington\u0026rdquo; to the \u0026ldquo;City\u0026rdquo; Textbox and click on \u0026ldquo;Run Query\u0026rdquo;.\nClick on the \u0026ldquo;Develop\u0026rdquo; Button next to the Build Query window. This URL contains the address of the service together with the filter we\u0026rsquo;ve added earlier in the Query Box. You can use the whole URL if you like but you can also use the root service URL and filter the data using LINQ in the custom .Net Connector. At the top of the screen locate the Service Root URL and copy it. Create a Custom .Net Connector for Connecting to the Azure Service\nAfter registering for an Azure Marketplace account we are going to create a custom .Net Connector to connect the data feed with SharePoint. The build in WCF connector is not suitable for this scenario because the marketplace feed expects the developer key for consuming the service. So in this case a custom connector needs to be developed using Visual Studio.\nFor this example we are going to create a .Net Assembly Connector. This type of connector is used when the External System Schema is fixed, like the data schema of the Crime data feed.\nOpen Visual Studio and create a new project.\nChoose the \u0026ldquo;Business Data Connectivity Model\u0026rdquo; as the project type. Call it \u0026ldquo;USCrimeDataConnector\u0026rdquo; (or call it anything you like) and click \u0026ldquo;OK\u0026rdquo;.\nChoose the SharePoint Server URL on which you\u0026rsquo;re going to debug and click Finish.\nRename the default BDCModel and call it \u0026ldquo;CrimeDataModel\u0026rdquo;.\nWe start by creating an External List for the Azure Crime Data. Right click the existing Entity1 and select Delete.\nSelect Entity1.cs and EntityService1.cs in the Solution Explorer and delete them.\nRight click the canvas and select Add -\u0026gt; Entity. Right click the new Entity and select Properties. In the properties window set the Name to CrimeData.\nRight click the CrimeData entity and select Add -\u0026gt; Identifier.\nSelect the Identifier and set the Name to Id using the Properties Window.\nAdd a ReadList method to the CrimeData Entity. Right click the CrimeData Entity and select Add -\u0026gt; Method. Rename the method to ReadList. In the BDC Method Details pane locate the ReadList Method and expand its parameters. Click the dropdown in \u0026lt;Add a Parameter\u0026gt; and choose Create Parameter. Set the following properties in the properties window:\nName to ReturnParameter\nParameterDirection to Return.\nIn the BDC Method Details pane locate the Instances node, select \u0026lt;Add a Method Instance\u0026gt; and choose Create Finder Instance. Set the following properties in the Properties Window:\nName to ReadList\nDefault to True\nDefaultDisplay Name to Read List\nReturn Parameter name to returnParameter.\nOpen the BDC Explorer Window, expand the ReadList message and select the returnParameterTypeDescriptor. Set the following properties in the Properties Window:\nName = CrimeDataList\nTypeName = System.Collections.Generic.IEnumerable`1[[USCrimeDataConnector.CrimeDataModel.CrimeData, CrimeDataModel]]\nIsCollection = True.\nIn the BDC Explorer, right click CrimeDataList and select Add Type Descriptor. Set the following properties in the Properties Window:\nName = CrimeData\nTypeName = USCrimeDataConnector.CrimeDataModel.CrimeData, CrimeDataModel.\nIn the BDC Explorer, right click CrimeData and select Add Type Descriptor. Set the following properties in the Properties Window:\nName = Id\nTypeName = System.Int32\nIdentifier = Id.\nAdd 3 more type descriptors and set the following properties (same as above):\nName = City\nTypeName = System.String\nName = State\nTypeName = System.String\nName = Year\nTypeName = System.Int32\nThe next step is to define the ReadItem method. Right click the CrimeData Enitity in the canvas and select Add -\u0026gt; Method. Rename the method to ReadItem.\nSwitch to the BDC Method Details Pane and select the ReadItem node. Click the dropdown in \u0026lt;Add a Parameter\u0026gt; and choose Create Parameter. Set the following properties in the properties window:\nName = ReturnParameter\nParameterDirection = Return.\nAdd another parameter and set the following properties:\nName = Id\nParameterDirection = In.\nIn the ReadItem method instances node add a new Create Finder instance and set the following properties:\nName = ReadItem.\nType = Specific Finder\nDefault = True\nDefaultDisplayName = ReadItem\nReturn Parameter = ReturnParameter\nIn the BDC Explorer Window locate the ReadItem parameters and expand them both.\nSelect idTypeDescriptor under the ReadItem\u0026rsquo;s id parameter and set the following values in the Properties window:\nName = CrimeDataId.\nTypeName = System. Int32.\nIdentifier = Id.\nRight Click CrimeData under ReadList -\u0026gt; ReturnParameter -\u0026gt; CrimeDataList -\u0026gt; CrimeData and select Copy.\nRight Click ReturnParameter under ReadItem and select Paste.\nClick Yes.\nLocate the Model and rename it from BDCModel1 to CrimeDataModel. Repeat this for the LobSystem and the LobSystemInstance.\nthe BDC Explorer Window will look like the following figure:\nThe BDC Model is ready. The next step is adding the Azure Marketplace Service Reference. Switch to the Solution Explorer and a Service Reference.\nAdd the Azure Marketplace URL to the Address box and Call the Service CrimeDataServiceReference. Click OK.\nSwitch back to the Solution Explorer and add a new class to the project. Call it CrimeData.\nAdd the following code to the CrimeData class:\n+\u0026mdash;+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;+ | 1 | public class CrimeData { | | | | | 2 | public int Id { get; set; } | | | | | 3 | public string City { get; set; } | | | | | 4 | public string State { get; set; } | | | | | 5 | public int Year { get; set; } | | | | | 6 | | | | | | 7 | } | +===+====================================+\nAdd a new class to the project and call it CrimeDataService. Add the following code to the CrimeDataService class: +\u0026mdash;-+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;+ | 1 | public partial class CrimeDataService { | | | | | 2 | | | | | | 3 | private string _url = \u0026quot;[https://api.datamarket.azure.com/data.gov/Crimes/]{.underline}\u0026quot;; | | | | | 4 | | | | | | 5 | private string _liveID = \u0026quot;{Your LiveID}\u0026quot;; | | | | | 6 | | | | | | 7 | private string _accountID = \u0026quot;{Your AccountKey}\u0026quot;; | | | | | 8 | | | | | | 9 | private static CrimeDataServiceReference.datagovCrimesContainer _context; | | | | | 10 | | | | | | 11 | public CrimeDataService() { | | | | | 12 | | | | | | 13 | _context = new CrimeDataServiceReference.datagovCrimesContainer(new Uri(_url)) Credentials = new NetworkCredential(_liveID, _accountID) | | | | | 14 | | | | | | 15 | }; | | | | | 16 | | | | | | 17 | /// The underlying connection was closed: Could not establish trust relationship for the SSL/TLS secure channel. | | | | | 18 | | | | | | 19 | /// ---\u0026gt; System.Security.Authentication.AuthenticationException: The remote certificate is invalid according to the validation procedure. | | | | | 20 | | | | | | 21 | ServicePointManager.ServerCertificateValidationCallback += (sender1, certificate, chain, sslPolicyErrors) =\u0026gt; true; | | | | | 22 | | | | | | 23 | } | | | | | 24 | | | | | | 25 | public static IEnumerable\u0026lt;CrimeData\u0026gt; ReadList() { | | | | | 26 | | | | | | 27 | try { | | | | | 28 | | | | | | 29 | var crimeData = (from c in _context.CityCrime | | | | | 30 | | | | | | 31 | where c.City == \u0026quot;Washington\u0026quot; | | | | | 32 | | | | | | 33 | select new CrimeData { | | | | | 34 | | | | | | 35 | Id = c.ROWID, | | | | | 36 | | | | | | 37 | City = c.City, | | | | | 38 | | | | | | 39 | State = c.State, | | | | | 40 | | | | | | 41 | Year = c.Year | | | | | 42 | | | | | | 43 | }).ToList(); | | | | | 44 | | | | | | 45 | return crimeData; | | | | | 46 | | | | | | 47 | } catch (Exception ex) { | | | | | 48 | | | | | | 49 | SPDiagnosticsService.Local.WriteTrace(0, new SPDiagnosticsCategory(\u0026quot;Azure BCS connector: failed to fetch read list\u0026quot;, TraceSeverity.Unexpected, EventSeverity.Error), TraceSeverity.Unexpected, ex.Message, ex.StackTrace); | | | | | 50 | | | | | | 51 | } | | | | | 52 | | | | | | 53 | return null; | | | | | 54 | | | | | | 55 | } | | | | | 56 | | | | | | 57 | \u0026amp;amp;amp;amp;nbsp; | | | | | 58 | | | | | | 59 | public static CrimeData ReadItem(int Id) { | | | | | 60 | | | | | | 61 | try { | | | | | 62 | | | | | | 63 | var item = _context.CityCrime.Where(x =\u0026gt; x.ROWID == Id).ToList().First(); | | | | | 64 | | | | | | 65 | var crimeData = new CrimeData { | | | | | 66 | | | | | | 67 | Id = item.ROWID, | | | | | 68 | | | | | | 69 | City = item.City, | | | | | 70 | | | | | | 71 | State = item.State, | | | | | 72 | | | | | | 73 | Year = item.Year | | | | | 74 | | | | | | 75 | }; | | | | | 76 | | | | | | 77 | return crimeData; | | | | | 78 | | | | | | 79 | } catch (Exception ex) { | | | | | 80 | | | | | | 81 | SPDiagnosticsService.Local.WriteTrace(0, new SPDiagnosticsCategory(\u0026quot;Azure BCS connector: failed to fetch read item\u0026quot;, TraceSeverity.Unexpected, EventSeverity.Error), TraceSeverity.Unexpected, ex.Message, ex.StackTrace); | | | | | 82 | | | | | | 83 | } | | | | | 84 | | | | | | 85 | return null; | | | | | 86 | | | | | | 87 | } | +====+==============================================================================================================================================================================================================================+\nPress F5;\nAfter deploying the external Content Type we first need to set the permissions in the BDC Service Application. Browse to Central Administration. Go to Application Management -\u0026gt; Service Applications and click the BDC Service application. Select the CrimeData ECT and click Set Object Permissions.\nAdd yourself and assign all the permissions.\nNext is creating an external list for the CrimeData ECT. Creating an external list can be done by using SharePoint Designer or the browser. We will use the browser for this sample.\nBrowse to the SharePoint site, click on Site Actions -\u0026gt; View All Site Content -\u0026gt; Create.\nChoose External List and Click Create.\nName the List CrimeData, click on select Select External Content Type and choose the CrimeData external content type from the dialog. Click the Create button.\nAfter creating the External list verify that the Azure Marketplace CrimeData is visible in the page.\nClick on one of the list items to see the details.\n","permalink":"//localhost:1313/posts/2012-02-02-microsoft-azure-a-cloud-native-success-story-copy/","summary":"\u003cp\u003e\u003cstrong\u003eLeveraging Azure Marketplace Data in SharePoint Part 1: Consuming Azure Marketplace Data in BCS\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eWindows Azure Marketplace data is a service by Microsoft that hosts WCF services in Windows Azure. Organizations and individuals can consume that data via a subscription model. These services expose data using REST services which can be leveraged in SharePoint using BCS.\u003c/p\u003e\n\u003cp\u003eFor this example we are going to use US Crime data statistics Service (DATA.gov). By using BCS we can consume the Azure WCF Service and display this data in SharePoint via an External List.\u003c/p\u003e","title":"SharePoint 2013 Workflow Part 1: Workflow Manager Architecture"},{"content":"Please tell us about your own role with Capgemini I have multiple roles at Capgemini at the moment. I work as a CTO for Microsoft technologies, which means that I am responsible for the strategy and vision we have around implementing Microsoft technologies at our customers. Next to that I am a cluster lead for the Custom Software Development practice, where I am mostly responsible for business development and sales. At last, I am involved in our Cloud Center of Excellence, where I work as a cloud solution architect.\nYou are a frequent speaker at national and international events. What advice would you offer to those who wish to gain Speaking experience? To be honest, speaking in public was my biggest fear. However, I really wanted to overcome that fear, because it prevented me from sharing my ideas with others and speak up about my work. That\u0026rsquo;s why I decided to start giving presentations. I started small, at local user groups and signed up for the MVP mentor program. My mentor gave me some good advice on how to structure my storyline, we\u0026rsquo;ve practiced the sessions together and he attended my first presentation and gave me tips afterwards.\nI recommend others to do this as well, look for a mentor, someone who has been doing this for a while and start speaking at small user groups. And of course, keep practicing! It takes time to become good at it.\n{width=\u0026ldquo;4.936859142607174in\u0026rdquo; height=\u0026ldquo;3.701388888888889in\u0026rdquo;}\nAs a co-founder of the user groups Tech Daily Chronicle, Global XR Community and the Mixed Reality User Group; and a board member of Azure Thursdays and Global Azure, would you describe how you have become so actively involved in the Community and what drives your involvement? I really enjoy what I am doing and working with the community gives me a lot of energy. The community helped me to become a better Microsoft professional and I want to give something back to the community and help other to learn and become better.\nI really love organizing events and I was very lucky to meet others that share that passion with me. This resulted in founding several user groups and meeting new friends. We all have that drive to share knowledge and connecting people.\nYou are also an author of several books; with such a varied workload could you describe what your working week entails? With my busy schedule at Capgemini and the different communities I\u0026rsquo;m involved in, I need to do a lot of planning. This means blocking my agenda to have time to work on books, blogs, new presentations, and reading about new stuff. This is always a big challenge, but it gives me the chance to really dive deeply into different technologies and keeps me updated with new things.\nMost user groups I\u0026rsquo;m involved in organize meetups in the evening, so I\u0026rsquo;m busy with that as well, several evenings a month. Normally, I travel often for speaking at conferences, but of course during the pandemic this has stopped and I have shifted to doing everything online now.\nWould you outline what you find to be the most challenging and the most rewarding aspects of your work? Most challenging for me is time management. It is really a big challenge to keep on doing all of these different things and combining this with the responsibilities I have at Capgemini. But because I can do what I really love, working with technology, the community, and sharing my knowledge, I can still manage it.\nHave you had or do you have a mentor? Yes, like I said before, I had a mentor from the MVP program. But throughout my career I had a lot of different mentors, people that were eagerly sharing their knowledge and inspired me to take this path as well.\nTo date what has been your proudest accomplishment? I have a lot of accomplishments that I\u0026rsquo;m proud of! But what I\u0026rsquo;m mostly proud of, is my first MVP award in 2017 and of course becoming a Microsoft Regional Director in July this year.\nIn your opinion what is the most exciting thing about working with Azure? I think most exciting about Azure is that the platform is evolving rapidly. And also, the last couple of years you see clients embracing Azure, which means that you can put all these awesome products in practice. And that is where it becomes really exciting!\nWhat is your favorite Azure Product and why? This is a very difficult question! I have lots of favorite Azure products. But, if I have to pick one at this moment, I will go for Azure Kubernetes Service. I really love the way application development is evolving and transforming at the moment.\nWhat in your opinion has been the biggest advance in Azure in the past 18 months? Definitely, Azure Arc. I strongly believe that this will bring lots of opportunities for Microsoft and their partners in the future. In my opinion this is also what differentiates Azure from other Cloud providers at this moment.\nYou are both a Microsoft Regional Director and an MVP. May we ask about the path to receiving both of those Awards? Do you have suggestions for other community members who hope to become MVP\u0026rsquo;s? I only have one advice for this, do what you love and this all will follow! And this is also the only way to become an MVP or Regional Director. It takes a lot of dedication, time, and effort and you can only keep doing this when you really love it.\n{width=\u0026ldquo;4.41040135608049in\u0026rdquo; height=\u0026ldquo;3.3055555555555554in\u0026rdquo;}\nCould you describe what the unique spirit of the Microsoft Community means to you? Being part of the Microsoft Community is very rewarding to me! You meet a lot of new people and make new friends, with whom you share the same passion: Microsoft technology. You meet these persons again and again when you travel the world speaking at different conferences, getting to know them better each time you meet. For me this is the most rewarding part of it. Connecting with people, learning from others, and sharing our passion for technology.\n{width=\u0026ldquo;5.111111111111111in\u0026rdquo; height=\u0026ldquo;3.8307305336832895in\u0026rdquo;}\n","permalink":"//localhost:1313/posts/interview-european-cloud-conference/","summary":"\u003ch1 id=\"please-tell-us-about-your-own-role-with-capgemini\"\u003ePlease tell us about your own role with Capgemini\u003c/h1\u003e\n\u003cp\u003eI have multiple roles at Capgemini at the moment. I work as a CTO for Microsoft technologies, which means that I am responsible for the strategy and vision we have around implementing Microsoft technologies at our customers. Next to that I am a cluster lead for the Custom Software Development practice, where I am mostly responsible for business development and sales. At last, I am involved in our Cloud Center of Excellence, where I work as a cloud solution architect.\u003c/p\u003e","title":""}]